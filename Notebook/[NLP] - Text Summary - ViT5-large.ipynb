{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEkOub9vVMsK","executionInfo":{"status":"ok","timestamp":1670718684069,"user_tz":-420,"elapsed":24242,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"8bd35af5-d0f9-41cb-d2ab-38cffce7f9a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip uninstall dill\n","!pip install dill==0.3.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":430},"id":"rSXmZL4qk-xj","executionInfo":{"status":"ok","timestamp":1670719032042,"user_tz":-420,"elapsed":9022,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"71b195cf-d45d-49b7-873b-6782e4f32f18"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: dill 0.3.6\n","Uninstalling dill-0.3.6:\n","  Would remove:\n","    /usr/local/bin/get_objgraph\n","    /usr/local/bin/undill\n","    /usr/local/lib/python3.8/dist-packages/dill-0.3.6.dist-info/*\n","    /usr/local/lib/python3.8/dist-packages/dill/*\n","Proceed (y/n)? y\n","  Successfully uninstalled dill-0.3.6\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting dill==0.3.4\n","  Downloading dill-0.3.4-py2.py3-none-any.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 7.0 MB/s \n","\u001b[?25hInstalling collected packages: dill\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","multiprocess 0.70.14 requires dill>=0.3.6, but you have dill 0.3.4 which is incompatible.\n","evaluate 0.3.0 requires datasets>=2.0.0, but you have datasets 1.0.2 which is incompatible.\u001b[0m\n","Successfully installed dill-0.3.4\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["dill"]}}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"uVWe-Np2SrIA"},"source":["## Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G7BBg2SNEaDa"},"outputs":[],"source":["%%capture\n","!pip install datasets evaluate transformers[sentencepiece]\n","!pip install rouge_score"]},{"cell_type":"code","source":["# Install the vncorenlp python wrapper\n","#!pip install vncorenlp\n","\n","# Download VnCoreNLP-1.1.1.jar & its word segmentation component (i.e. RDRSegmenter) \n","# !mkdir -p vncorenlp/models/wordsegmenter\n","# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n","# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n","# !wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n","# !mv VnCoreNLP-1.1.1.jar vncorenlp/ \n","# !mv vi-vocab vncorenlp/models/wordsegmenter/\n","# !mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/\n","!pip install datasets==1.0.2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V12w92ApjA-_","executionInfo":{"status":"ok","timestamp":1670718712078,"user_tz":-420,"elapsed":7121,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"f7d03b79-1c4d-4e7d-c029-e23c5ec4a367"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets==1.0.2\n","  Downloading datasets-1.0.2-py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 17.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.2) (1.3.5)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.2) (9.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.2) (3.8.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.2) (1.21.6)\n","Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.2) (0.3.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.2) (4.64.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.2) (3.1.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.0.2) (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.0.2) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.0.2) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.0.2) (2022.9.24)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.0.2) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.0.2) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.0.2) (1.15.0)\n","Installing collected packages: datasets\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.7.1\n","    Uninstalling datasets-2.7.1:\n","      Successfully uninstalled datasets-2.7.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","evaluate 0.3.0 requires datasets>=2.0.0, but you have datasets 1.0.2 which is incompatible.\u001b[0m\n","Successfully installed datasets-1.0.2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDuUZV7nF4IL"},"outputs":[],"source":["from datasets import Dataset\n","from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, TrainingArguments, Seq2SeqTrainingArguments, AutoModel\n","from tqdm.notebook import tqdm\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","import pandas as pd\n","import numpy as np"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ah6qWnJtF-Wn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670719892436,"user_tz":-420,"elapsed":72371,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"682f7ebe-607c-49ba-f705-4394d99fd19d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(36100, 1024)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(36100, 1024)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","              (relative_attention_bias): Embedding(32, 16)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (12): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (13): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (14): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (15): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (16): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (17): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (18): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (19): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (20): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (21): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (22): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (23): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(36100, 1024)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","              (relative_attention_bias): Embedding(32, 16)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (12): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (13): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (14): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (15): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (16): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (17): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (18): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (19): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (20): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (21): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (22): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (23): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=1024, out_features=36100, bias=False)\n",")"]},"metadata":{},"execution_count":2}],"source":["tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-large\")  \n","model = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-large\")\n","\n","#tokenizer = AutoTokenizer.from_pretrained(\"VietAI/vit5-base\")  \n","#model = AutoModelForSeq2SeqLM.from_pretrained(\"VietAI/vit5-base\")\n","\n","\n","model.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YgIAqjmjF9UC"},"outputs":[],"source":["def preprocess_function(examples):\n","    model_inputs = tokenizer(\n","        examples[\"inputs\"], max_length=1024, truncation=True, padding=True\n","    )\n","    \n","    labels = tokenizer(\n","        examples[\"labels\"], max_length=64, truncation=True, padding=True\n","    )\n","    model_inputs['labels'] = labels['input_ids']\n","    model_inputs['input_ids'] = model_inputs['input_ids']\n","    return model_inputs"]},{"cell_type":"code","source":["#data = pd.read_excel(\"/content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/data_Tuoi_Tre.xlsx\")\n","#data = pd.read_excel(\"/content/drive/MyDrive/data_Tuoi_Tre.xlsx\")\n","#data = pd.read_excel(\"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/data_Tuoi_Tre.xlsx\")\n","#data = pd.read_excel(\"/content/drive/MyDrive/NLP/Data/data_Tuoi_Tre.xlsx\")\n","\n","train = pd.read_excel(\"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4model/train.xlsx\")\n","val = pd.read_excel(\"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4model/val.xlsx\")\n","test = pd.read_excel(\"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4model/test.xlsx\")"],"metadata":{"id":"55ldcUQpVqis"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#data = data.iloc[:100, :]"],"metadata":{"id":"aoMIeVC9Wala"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Obl3e6AFGzI"},"outputs":[],"source":["input_lines = train[\"Content\"].values\n","label_lines = train[\"Summary\"].values\n","\n","for i in range(len(input_lines)):\n","  input_lines[i] = str(input_lines[i]) + '</s>'\n","\n","dict_obj = {'inputs': input_lines, 'labels': label_lines}\n","dataset = Dataset.from_dict(dict_obj)\n","train_tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=8)"]},{"cell_type":"code","source":["input_lines = val[\"Content\"].values\n","label_lines = val[\"Summary\"].values\n","\n","for i in range(len(input_lines)):\n","  input_lines[i] = str(input_lines[i]) + '</s>'\n","\n","dict_obj = {'inputs': input_lines, 'labels': label_lines}\n","dataset = Dataset.from_dict(dict_obj)\n","val_tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=8)"],"metadata":{"id":"Lz5pnj3O2ouO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_lines = test[\"Content\"].values\n","label_lines = test[\"Summary\"].values\n","\n","for i in range(len(input_lines)):\n","  input_lines[i] = str(input_lines[i]) + '</s>'\n","\n","dict_obj = {'inputs': input_lines, 'labels': label_lines}\n","dataset = Dataset.from_dict(dict_obj)\n","test_tokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=['inputs'], num_proc=8)"],"metadata":{"id":"ZcDe9n7i2pCO"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHiwgkvsGfVD"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n","\n","training_args = Seq2SeqTrainingArguments(\"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5-large/\",\n","                                      do_train=True,\n","                                      do_eval=True,\n","                                      num_train_epochs=3,\n","                                      learning_rate=1e-4,\n","                                      warmup_ratio=0.05,\n","                                      weight_decay=0.01,\n","                                      per_device_train_batch_size=8,\n","                                      per_device_eval_batch_size=16,\n","                                      gradient_accumulation_steps=8,\n","                                      logging_dir='./log',\n","                                      group_by_length=True,\n","                                      save_strategy=\"steps\",\n","                                      save_steps= 2000,  \n","                                      save_total_limit=2,\n","                                      overwrite_output_dir=True,\n","                                      fp16=True,\n","                                      )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lnrMZZJHro8","colab":{"base_uri":"https://localhost:8080/","height":686},"executionInfo":{"status":"error","timestamp":1670673926855,"user_tz":-420,"elapsed":29631,"user":{"displayName":"Ngọc Cao Đình Duy","userId":"01089957364585505807"}},"outputId":"7dc46b0d-df44-40ec-9f9a-20a16b4e207e"},"outputs":[{"output_type":"stream","name":"stderr","text":["Using cuda_amp half precision backend\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 29893\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 1\n","  Total train batch size (w. parallel, distributed & accumulation) = 1\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 89679\n","  Number of trainable parameters = 791284736\n","You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='7' max='89679' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [    7/89679 00:03 < 19:43:11, 1.26 it/s, Epoch 0.00/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"error","ename":"OutOfMemoryError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-991fe50fe392>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         )\n\u001b[0;32m-> 1527\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1528\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1773\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m                 if (\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2523\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2553\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2554\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2555\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2556\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2557\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1611\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1612\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                 )\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1041\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    674\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    577\u001b[0m     ):\n\u001b[1;32m    578\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    580\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mposition_bias_masked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 539\u001b[0;31m         attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(\n\u001b[0m\u001b[1;32m    540\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m         )  # (batch_size, n_heads, seq_length, key_length)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 14.76 GiB total capacity; 13.48 GiB already allocated; 17.75 MiB free; 13.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["import torch\n","torch.cuda.empty_cache()\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_tokenized_datasets,\n","    data_collator=data_collator,\n","    eval_dataset= val_tokenized_datasets,\n",")\n","\n","trainer.train()"]},{"cell_type":"code","source":["import torch\n","#torch.cuda.empty_cache()\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_datasets,\n","    data_collator=data_collator,\n",")\n","trainer.train(resume_from_checkpoint=\"/content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-150\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":880,"referenced_widgets":["f2088c3c731a452fab10010b4e24e8a2","ec42f778e5814ee98122fe100dc358ae","21600ca61c914aeb85c466871ffe0187","1a143f0245c041869e9d61df7f585321","f9104c0c2b844cfcb349734714df34db","6fdbc53c8a194977858baad82043d389","3d7f00bb74444b028bc7038f69bc5276","46e8d85d3491404082cab20b286d2242","35c859acbb554798aa6f4a2325d9843f","855a284908b940bf94de25e1c9277503","316b5f27457c47fc976b37c350b05f89"]},"id":"GIx0jk58h5Q6","executionInfo":{"status":"error","timestamp":1669798304045,"user_tz":-420,"elapsed":105579,"user":{"displayName":"Ngọc Cao Đình Duy","userId":"01089957364585505807"}},"outputId":"ff878d96-7c96-406b-e890-169157a17072"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cuda_amp half precision backend\n","Loading model from /content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-150.\n","***** Running training *****\n","  Num examples = 100\n","  Num Epochs = 10\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 250\n","  Number of trainable parameters = 225950976\n","  Continuing training from checkpoint, will skip to saved global_step\n","  Continuing training from epoch 6\n","  Continuing training from global step 150\n","  Will skip the first 6 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"]},{"output_type":"display_data","data":{"text/plain":["0it [00:00, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2088c3c731a452fab10010b4e24e8a2"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='210' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [210/250 01:16 < 00:53, 0.75 it/s, Epoch 8.36/10]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to /content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-175\n","Configuration saved in /content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-175/config.json\n","Model weights saved in /content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-175/pytorch_model.bin\n","Saving model checkpoint to /content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-200\n","Configuration saved in /content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-200/config.json\n","Model weights saved in /content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-200/pytorch_model.bin\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-9ecf2f222674>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/D:  /Đại học/Năm 3/Kỳ 1/Xử lí ngôn ngữ tự nhiên/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5/Base/checkpoint-150\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1505\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1506\u001b[0m         )\n\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1809\u001b[0m                     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_grad_scaling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1810\u001b[0m                         \u001b[0mscale_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1811\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1812\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m                         \u001b[0mscale_after\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_scale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"No inf checks were recorded for this optimizer.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/cuda/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                 \u001b[0mbeta2t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"step\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"decay_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                 \u001b[0mupdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfactored\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m                     \u001b[0mexp_avg_sq_row\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"exp_avg_sq_row\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"XzEWbrNMSo8c"},"source":["## Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BHCiygFJ7RhP"},"outputs":[],"source":["from datasets import load_metric\n","metric = load_metric(\"rouge\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tk_hB4b-8GkP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670719985685,"user_tz":-420,"elapsed":32622,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"eb7dad6e-f927-457b-e40c-3c6e91642a95"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(36100, 1024)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(36100, 1024)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","              (relative_attention_bias): Embedding(32, 16)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (12): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (13): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (14): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (15): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (16): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (17): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (18): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (19): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (20): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (21): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (22): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (23): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(36100, 1024)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","              (relative_attention_bias): Embedding(32, 16)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (12): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (13): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (14): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (15): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (16): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (17): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (18): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (19): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (20): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (21): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (22): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (23): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=1024, out_features=1024, bias=False)\n","              (k): Linear(in_features=1024, out_features=1024, bias=False)\n","              (v): Linear(in_features=1024, out_features=1024, bias=False)\n","              (o): Linear(in_features=1024, out_features=1024, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseGatedActDense(\n","              (wi_0): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wi_1): Linear(in_features=1024, out_features=2816, bias=False)\n","              (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","              (act): NewGELUActivation()\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=1024, out_features=36100, bias=False)\n",")"]},"metadata":{},"execution_count":9}],"source":["model = AutoModelForSeq2SeqLM.from_pretrained(\"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/ViT5-Large/checkpoint-44000\")\n","model.to('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"55vewVrY7UG3"},"outputs":[],"source":["data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"54ckcdzG7Xlc","colab":{"base_uri":"https://localhost:8080/","height":195,"referenced_widgets":["e7742e223675425ab48841256373fecd","361726c0bddb43f588ac52a73ecdb598","c7153fd75f5c4016af3318b66f8c5347","b7a76e7cf88f41de9d2623c1ec7fe182","3678752268f14395b167fff42cb66a7a","010afc9eacb647929f5b3c7ee8bbc644","e91efc1f72124a91aee3a6af6799dbe2","39503348a1ca4d3d8a6e32c777cef0cc","23eda415163a49839e919a004b60f116","f6326e0d31f740b398db7c58fae841e9","0d3400907b4348a1b24aae009ebb3c81"]},"executionInfo":{"status":"ok","timestamp":1670722398251,"user_tz":-420,"elapsed":2412587,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"782dbec0-4935-4093-f016-09ed96e2fdcf"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/234 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7742e223675425ab48841256373fecd"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:3578: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"output_type":"execute_result","data":{"text/plain":["{'rouge1': AggregateScore(low=Score(precision=0.6342675463503177, recall=0.5699230516252229, fmeasure=0.5879188991257532), mid=Score(precision=0.6385557609087045, recall=0.5740028197803962, fmeasure=0.591365566127864), high=Score(precision=0.6426692483762345, recall=0.5786611104995927, fmeasure=0.594833702507042)),\n"," 'rougeL': AggregateScore(low=Score(precision=0.4115788069597472, recall=0.3708264925619341, fmeasure=0.3819884115180466), mid=Score(precision=0.41624981501435054, recall=0.37549185596912915, fmeasure=0.38616028312746287), high=Score(precision=0.4209671058814223, recall=0.3801817451668905, fmeasure=0.3905406082806975))}"]},"metadata":{},"execution_count":11}],"source":["import torch \n","import numpy as np\n","metrics = load_metric('rouge')\n","\n","max_target_length = 64\n","dataloader = torch.utils.data.DataLoader(test_tokenized_datasets, collate_fn=data_collator, batch_size=16)\n","\n","predictions = []\n","references = []\n","for i, batch in enumerate(tqdm(dataloader)):\n","  outputs = model.generate(\n","      input_ids=batch['input_ids'].to('cuda'),\n","      max_length=max_target_length,\n","      attention_mask=batch['attention_mask'].to('cuda'),\n","  )\n","  with tokenizer.as_target_tokenizer():\n","    outputs = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in outputs]\n","\n","    labels = np.where(batch['labels'] != -100,  batch['labels'], tokenizer.pad_token_id)\n","    actuals = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in labels]\n","  predictions.extend(outputs)\n","  references.extend(actuals)\n","  metrics.add_batch(predictions=outputs, references=actuals)\n","\n","\n","metrics.compute()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXJrSJVz7cOM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670722489547,"user_tz":-420,"elapsed":11463,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"4c6c79f5-007e-4440-d938-45393324aa14"},"outputs":[{"output_type":"stream","name":"stdout","text":["ROUGE 1 SCORE:  Score(precision=0.6385557609087045, recall=0.5740028197803962, fmeasure=0.591365566127864)\n","ROUGE 2 SCORE:  Score(precision=0.30050443193794135, recall=0.27171227231566, fmeasure=0.2793076425579033)\n","ROUGE L SCORE:  Score(precision=0.4163705307639243, recall=0.3754277577557638, fmeasure=0.38621089045389456)\n"]}],"source":["# load rouge for validation\n","import datasets\n","rouge = datasets.load_metric(\"rouge\")\n","print(\"ROUGE 1 SCORE: \",rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge1\"])[\"rouge1\"].mid)\n","print(\"ROUGE 2 SCORE: \",rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n","print(\"ROUGE L SCORE: \",rouge.compute(predictions=predictions, references=references, rouge_types=[\"rougeL\"])[\"rougeL\"].mid)"]},{"cell_type":"markdown","source":["# Val"],"metadata":{"id":"QQNzK90h18qD"}},{"cell_type":"code","source":["import torch \n","import numpy as np\n","metrics = load_metric('rouge')\n","\n","max_target_length = 64\n","dataloader = torch.utils.data.DataLoader(val_tokenized_datasets, collate_fn=data_collator, batch_size=16)\n","\n","predictions = []\n","references = []\n","for i, batch in enumerate(tqdm(dataloader)):\n","  outputs = model.generate(\n","      input_ids=batch['input_ids'].to('cuda'),\n","      max_length=max_target_length,\n","      attention_mask=batch['attention_mask'].to('cuda'),\n","  )\n","  with tokenizer.as_target_tokenizer():\n","    outputs = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in outputs]\n","\n","    labels = np.where(batch['labels'] != -100,  batch['labels'], tokenizer.pad_token_id)\n","    actuals = [tokenizer.decode(out, clean_up_tokenization_spaces=False, skip_special_tokens=True) for out in labels]\n","  predictions.extend(outputs)\n","  references.extend(actuals)\n","  metrics.add_batch(predictions=outputs, references=actuals)\n","\n","\n","metrics.compute()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["a09544fbaeb0411788a8d06294a82fc7","6b212f5e6af3473dba43237afac7ed91","390191b7ebd34ff9b2ec8ff3e0172a89","c3a914e397e243ad85f79111e67d4b59","1c98c4f2ca6c42b4930a7c39762fe50c","0829de0f6fff486fbcc871e9714b55d0","ba799945cbfc482389561573b0301579","b2ab07edcc794836913fde1b87a4232d","84885141202c4b3db1eb10ada699b535","372b533970a54bdb9f5fc50916d788e4","2ebaa79259574f32b3c4c1c609525636"]},"id":"ako61VeO0Q_i","executionInfo":{"status":"ok","timestamp":1670725306494,"user_tz":-420,"elapsed":2417176,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"7d945b97-782c-4380-f5da-f85e45efaf9a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/234 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a09544fbaeb0411788a8d06294a82fc7"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'rouge1': AggregateScore(low=Score(precision=0.6357934468287294, recall=0.5714531241777336, fmeasure=0.5898113757367743), mid=Score(precision=0.6401763145862083, recall=0.575762781605401, fmeasure=0.5933436499389863), high=Score(precision=0.6447498959581922, recall=0.5802708866542037, fmeasure=0.597130614991163)),\n"," 'rougeL': AggregateScore(low=Score(precision=0.4140349433415054, recall=0.37260716288484136, fmeasure=0.3843239321768729), mid=Score(precision=0.4187333863346252, recall=0.37739119212717376, fmeasure=0.38849575321015806), high=Score(precision=0.4233443520104969, recall=0.3821367524664972, fmeasure=0.39277122697243044))}"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# load rouge for validation\n","import datasets\n","rouge = datasets.load_metric(\"rouge\")\n","print(\"ROUGE 1 SCORE: \",rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge1\"])[\"rouge1\"].mid)\n","print(\"ROUGE 2 SCORE: \",rouge.compute(predictions=predictions, references=references, rouge_types=[\"rouge2\"])[\"rouge2\"].mid)\n","print(\"ROUGE L SCORE: \",rouge.compute(predictions=predictions, references=references, rouge_types=[\"rougeL\"])[\"rougeL\"].mid)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-ZsJZZAU2Dae","executionInfo":{"status":"ok","timestamp":1670725437436,"user_tz":-420,"elapsed":8418,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"e907e9b3-ab38-4d59-c98e-076b120ae952"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ROUGE 1 SCORE:  Score(precision=0.6401763145862083, recall=0.575762781605401, fmeasure=0.5933436499389863)\n","ROUGE 2 SCORE:  Score(precision=0.3043488893223688, recall=0.2743042650972063, fmeasure=0.2824560577771481)\n","ROUGE L SCORE:  Score(precision=0.41867799689934004, recall=0.37736197547300143, fmeasure=0.3885618576000381)\n"]}]},{"cell_type":"code","source":["i = 10\n","print('Prediction: ',predictions[i])\n","print('Truth: ',label_lines[i])\n","print('Content: ',input_lines[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0eMH4GCxK-Z2","executionInfo":{"status":"ok","timestamp":1670725470247,"user_tz":-420,"elapsed":582,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"4b6ea806-a95d-442e-c399-1251085da2ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Prediction:  Ford Ranger 2023 thế hệ mới dự kiến ra mắt trong tháng 8, tuy nhiên, mức giá có thể không thay đổi nhiều.\n","Truth:  Chiều 3-8, TAND tỉnh Đồng Tháp đã tuyên phạt tử hình 7 bị cáo. Hai bị cáo khác bị phạt tù chung thân và 20 năm tù, trong vụ án \"vận chuyển trái phép chất ma túy\" gần 46kg ma túy xuyên quốc gia.\n","Content:  Theo đó, hội đồng xét xử đã tuyên mức án tử hình đối với bị cáo Lê Duy Linh về 3 tội danh: vận chuyển trái phép chất ma túy, tàng trữ sử dụng trái phép chất ma túy và tổ chức sử dụng trái phép chất ma túy. Cùng bị tuyên mức án tử hình với Linh còn có 6 bị cáo: Trần Văn Thức, Nguyễn Minh Thiệt, Huỳnh Văn Lý, Lê Văn Thành, Nguyễn Văn Thái và Nguyễn Trung Hiếu, cùng về tội vận chuyển trái phép chất ma túy. Hai bị cáo Bùi Minh Cẩm Chướng và Nguyễn Y Fin lần lượt nhận mức án tù chung thân và 20 năm tù giam, cùng về tội tàng trữ trái phép chất ma túy. Theo cáo trạng, khoảng 22h ngày 15-8-2020, Phòng cảnh sát điều tra tội phạm về ma túy Công an tỉnh Đồng Tháp phối hợp Công an huyện Tân Hồng và Công an xã Tân Phước bắt quả tang Nguyễn Văn Thái lái ôtô 7 chỗ, vận chuyển trái phép 45,3kg ma túy (hơn 7kg heroin, 10kg Methamphetamine; 11,3kg MDMA và gần 17kg Ketamine) từ Trường tiểu học Dinh Bà, xã Tân Hộ Cơ, huyện Tân Hồng đến TP.HCM để giao cho người khác (không xác định được lai lịch). Qua truy xét, Cơ quan cảnh sát điều tra Công an tỉnh Đồng Tháp lần lượt bắt giữ Linh, Thức, Hiếu, Y Fin, Chướng, Thành, còn Thiệt và Lý ra đầu thú. Tất cả các bị cáo tham gia vận chuyển trái phép chất ma túy đều vì động cơ tư lợi nhằm mục đích thu lợi bất chính. Ngoài ra, các bị cáo còn khai từ giữa tháng 6-2020 đến trước khi bị bắt quả tang (ngày 15-8-2020), Linh đã tổ chức, điều hành 8 bị cáo khác vận chuyển ma túy qua biên giới Campuchia - Việt Nam trót lọt 13 lần, mỗi lần 1 - 3 balô, trọng lượng 20 - 45kg. Trong đó, Thức tham gia 14 lần, Thiệt và Chưởng 13 lần, Thái 7 lần, Hiếu và Lý 4 lần, Thành và Fin 3 lần. Khi thi hành lệnh giữ người trong trường hợp khẩn cấp với Lê Duy Linh, khám xét chỗ ở của Linh tại phòng 317, khách sạn Đ.X., TP Long Xuyên, An Giang, Công an tỉnh Đồng Tháp phát hiện thu giữ 142,564g ma túy các loại. Quá trình phạm tội, các bị cáo đã thu lợi bất chính, trong đó Lê Duy Linh hưởng lợi 497 triệu đồng và 2.500 USD. Linh chia cho các đồng phạm 119 triệu đồng, còn lại 377 triệu đồng; Trần Văn Thức 72,5 triệu đồng, Thiệt 22 triệu đồng, Thái 16,8 triệu đồng, Hiếu 11 triệu đồng, Chướng 10 triệu đồng, Thành 7,2 triệu đồng, Lý 7,2 triệu đồng, Y Fin được 2,4 triệu đồng.</s>\n"]}]},{"cell_type":"code","source":["input_lines[i]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":377},"id":"0BcRNAydLEJj","executionInfo":{"status":"ok","timestamp":1669818029952,"user_tz":-420,"elapsed":1025,"user":{"displayName":"Duy Ngọc Cao","userId":"17288859416632302215"}},"outputId":"2cb7a56b-5251-4eeb-e267-186d33cbcc7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Điều quan trọng nhất bạn có thể làm là giữ bình tĩnh và tự chủ trong tình huống khẩn cấp. Đôi khi bạn không thể làm gì, và như vậy cũng không sao. Đừng bận lòng khi phải thừa nhận rằng bạn không giúp gì được.  Nếu những người khác có mặt ở hiện trường rối trí hoặc sợ hãi, bạn hãy trấn an họ và huy động mọi người cùng hỗ trợ. Tốt hơn là bạn nên ở bên cạnh người bị nạn với cử chỉ ân cần thay vì có những hành động có thể gây tổn hại thêm. Nếu không chắc phải làm gì, bạn chỉ cần ở lại với nạn nhân. Nếu có thể, bạn hãy bắt mạch cho nạn nhân, ghi nhớ các sự kiện xảy ra và hỏi về bệnh sử của họ. Đây là những thông tin mà có thể bạn cần biết để báo lại với đội cứu hộ. Người ta thường suy nghĩ và hành động hoảng loạn khi đối mặt với tình huống khẩn cấp. Thay vì phản ứng ngay, bạn hãy dành chút thời gian lấy lại bình tĩnh. Hít thở thật sâu trước khi bắt tay vào hành động.  Sự việc thường thay đổi bất ngờ trong tình huống khẩn cấp. Đừng hoảng hốt nếu sự việc không diễn biến như bạn dự tính. Nghỉ một chút mỗi khi bạn bị quá tải, hoảng sợ hoặc bối rối. Bạn đừng ngại nếu phải ngừng làm việc gì đó nửa chừng để bình tĩnh lại. Bộ sơ cứu là công cụ đắc lực trong nhiều tình huống cấp cứu. Mỗi bộ sơ cứu cần có băng cá nhân, gạc, băng dính, thuốc sát trùng và các vật dụng cần thiết khác.  Nếu không có bộ sơ cứu, bạn hãy tìm xem có vật gì khác gần đó có thể thay thế. Bạn nên chuẩn bị một bộ sơ cứu ở nhà, và ở nơi làm việc cũng phải có một bộ sơ cứu theo quy định. Một bộ sơ cứu tốt nên có một chiếc “chăn không gian”, mảnh vật liệu nhẹ với chất liệu đặc biệt để giữ ấm cơ thể. Đây là vật dụng vô cùng cần thiết cho những người rét hoặc run, vì nó có thể giúp họ khỏi rơi vào trạng thái sốc. Điều quan trọng là biết về trạng thái ý thức của nạn nhân để hiểu rõ hơn về chấn thương của họ. Nếu nạn nhân dường như nhầm lẫn khi được hỏi hoặc trả lời sai câu hỏi, điều này cho thấy là có thể đã có những chấn thương khác. Nếu không biết nạn nhân có bất tỉnh hay không, bạn hãy chạm vào vai họ và hỏi to “Anh có sao không?\"  Bạn nên hỏi những câu như: Anh tên là gì? Hôm nay là thứ mấy? Anh bao nhiêu tuổi? Nếu nạn nhân không trả lời, bạn có thể thử xoa ngực hoặc kéo dái tai của họ để giữ cho họ tỉnh táo. Bạn cũng có thể chạm khẽ vào mí mắt nạn nhân xem họ có mở mắt không. Khi đã xác định được trạng thái ý thức của người đó, bạn hãy kiểm tra xem họ có biến chứng y khoa nào không. Hỏi xem họ có vòng đeo tay theo dõi sức khỏe hoặc ID y tế (định danh y tế) nào không. Nếu nạn nhân bị chấn thương cổ, việc di chuyển có thể gây tổn thương cột sống. Bạn luôn luôn phải gọi dịch vụ cấp cứu nếu có người bị chấn thương cổ và không thể tự di chuyển.  Nếu nạn nhân không tự bước đi được vì bị thương ở chân hoặc bàn chân, bạn có thể giúp họ di chuyển bằng cách giữ hai vai và dìu họ đi. Nếu người bị nạn sợ phải rời khỏi tình huống nguy hiểm, bạn hãy trấn an họ. Bạn cần hoàn toàn tập trung vào tình huống hiện tại, và bạn sẽ bị phân tâm nếu nói chuyện điện thoại. Hơn nữa, nếu bạn dùng điện thoại kiểu cũ thì có thể không nhận được cuộc gọi mà điều hướng viên cứu hộ đang cố liên lạc với bạn. Đừng dùng điện thoại trừ khi cần gọi sự giúp đỡ.  Nếu bạn không chắc đây có phải là tình huống khẩn cấp thực sự hay không, hãy gọi cho dịch vụ cấp cứu, và điều phối viên sẽ giúp bạn biết có cần điều nhân viên cứu hộ đến không. Đừng cố ghi lại tình huống khẩn cấp, trừ khi bạn biết chắc là mình đã ra khỏi tình huống nguy hiểm. Việc chụp ảnh “tự sướng” hoặc đăng tình huống đang xảy ra lên các mạng xã hội có thể gây tổn thương thêm và dẫn đến các rắc rối về pháp luật.</s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["sentence = \"\"\"\n","Chương trình hành động của Chính phủ đã đề ra 33 nhiệm vụ tập trung vào 5 nhóm nhiệm vụ giải pháp, trước hết là nhóm nhiệm vụ nâng cao, thống nhất nhận thức về đặc thù của đô thị, vai trò, vị thế của đô thị trong sự phát triển chung. Trong đó, xác định phát triển đô thị gồm 3 trụ cột chính là quy hoạch, xây dựng và quản lý đô thị.\n","\n","\"Quy hoạch phải đi trước một bước với tư duy đột phá, tầm nhìn chiến lược. Quy hoạch lúng túng, chậm chạp, không được đầu tư ngang tầm thì lãng phí nguồn lực, phát triển không bền vững, phát triển không đột phá. Công tác quy hoạch làm tốt thì mới tiết kiệm được nguồn lực, xác định và phát huy được tiềm năng khác biệt, cơ hội nổi trội, lợi thế cạnh tranh, chỉ ra và hóa giải những khó khăn, thách thức của địa phương, của vùng”, Thủ tướng phân tích.\n","\n","Theo Thủ tướng, quy hoạch tổng thể nhưng thực hiện có thể phân kỳ phù hợp với nguồn lực; có thể kiên trì thực hiện trong 5 năm, 10 năm, 20 năm, thậm chí hàng trăm năm… và nếu tôn trọng, làm theo quy hoạch hoàn chỉnh thì sẽ có một đô thị trật tự và phát triển.\n","\n","Ngoài ra cần đầu tư phát triển hệ thống hạ tầng đô thị đồng bộ, hiện đại trên cơ sở huy động mạnh mẽ các nguồn lực, có cơ chế, chính sách để huy động nguồn lực, kết hợp hài hòa, hợp lý, hiệu quả giữa nguồn lực bên trong và bên ngoài, giữa nguồn lực Nhà nước với các nguồn lực xã hội; đồng thời tiết kiệm, tăng thu, giảm chi, phân bổ và sử dụng nguồn lực hiệu quả, đầu tư có trọng tâm, trọng điểm.\n","\n","“Hiện vẫn còn nhiều vướng mắc về thể chế, cơ chế, chính sách cần xử lý nhưng với cơ chế đang có thì các địa phương vẫn có thể làm được nếu lãnh đạo các bộ, ngành, địa phương tập trung suy nghĩ, vận dụng tối đa các quy định hiện có trên tinh thần dám nghĩ, dám làm\", Thủ tướng lưu ý.\n","Bùi Thạc Chuyên cho biết gặp áp lực khi chuyển thể văn học từ các tác phẩm của Nguyễn Ngọc Tư. Theo anh, truyện của nữ nhà văn thường có lối viết ngắn nhưng giàu tính gợi mở, các nhân vật có số phận, chứa đựng nội tâm và tiếng nói mạnh mẽ. Do đó, ngoài yếu tố sáng tạo, anh cố gắng để giữ lại những gì mình tâm đắc trong truyện gốc. \n","\n","Tro tàn rực rỡ có sự tham gia của Phương Anh Đào, Lê Công Hoàng, Bảo Ngọc Doling, Ngô Quang Tuấn, Hạnh Thúy… Phim là câu chuyện của ba phụ nữ với 3 số phận khác nhau, cách họ sống với niềm vui, nỗi ẩn ức và ngọn lửa tình yêu với chính những người đàn ông của đời mình. Bối cảnh của chuyện xảy ra ở xóm Thơm Rơm, Cà Mau với nhiều chất liệu điện ảnh và văn hóa.\n","\n","Chia sẻ với VietNamNet, Phương Anh Đào cho biết đây là vai diễn thử thách lớn nhất trong sự nghiệp diễn xuất. Trong phim, diễn viên đảm nhận vai Nhàn - một trong 3 nữ chính. Nhàn có số phận tréo ngoe, luôn khao khát yêu và được yêu đến cháy bỏng. Vốn được biết đến với nét đẹp hiện đại, song nữ diễn viên đã “lột xác” ấn tượng với vai diễn người phụ nữ khắc khổ, tảo tần trong phim.\n","\n","“Tôi thay đổi hình ảnh rất nhiều cho vai diễn với mong muốn mang đến một Phương Anh Đào khác trên màn ảnh. Thay vì những cảnh bầm dập thể xác, nhân vật của tôi chứa nhiều thương tổn tinh thần. Mọi cảm xúc đều được giữ ở bên trong, và nó khiến nhiều người suy ngẫm. Đó cũng là cách kể chuyện của nhân vật mà anh Bùi Thạc Chuyên muốn hướng đến trong phim”, cô chia sẻ. \n","\n","Phim ghi nhận điểm sáng với diễn xuất của các diễn viên. Họ lột tả được nhân vật của mình khi bị đẩy vào hoàn cảnh cùng cực và cách thích nghi một cách bị động trước cuộc đời gần như không lối thoát. \n","\n","Hai diễn viên nam Lê Công Hoàng và Quang Tuấn thể hiện tròn vai giữa bối cảnh câu chuyện hầu hết đều xoay quanh những người đàn bà. Diễn viên Quang Tuấn tiết chế được biểu cảm cường điệu so với các vai diễn trước đây. Anh khắc họa một gã đàn ông đau đớn mất con gái và nỗi ám ảnh thường trực vì ánh sáng rực rỡ của những ngọn lửa được đốt lên từ chính ngôi nhà mình. Nam diễn viên diễn chừng mực, không lên gân hay gào thét để người xem tự thấu cảm về nỗi đau cùng cực.\n","\n","\"Do quá nhập vai, tôi đã tự lấy bật lửa đốt tay mình và đến giờ vẫn còn để lại vết bỏng\", anh kể. Trước ngày phim bấm máy, nam diễn viên dành một tháng để đến set quay, học nghề ở các lò than gần một tháng để đảm bảo hóa thân tốt. \n","\n","\"\"\"\n","\n","text =  sentence + \" </s>\"\n","encoding = tokenizer(text, return_tensors=\"pt\")\n","input_ids, attention_masks = encoding[\"input_ids\"].to(\"cuda\"), encoding[\"attention_mask\"].to(\"cuda\")\n","outputs = model.generate(\n","    input_ids=input_ids, attention_mask=attention_masks,\n","    max_length=64,\n","    early_stopping=False\n",")\n","for output in outputs:\n","    line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","    print(line)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lTQ_1p9_L3uo","executionInfo":{"status":"ok","timestamp":1670714381588,"user_tz":-420,"elapsed":1328,"user":{"displayName":"Duy Ngọc ADN","userId":"08142011782414942112"}},"outputId":"078fd76b-b464-41f2-c3ff-3048b654a347"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Thủ tướng yêu cầu các bộ, ngành nghiên cứu, quy hoạch phải đi trước một bước với tư duy đột phá, tầm nhìn chiến lược.\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"lz-qpqxoOJKi","executionInfo":{"status":"ok","timestamp":1669202526891,"user_tz":-420,"elapsed":770,"user":{"displayName":"Duy Ngọc ADN","userId":"08142011782414942112"}},"outputId":"b21a8017-1b99-48e7-b3ad-ce0ddbbf7321"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["''"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[],"metadata":{"id":"qg5-fFuHORNk"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"f2088c3c731a452fab10010b4e24e8a2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec42f778e5814ee98122fe100dc358ae","IPY_MODEL_21600ca61c914aeb85c466871ffe0187","IPY_MODEL_1a143f0245c041869e9d61df7f585321"],"layout":"IPY_MODEL_f9104c0c2b844cfcb349734714df34db"}},"ec42f778e5814ee98122fe100dc358ae":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fdbc53c8a194977858baad82043d389","placeholder":"​","style":"IPY_MODEL_3d7f00bb74444b028bc7038f69bc5276","value":"Skipping the first batches: "}},"21600ca61c914aeb85c466871ffe0187":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_46e8d85d3491404082cab20b286d2242","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35c859acbb554798aa6f4a2325d9843f","value":0}},"1a143f0245c041869e9d61df7f585321":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_855a284908b940bf94de25e1c9277503","placeholder":"​","style":"IPY_MODEL_316b5f27457c47fc976b37c350b05f89","value":" 0/0 [00:00&lt;?, ?it/s]"}},"f9104c0c2b844cfcb349734714df34db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fdbc53c8a194977858baad82043d389":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d7f00bb74444b028bc7038f69bc5276":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"46e8d85d3491404082cab20b286d2242":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"35c859acbb554798aa6f4a2325d9843f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"855a284908b940bf94de25e1c9277503":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"316b5f27457c47fc976b37c350b05f89":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e7742e223675425ab48841256373fecd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_361726c0bddb43f588ac52a73ecdb598","IPY_MODEL_c7153fd75f5c4016af3318b66f8c5347","IPY_MODEL_b7a76e7cf88f41de9d2623c1ec7fe182"],"layout":"IPY_MODEL_3678752268f14395b167fff42cb66a7a"}},"361726c0bddb43f588ac52a73ecdb598":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_010afc9eacb647929f5b3c7ee8bbc644","placeholder":"​","style":"IPY_MODEL_e91efc1f72124a91aee3a6af6799dbe2","value":"100%"}},"c7153fd75f5c4016af3318b66f8c5347":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_39503348a1ca4d3d8a6e32c777cef0cc","max":234,"min":0,"orientation":"horizontal","style":"IPY_MODEL_23eda415163a49839e919a004b60f116","value":234}},"b7a76e7cf88f41de9d2623c1ec7fe182":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f6326e0d31f740b398db7c58fae841e9","placeholder":"​","style":"IPY_MODEL_0d3400907b4348a1b24aae009ebb3c81","value":" 234/234 [40:07&lt;00:00,  9.05s/it]"}},"3678752268f14395b167fff42cb66a7a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"010afc9eacb647929f5b3c7ee8bbc644":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e91efc1f72124a91aee3a6af6799dbe2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39503348a1ca4d3d8a6e32c777cef0cc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23eda415163a49839e919a004b60f116":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f6326e0d31f740b398db7c58fae841e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0d3400907b4348a1b24aae009ebb3c81":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a09544fbaeb0411788a8d06294a82fc7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6b212f5e6af3473dba43237afac7ed91","IPY_MODEL_390191b7ebd34ff9b2ec8ff3e0172a89","IPY_MODEL_c3a914e397e243ad85f79111e67d4b59"],"layout":"IPY_MODEL_1c98c4f2ca6c42b4930a7c39762fe50c"}},"6b212f5e6af3473dba43237afac7ed91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0829de0f6fff486fbcc871e9714b55d0","placeholder":"​","style":"IPY_MODEL_ba799945cbfc482389561573b0301579","value":"100%"}},"390191b7ebd34ff9b2ec8ff3e0172a89":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b2ab07edcc794836913fde1b87a4232d","max":234,"min":0,"orientation":"horizontal","style":"IPY_MODEL_84885141202c4b3db1eb10ada699b535","value":234}},"c3a914e397e243ad85f79111e67d4b59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_372b533970a54bdb9f5fc50916d788e4","placeholder":"​","style":"IPY_MODEL_2ebaa79259574f32b3c4c1c609525636","value":" 234/234 [40:08&lt;00:00,  8.98s/it]"}},"1c98c4f2ca6c42b4930a7c39762fe50c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0829de0f6fff486fbcc871e9714b55d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ba799945cbfc482389561573b0301579":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b2ab07edcc794836913fde1b87a4232d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84885141202c4b3db1eb10ada699b535":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"372b533970a54bdb9f5fc50916d788e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ebaa79259574f32b3c4c1c609525636":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}