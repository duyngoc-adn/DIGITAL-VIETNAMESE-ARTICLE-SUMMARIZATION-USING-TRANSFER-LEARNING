{"cells":[{"cell_type":"markdown","metadata":{"id":"oLFaqXZyFsx3"},"source":["# Install Lib"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":51141,"status":"ok","timestamp":1671288896032,"user":{"displayName":"Duy Ngọc ADN","userId":"08142011782414942112"},"user_tz":-420},"id":"t-vDCx9WGMYS","outputId":"0eb84c77-84ae-4bd4-f3a0-9fcd9ce45abe"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n","\u001b[K     |████████████████████████████████| 451 kB 25.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 77.5 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 75.3 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 78.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 73.5 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","Successfully installed datasets-2.7.1 huggingface-hub-0.11.1 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting evaluate\n","  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n","\u001b[K     |████████████████████████████████| 81 kB 8.0 MB/s \n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.3.5)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from evaluate) (3.1.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.6)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2022.11.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (21.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.21.6)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.18.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.70.14)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.7.1)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.23.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.11.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.64.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.2)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->evaluate) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.12.7)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2022.6)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n","Installing collected packages: evaluate\n","Successfully installed evaluate-0.4.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rouge_score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.3.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (from rouge_score) (3.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.21.6)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from rouge_score) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk->rouge_score) (4.64.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk->rouge_score) (2022.6.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk->rouge_score) (1.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk->rouge_score) (7.1.2)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=2d2c7e2665f34822aee3605dbbd8a164da48dd2a734b9fd43df81ad9f2b51265\n","  Stored in directory: /root/.cache/pip/wheels/24/55/6f/ebfc4cb176d1c9665da4e306e1705496206d08215c1acd9dde\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.8/dist-packages (3.19.6)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: nltk in /usr/local/lib/python3.8/dist-packages (3.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from nltk) (1.2.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from nltk) (2022.6.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from nltk) (4.64.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from nltk) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting py7zr\n","  Downloading py7zr-0.20.2-py3-none-any.whl (65 kB)\n","\u001b[K     |████████████████████████████████| 65 kB 3.6 MB/s \n","\u001b[?25hCollecting pycryptodomex>=3.6.6\n","  Downloading pycryptodomex-3.16.0-cp35-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.3 MB)\n","\u001b[K     |████████████████████████████████| 2.3 MB 63.3 MB/s \n","\u001b[?25hCollecting pyppmd<1.1.0,>=0.18.1\n","  Downloading pyppmd-1.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (139 kB)\n","\u001b[K     |████████████████████████████████| 139 kB 74.6 MB/s \n","\u001b[?25hCollecting pyzstd>=0.14.4\n","  Downloading pyzstd-0.15.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\n","\u001b[K     |████████████████████████████████| 378 kB 76.3 MB/s \n","\u001b[?25hCollecting multivolumefile>=0.2.3\n","  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from py7zr) (5.4.8)\n","Collecting pybcj>=0.6.0\n","  Downloading pybcj-1.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (50 kB)\n","\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n","\u001b[?25hCollecting inflate64>=0.3.1\n","  Downloading inflate64-0.3.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n","\u001b[?25hCollecting brotli>=1.0.9\n","  Downloading Brotli-1.0.9-cp38-cp38-manylinux1_x86_64.whl (357 kB)\n","\u001b[K     |████████████████████████████████| 357 kB 74.6 MB/s \n","\u001b[?25hCollecting texttable\n","  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n","Installing collected packages: texttable, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, inflate64, brotli, py7zr\n","Successfully installed brotli-1.0.9 inflate64-0.3.1 multivolumefile-0.2.3 py7zr-0.20.2 pybcj-1.0.1 pycryptodomex-3.16.0 pyppmd-1.0.0 pyzstd-0.15.3 texttable-1.6.7\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting accelerate\n","  Downloading accelerate-0.15.0-py3-none-any.whl (191 kB)\n","\u001b[K     |████████████████████████████████| 191 kB 31.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.21.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (21.3)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from accelerate) (1.13.0+cu116)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from accelerate) (6.0)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from accelerate) (5.4.8)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->accelerate) (3.0.9)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.4.0->accelerate) (4.4.0)\n","Installing collected packages: accelerate\n","Successfully installed accelerate-0.15.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.13.0+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.4.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 21.3 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.97\n"]}],"source":["!pip install datasets\n","!pip install evaluate\n","!pip install rouge_score\n","!pip install protobuf\n","!pip install nltk\n","!pip install py7zr\n","!pip install accelerate\n","!pip install torch\n","!pip install sentencepiece"]},{"cell_type":"markdown","metadata":{"id":"rARvZDULFxuQ"},"source":["# Prepair code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e3bC6uwAGO96","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671288968550,"user_tz":-420,"elapsed":72526,"user":{"displayName":"Duy Ngọc ADN","userId":"08142011782414942112"}},"outputId":"d89f4412-3297-40e5-f4c9-3a05a29b978c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pifQRaeQGM1U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671289009727,"user_tz":-420,"elapsed":41182,"user":{"displayName":"Duy Ngọc ADN","userId":"08142011782414942112"}},"outputId":"c6ec251e-88e2-46b5-8f8f-643465762906"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'transformers'...\n","remote: Enumerating objects: 120046, done.\u001b[K\n","remote: Counting objects: 100% (313/313), done.\u001b[K\n","remote: Compressing objects: 100% (157/157), done.\u001b[K\n","remote: Total 120046 (delta 176), reused 230 (delta 136), pack-reused 119733\u001b[K\n","Receiving objects: 100% (120046/120046), 113.65 MiB | 17.26 MiB/s, done.\n","Resolving deltas: 100% (89666/89666), done.\n","/content/transformers\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Processing /content/transformers\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (4.64.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 31.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (2.23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (6.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (0.11.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (21.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (3.8.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.26.0.dev0) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.26.0.dev0) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.26.0.dev0) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (2022.12.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.26.0.dev0) (2.10)\n","Building wheels for collected packages: transformers\n","  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for transformers: filename=transformers-4.26.0.dev0-py3-none-any.whl size=5984778 sha256=4bba8173475a141e0a0145414f96357bed70b879a86629ff4354f1c6a04b4449\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-qxijerpc/wheels/15/57/14/0d2873a0295966ca166ea9d9225761a50cce27e4d6b0341fcc\n","Successfully built transformers\n","Installing collected packages: tokenizers, transformers\n","Successfully installed tokenizers-0.13.2 transformers-4.26.0.dev0\n"]}],"source":["# Tải code và cài thư viện transformer \n","\n","# Tải code \n","%cd /content\n","!git clone https://github.com/huggingface/transformers\n","\n","# Cài thư viện transformer\n","%cd transformers\n","!pip install .\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10819,"status":"ok","timestamp":1670740401610,"user":{"displayName":"Duy Ngọc Cao","userId":"17288859416632302215"},"user_tz":-420},"id":"5xQCMSo7K5gr","outputId":"f0c4a6f1-046d-4bfc-b21a-bf471e3c290f"},"outputs":[{"output_type":"stream","name":"stdout","text":["usage: run_summarization.py\n","       [-h]\n","       --model_name_or_path\n","       MODEL_NAME_OR_PATH\n","       [--config_name CONFIG_NAME]\n","       [--tokenizer_name TOKENIZER_NAME]\n","       [--cache_dir CACHE_DIR]\n","       [--use_fast_tokenizer [USE_FAST_TOKENIZER]]\n","       [--no_use_fast_tokenizer]\n","       [--model_revision MODEL_REVISION]\n","       [--use_auth_token [USE_AUTH_TOKEN]]\n","       [--resize_position_embeddings RESIZE_POSITION_EMBEDDINGS]\n","       [--lang LANG]\n","       [--dataset_name DATASET_NAME]\n","       [--dataset_config_name DATASET_CONFIG_NAME]\n","       [--text_column TEXT_COLUMN]\n","       [--summary_column SUMMARY_COLUMN]\n","       [--train_file TRAIN_FILE]\n","       [--validation_file VALIDATION_FILE]\n","       [--test_file TEST_FILE]\n","       [--overwrite_cache [OVERWRITE_CACHE]]\n","       [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]\n","       [--max_source_length MAX_SOURCE_LENGTH]\n","       [--max_target_length MAX_TARGET_LENGTH]\n","       [--val_max_target_length VAL_MAX_TARGET_LENGTH]\n","       [--pad_to_max_length [PAD_TO_MAX_LENGTH]]\n","       [--max_train_samples MAX_TRAIN_SAMPLES]\n","       [--max_eval_samples MAX_EVAL_SAMPLES]\n","       [--max_predict_samples MAX_PREDICT_SAMPLES]\n","       [--num_beams NUM_BEAMS]\n","       [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]\n","       [--no_ignore_pad_token_for_loss]\n","       [--source_prefix SOURCE_PREFIX]\n","       [--forced_bos_token FORCED_BOS_TOKEN]\n","       --output_dir\n","       OUTPUT_DIR\n","       [--overwrite_output_dir [OVERWRITE_OUTPUT_DIR]]\n","       [--do_train [DO_TRAIN]]\n","       [--do_eval [DO_EVAL]]\n","       [--do_predict [DO_PREDICT]]\n","       [--evaluation_strategy {no,steps,epoch}]\n","       [--prediction_loss_only [PREDICTION_LOSS_ONLY]]\n","       [--per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE]\n","       [--per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE]\n","       [--per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE]\n","       [--per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE]\n","       [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n","       [--eval_accumulation_steps EVAL_ACCUMULATION_STEPS]\n","       [--eval_delay EVAL_DELAY]\n","       [--learning_rate LEARNING_RATE]\n","       [--weight_decay WEIGHT_DECAY]\n","       [--adam_beta1 ADAM_BETA1]\n","       [--adam_beta2 ADAM_BETA2]\n","       [--adam_epsilon ADAM_EPSILON]\n","       [--max_grad_norm MAX_GRAD_NORM]\n","       [--num_train_epochs NUM_TRAIN_EPOCHS]\n","       [--max_steps MAX_STEPS]\n","       [--lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}]\n","       [--warmup_ratio WARMUP_RATIO]\n","       [--warmup_steps WARMUP_STEPS]\n","       [--log_level {debug,info,warning,error,critical,passive}]\n","       [--log_level_replica {debug,info,warning,error,critical,passive}]\n","       [--log_on_each_node [LOG_ON_EACH_NODE]]\n","       [--no_log_on_each_node]\n","       [--logging_dir LOGGING_DIR]\n","       [--logging_strategy {no,steps,epoch}]\n","       [--logging_first_step [LOGGING_FIRST_STEP]]\n","       [--logging_steps LOGGING_STEPS]\n","       [--logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]]\n","       [--no_logging_nan_inf_filter]\n","       [--save_strategy {no,steps,epoch}]\n","       [--save_steps SAVE_STEPS]\n","       [--save_total_limit SAVE_TOTAL_LIMIT]\n","       [--save_on_each_node [SAVE_ON_EACH_NODE]]\n","       [--no_cuda [NO_CUDA]]\n","       [--use_mps_device [USE_MPS_DEVICE]]\n","       [--seed SEED]\n","       [--data_seed DATA_SEED]\n","       [--jit_mode_eval [JIT_MODE_EVAL]]\n","       [--use_ipex [USE_IPEX]]\n","       [--bf16 [BF16]]\n","       [--fp16 [FP16]]\n","       [--fp16_opt_level FP16_OPT_LEVEL]\n","       [--half_precision_backend {auto,cuda_amp,apex,cpu_amp}]\n","       [--bf16_full_eval [BF16_FULL_EVAL]]\n","       [--fp16_full_eval [FP16_FULL_EVAL]]\n","       [--tf32 TF32]\n","       [--local_rank LOCAL_RANK]\n","       [--xpu_backend {mpi,ccl,gloo}]\n","       [--tpu_num_cores TPU_NUM_CORES]\n","       [--tpu_metrics_debug [TPU_METRICS_DEBUG]]\n","       [--debug DEBUG]\n","       [--dataloader_drop_last [DATALOADER_DROP_LAST]]\n","       [--eval_steps EVAL_STEPS]\n","       [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n","       [--past_index PAST_INDEX]\n","       [--run_name RUN_NAME]\n","       [--disable_tqdm DISABLE_TQDM]\n","       [--remove_unused_columns [REMOVE_UNUSED_COLUMNS]]\n","       [--no_remove_unused_columns]\n","       [--label_names LABEL_NAMES [LABEL_NAMES ...]]\n","       [--load_best_model_at_end [LOAD_BEST_MODEL_AT_END]]\n","       [--metric_for_best_model METRIC_FOR_BEST_MODEL]\n","       [--greater_is_better GREATER_IS_BETTER]\n","       [--ignore_data_skip [IGNORE_DATA_SKIP]]\n","       [--sharded_ddp SHARDED_DDP]\n","       [--fsdp FSDP]\n","       [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n","       [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n","       [--deepspeed DEEPSPEED]\n","       [--label_smoothing_factor LABEL_SMOOTHING_FACTOR]\n","       [--optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,adamw_anyprecision,sgd,adagrad}]\n","       [--optim_args OPTIM_ARGS]\n","       [--adafactor [ADAFACTOR]]\n","       [--group_by_length [GROUP_BY_LENGTH]]\n","       [--length_column_name LENGTH_COLUMN_NAME]\n","       [--report_to REPORT_TO [REPORT_TO ...]]\n","       [--ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS]\n","       [--ddp_bucket_cap_mb DDP_BUCKET_CAP_MB]\n","       [--dataloader_pin_memory [DATALOADER_PIN_MEMORY]]\n","       [--no_dataloader_pin_memory]\n","       [--skip_memory_metrics [SKIP_MEMORY_METRICS]]\n","       [--no_skip_memory_metrics]\n","       [--use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]]\n","       [--push_to_hub [PUSH_TO_HUB]]\n","       [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n","       [--hub_model_id HUB_MODEL_ID]\n","       [--hub_strategy {end,every_save,checkpoint,all_checkpoints}]\n","       [--hub_token HUB_TOKEN]\n","       [--hub_private_repo [HUB_PRIVATE_REPO]]\n","       [--gradient_checkpointing [GRADIENT_CHECKPOINTING]]\n","       [--include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]]\n","       [--fp16_backend {auto,cuda_amp,apex,cpu_amp}]\n","       [--push_to_hub_model_id PUSH_TO_HUB_MODEL_ID]\n","       [--push_to_hub_organization PUSH_TO_HUB_ORGANIZATION]\n","       [--push_to_hub_token PUSH_TO_HUB_TOKEN]\n","       [--mp_parameters MP_PARAMETERS]\n","       [--auto_find_batch_size [AUTO_FIND_BATCH_SIZE]]\n","       [--full_determinism [FULL_DETERMINISM]]\n","       [--torchdynamo {eager,aot_eager,inductor,nvfuser,aot_nvfuser,aot_cudagraphs,ofi,fx2trt,onnxrt,ipex}]\n","       [--ray_scope RAY_SCOPE]\n","       [--ddp_timeout DDP_TIMEOUT]\n","       [--torch_compile [TORCH_COMPILE]]\n","       [--torch_compile_backend {eager,aot_eager,inductor,nvfuser,aot_nvfuser,aot_cudagraphs,ofi,fx2trt,onnxrt,ipex}]\n","       [--torch_compile_mode {default,reduce-overhead,max-autotune}]\n","       [--sortish_sampler [SORTISH_SAMPLER]]\n","       [--predict_with_generate [PREDICT_WITH_GENERATE]]\n","       [--generation_max_length GENERATION_MAX_LENGTH]\n","       [--generation_num_beams GENERATION_NUM_BEAMS]\n","\n","optional arguments:\n","  -h, --help\n","    show this\n","    help\n","    message and\n","    exit\n","  --model_name_or_path MODEL_NAME_OR_PATH\n","    Path to\n","    pretrained\n","    model or\n","    model\n","    identifier\n","    from huggin\n","    gface.co/mo\n","    dels\n","    (default:\n","    None)\n","  --config_name CONFIG_NAME\n","    Pretrained\n","    config name\n","    or path if\n","    not the\n","    same as\n","    model_name\n","    (default:\n","    None)\n","  --tokenizer_name TOKENIZER_NAME\n","    Pretrained\n","    tokenizer\n","    name or\n","    path if not\n","    the same as\n","    model_name\n","    (default:\n","    None)\n","  --cache_dir CACHE_DIR\n","    Where to\n","    store the\n","    pretrained\n","    models\n","    downloaded\n","    from huggin\n","    gface.co\n","    (default:\n","    None)\n","  --use_fast_tokenizer [USE_FAST_TOKENIZER]\n","    Whether to\n","    use one of\n","    the fast\n","    tokenizer\n","    (backed by\n","    the\n","    tokenizers\n","    library) or\n","    not.\n","    (default:\n","    True)\n","  --no_use_fast_tokenizer\n","    Whether to\n","    use one of\n","    the fast\n","    tokenizer\n","    (backed by\n","    the\n","    tokenizers\n","    library) or\n","    not.\n","    (default:\n","    False)\n","  --model_revision MODEL_REVISION\n","    The\n","    specific\n","    model\n","    version to\n","    use (can be\n","    a branch\n","    name, tag\n","    name or\n","    commit id).\n","    (default:\n","    main)\n","  --use_auth_token [USE_AUTH_TOKEN]\n","    Will use\n","    the token\n","    generated\n","    when\n","    running `hu\n","    ggingface-\n","    cli login`\n","    (necessary\n","    to use this\n","    script with\n","    private\n","    models).\n","    (default:\n","    False)\n","  --resize_position_embeddings RESIZE_POSITION_EMBEDDINGS\n","    Whether to \n","    automatical\n","    ly resize\n","    the\n","    position\n","    embeddings\n","    if `max_sou\n","    rce_length`\n","    exceeds the\n","    model's\n","    position\n","    embeddings.\n","    (default:\n","    None)\n","  --lang LANG\n","    Language id\n","    for summari\n","    zation.\n","    (default:\n","    None)\n","  --dataset_name DATASET_NAME\n","    The name of\n","    the dataset\n","    to use (via\n","    the\n","    datasets\n","    library).\n","    (default:\n","    None)\n","  --dataset_config_name DATASET_CONFIG_NAME\n","    The configu\n","    ration name\n","    of the\n","    dataset to\n","    use (via\n","    the\n","    datasets\n","    library).\n","    (default:\n","    None)\n","  --text_column TEXT_COLUMN\n","    The name of\n","    the column\n","    in the\n","    datasets\n","    containing\n","    the full\n","    texts (for \n","    summarizati\n","    on).\n","    (default:\n","    None)\n","  --summary_column SUMMARY_COLUMN\n","    The name of\n","    the column\n","    in the\n","    datasets\n","    containing\n","    the\n","    summaries\n","    (for summar\n","    ization).\n","    (default:\n","    None)\n","  --train_file TRAIN_FILE\n","    The input\n","    training\n","    data file\n","    (a\n","    jsonlines\n","    or csv\n","    file).\n","    (default:\n","    None)\n","  --validation_file VALIDATION_FILE\n","    An optional\n","    input\n","    evaluation\n","    data file\n","    to evaluate\n","    the metrics\n","    (rouge) on\n","    (a\n","    jsonlines\n","    or csv\n","    file).\n","    (default:\n","    None)\n","  --test_file TEST_FILE\n","    An optional\n","    input test\n","    data file\n","    to evaluate\n","    the metrics\n","    (rouge) on\n","    (a\n","    jsonlines\n","    or csv\n","    file).\n","    (default:\n","    None)\n","  --overwrite_cache [OVERWRITE_CACHE]\n","    Overwrite\n","    the cached\n","    training\n","    and\n","    evaluation\n","    sets\n","    (default:\n","    False)\n","  --preprocessing_num_workers PREPROCESSING_NUM_WORKERS\n","    The number\n","    of\n","    processes\n","    to use for\n","    the preproc\n","    essing.\n","    (default:\n","    None)\n","  --max_source_length MAX_SOURCE_LENGTH\n","    The maximum\n","    total input\n","    sequence\n","    length\n","    after token\n","    ization.\n","    Sequences\n","    longer than\n","    this will\n","    be\n","    truncated,\n","    sequences\n","    shorter\n","    will be\n","    padded.\n","    (default:\n","    1024)\n","  --max_target_length MAX_TARGET_LENGTH\n","    The maximum\n","    total\n","    sequence\n","    length for\n","    target text\n","    after token\n","    ization.\n","    Sequences\n","    longer than\n","    this will\n","    be\n","    truncated,\n","    sequences\n","    shorter\n","    will be\n","    padded.\n","    (default:\n","    128)\n","  --val_max_target_length VAL_MAX_TARGET_LENGTH\n","    The maximum\n","    total\n","    sequence\n","    length for\n","    validation\n","    target text\n","    after token\n","    ization.\n","    Sequences\n","    longer than\n","    this will\n","    be\n","    truncated,\n","    sequences\n","    shorter\n","    will be\n","    padded.\n","    Will\n","    default to \n","    `max_target\n","    _length`.Th\n","    is argument\n","    is also\n","    used to\n","    override\n","    the ``max_l\n","    ength``\n","    param of ``\n","    model.gener\n","    ate``,\n","    which is\n","    used during\n","    ``evaluate`\n","    ` and ``pre\n","    dict``.\n","    (default:\n","    None)\n","  --pad_to_max_length [PAD_TO_MAX_LENGTH]\n","    Whether to\n","    pad all\n","    samples to\n","    model\n","    maximum\n","    sentence\n","    length. If\n","    False, will\n","    pad the\n","    samples\n","    dynamically\n","    when\n","    batching to\n","    the maximum\n","    length in\n","    the batch.\n","    More\n","    efficient\n","    on GPU but\n","    very bad\n","    for TPU.\n","    (default:\n","    False)\n","  --max_train_samples MAX_TRAIN_SAMPLES\n","    For\n","    debugging\n","    purposes or\n","    quicker\n","    training,\n","    truncate\n","    the number\n","    of training\n","    examples to\n","    this value\n","    if set.\n","    (default:\n","    None)\n","  --max_eval_samples MAX_EVAL_SAMPLES\n","    For\n","    debugging\n","    purposes or\n","    quicker\n","    training,\n","    truncate\n","    the number\n","    of\n","    evaluation\n","    examples to\n","    this value\n","    if set.\n","    (default:\n","    None)\n","  --max_predict_samples MAX_PREDICT_SAMPLES\n","    For\n","    debugging\n","    purposes or\n","    quicker\n","    training,\n","    truncate\n","    the number\n","    of\n","    prediction\n","    examples to\n","    this value\n","    if set.\n","    (default:\n","    None)\n","  --num_beams NUM_BEAMS\n","    Number of\n","    beams to\n","    use for\n","    evaluation.\n","    This\n","    argument\n","    will be\n","    passed to `\n","    `model.gene\n","    rate``,\n","    which is\n","    used during\n","    ``evaluate`\n","    ` and ``pre\n","    dict``.\n","    (default:\n","    None)\n","  --ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]\n","    Whether to\n","    ignore the\n","    tokens corr\n","    esponding\n","    to padded\n","    labels in\n","    the loss\n","    computation\n","    or not.\n","    (default:\n","    True)\n","  --no_ignore_pad_token_for_loss\n","    Whether to\n","    ignore the\n","    tokens corr\n","    esponding\n","    to padded\n","    labels in\n","    the loss\n","    computation\n","    or not.\n","    (default:\n","    False)\n","  --source_prefix SOURCE_PREFIX\n","    A prefix to\n","    add before\n","    every\n","    source text\n","    (useful for\n","    T5 models).\n","    (default: )\n","  --forced_bos_token FORCED_BOS_TOKEN\n","    The token\n","    to force as\n","    the first\n","    generated\n","    token after\n","    the decoder\n","    _start_toke\n","    n_id.Useful\n","    for multili\n","    ngual\n","    models like\n","    mBART where\n","    the first\n","    generated\n","    tokenneeds\n","    to be the\n","    target\n","    language\n","    token\n","    (Usually it\n","    is the\n","    target\n","    language\n","    token)\n","    (default:\n","    None)\n","  --output_dir OUTPUT_DIR\n","    The output\n","    directory\n","    where the\n","    model\n","    predictions\n","    and\n","    checkpoints\n","    will be\n","    written.\n","    (default:\n","    None)\n","  --overwrite_output_dir [OVERWRITE_OUTPUT_DIR]\n","    Overwrite\n","    the content\n","    of the\n","    output\n","    directory.\n","    Use this to\n","    continue\n","    training if\n","    output_dir\n","    points to a\n","    checkpoint\n","    directory.\n","    (default:\n","    False)\n","  --do_train [DO_TRAIN]\n","    Whether to\n","    run\n","    training.\n","    (default:\n","    False)\n","  --do_eval [DO_EVAL]\n","    Whether to\n","    run eval on\n","    the dev\n","    set.\n","    (default:\n","    False)\n","  --do_predict [DO_PREDICT]\n","    Whether to\n","    run\n","    predictions\n","    on the test\n","    set.\n","    (default:\n","    False)\n","  --evaluation_strategy {no,steps,epoch}\n","    The\n","    evaluation\n","    strategy to\n","    use.\n","    (default:\n","    no)\n","  --prediction_loss_only [PREDICTION_LOSS_ONLY]\n","    When\n","    performing\n","    evaluation\n","    and predict\n","    ions, only\n","    returns the\n","    loss.\n","    (default:\n","    False)\n","  --per_device_train_batch_size PER_DEVICE_TRAIN_BATCH_SIZE\n","    Batch size\n","    per GPU/TPU\n","    core/CPU\n","    for\n","    training.\n","    (default:\n","    8)\n","  --per_device_eval_batch_size PER_DEVICE_EVAL_BATCH_SIZE\n","    Batch size\n","    per GPU/TPU\n","    core/CPU\n","    for\n","    evaluation.\n","    (default:\n","    8)\n","  --per_gpu_train_batch_size PER_GPU_TRAIN_BATCH_SIZE\n","    Deprecated,\n","    the use of \n","    `--per_devi\n","    ce_train_ba\n","    tch_size`\n","    is\n","    preferred.\n","    Batch size\n","    per GPU/TPU\n","    core/CPU\n","    for\n","    training.\n","    (default:\n","    None)\n","  --per_gpu_eval_batch_size PER_GPU_EVAL_BATCH_SIZE\n","    Deprecated,\n","    the use of \n","    `--per_devi\n","    ce_eval_bat\n","    ch_size` is\n","    preferred.\n","    Batch size\n","    per GPU/TPU\n","    core/CPU\n","    for\n","    evaluation.\n","    (default:\n","    None)\n","  --gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS\n","    Number of\n","    updates\n","    steps to\n","    accumulate\n","    before\n","    performing\n","    a backward/\n","    update\n","    pass.\n","    (default:\n","    1)\n","  --eval_accumulation_steps EVAL_ACCUMULATION_STEPS\n","    Number of\n","    predictions\n","    steps to\n","    accumulate\n","    before\n","    moving the\n","    tensors to\n","    the CPU.\n","    (default:\n","    None)\n","  --eval_delay EVAL_DELAY\n","    Number of\n","    epochs or\n","    steps to\n","    wait for\n","    before the\n","    first\n","    evaluation\n","    can be\n","    performed,\n","    depending\n","    on the eval\n","    uation_stra\n","    tegy.\n","    (default:\n","    0)\n","  --learning_rate LEARNING_RATE\n","    The initial\n","    learning\n","    rate for\n","    AdamW.\n","    (default:\n","    5e-05)\n","  --weight_decay WEIGHT_DECAY\n","    Weight\n","    decay for\n","    AdamW if we\n","    apply some.\n","    (default:\n","    0.0)\n","  --adam_beta1 ADAM_BETA1\n","    Beta1 for\n","    AdamW\n","    optimizer\n","    (default:\n","    0.9)\n","  --adam_beta2 ADAM_BETA2\n","    Beta2 for\n","    AdamW\n","    optimizer\n","    (default:\n","    0.999)\n","  --adam_epsilon ADAM_EPSILON\n","    Epsilon for\n","    AdamW\n","    optimizer.\n","    (default:\n","    1e-08)\n","  --max_grad_norm MAX_GRAD_NORM\n","    Max\n","    gradient\n","    norm.\n","    (default:\n","    1.0)\n","  --num_train_epochs NUM_TRAIN_EPOCHS\n","    Total\n","    number of\n","    training\n","    epochs to\n","    perform.\n","    (default:\n","    3.0)\n","  --max_steps MAX_STEPS\n","    If > 0: set\n","    total\n","    number of\n","    training\n","    steps to\n","    perform.\n","    Override nu\n","    m_train_epo\n","    chs.\n","    (default:\n","    -1)\n","  --lr_scheduler_type {linear,cosine,cosine_with_restarts,polynomial,constant,constant_with_warmup}\n","    The\n","    scheduler\n","    type to\n","    use.\n","    (default:\n","    linear)\n","  --warmup_ratio WARMUP_RATIO\n","    Linear\n","    warmup over\n","    warmup_rati\n","    o fraction\n","    of total\n","    steps.\n","    (default:\n","    0.0)\n","  --warmup_steps WARMUP_STEPS\n","    Linear\n","    warmup over\n","    warmup_step\n","    s.\n","    (default:\n","    0)\n","  --log_level {debug,info,warning,error,critical,passive}\n","    Logger log\n","    level to\n","    use on the\n","    main node.\n","    Possible\n","    choices are\n","    the log\n","    levels as\n","    strings:\n","    'debug',\n","    'info',\n","    'warning',\n","    'error' and\n","    'critical',\n","    plus a\n","    'passive'\n","    level which\n","    doesn't set\n","    anything\n","    and lets\n","    the\n","    application\n","    set the\n","    level.\n","    Defaults to\n","    'passive'.\n","    (default:\n","    passive)\n","  --log_level_replica {debug,info,warning,error,critical,passive}\n","    Logger log\n","    level to\n","    use on\n","    replica\n","    nodes. Same\n","    choices and\n","    defaults as\n","    ``log_level\n","    ``\n","    (default:\n","    passive)\n","  --log_on_each_node [LOG_ON_EACH_NODE]\n","    When doing\n","    a multinode\n","    distributed\n","    training,\n","    whether to\n","    log once\n","    per node or\n","    just once\n","    on the main\n","    node.\n","    (default:\n","    True)\n","  --no_log_on_each_node\n","    When doing\n","    a multinode\n","    distributed\n","    training,\n","    whether to\n","    log once\n","    per node or\n","    just once\n","    on the main\n","    node.\n","    (default:\n","    False)\n","  --logging_dir LOGGING_DIR\n","    Tensorboard\n","    log dir.\n","    (default:\n","    None)\n","  --logging_strategy {no,steps,epoch}\n","    The logging\n","    strategy to\n","    use.\n","    (default:\n","    steps)\n","  --logging_first_step [LOGGING_FIRST_STEP]\n","    Log the\n","    first\n","    global_step\n","    (default:\n","    False)\n","  --logging_steps LOGGING_STEPS\n","    Log every X\n","    updates\n","    steps.\n","    (default:\n","    500)\n","  --logging_nan_inf_filter [LOGGING_NAN_INF_FILTER]\n","    Filter nan\n","    and inf\n","    losses for\n","    logging.\n","    (default:\n","    True)\n","  --no_logging_nan_inf_filter\n","    Filter nan\n","    and inf\n","    losses for\n","    logging.\n","    (default:\n","    False)\n","  --save_strategy {no,steps,epoch}\n","    The\n","    checkpoint\n","    save\n","    strategy to\n","    use.\n","    (default:\n","    steps)\n","  --save_steps SAVE_STEPS\n","    Save\n","    checkpoint\n","    every X\n","    updates\n","    steps.\n","    (default:\n","    500)\n","  --save_total_limit SAVE_TOTAL_LIMIT\n","    Limit the\n","    total\n","    amount of c\n","    heckpoints.\n","    Deletes the\n","    older\n","    checkpoints\n","    in the\n","    output_dir.\n","    Default is\n","    unlimited\n","    checkpoints\n","    (default:\n","    None)\n","  --save_on_each_node [SAVE_ON_EACH_NODE]\n","    When doing\n","    multi-node\n","    distributed\n","    training,\n","    whether to\n","    save models\n","    and\n","    checkpoints\n","    on each\n","    node, or\n","    only on the\n","    main one\n","    (default:\n","    False)\n","  --no_cuda [NO_CUDA]\n","    Do not use\n","    CUDA even\n","    when it is\n","    available\n","    (default:\n","    False)\n","  --use_mps_device [USE_MPS_DEVICE]\n","    Whether to\n","    use Apple\n","    Silicon\n","    chip based\n","    `mps`\n","    device.\n","    (default:\n","    False)\n","  --seed SEED\n","    Random seed\n","    that will\n","    be set at\n","    the\n","    beginning\n","    of\n","    training.\n","    (default:\n","    42)\n","  --data_seed DATA_SEED\n","    Random seed\n","    to be used\n","    with data\n","    samplers.\n","    (default:\n","    None)\n","  --jit_mode_eval [JIT_MODE_EVAL]\n","    Whether or\n","    not to use\n","    PyTorch jit\n","    trace for\n","    inference\n","    (default:\n","    False)\n","  --use_ipex [USE_IPEX]\n","    Use Intel\n","    extension\n","    for PyTorch\n","    when it is\n","    available, \n","    installatio\n","    n: 'https:/\n","    /github.com\n","    /intel/inte\n","    l-\n","    extension-\n","    for-\n","    pytorch'\n","    (default:\n","    False)\n","  --bf16 [BF16]\n","    Whether to\n","    use bf16\n","    (mixed)\n","    precision\n","    instead of\n","    32-bit.\n","    Requires\n","    Ampere or\n","    higher\n","    NVIDIA arch\n","    itecture or\n","    using CPU\n","    (no_cuda).\n","    This is an \n","    experimenta\n","    l API and\n","    it may\n","    change.\n","    (default:\n","    False)\n","  --fp16 [FP16]\n","    Whether to\n","    use fp16\n","    (mixed)\n","    precision\n","    instead of\n","    32-bit\n","    (default:\n","    False)\n","  --fp16_opt_level FP16_OPT_LEVEL\n","    For fp16:\n","    Apex AMP op\n","    timization\n","    level\n","    selected in\n","    ['O0',\n","    'O1', 'O2',\n","    and 'O3'].\n","    See details\n","    at https://\n","    nvidia.gith\n","    ub.io/apex/\n","    amp.html\n","    (default:\n","    O1)\n","  --half_precision_backend {auto,cuda_amp,apex,cpu_amp}\n","    The backend\n","    to be used\n","    for half\n","    precision.\n","    (default:\n","    auto)\n","  --bf16_full_eval [BF16_FULL_EVAL]\n","    Whether to\n","    use full\n","    bfloat16\n","    evaluation\n","    instead of\n","    32-bit.\n","    This is an \n","    experimenta\n","    l API and\n","    it may\n","    change.\n","    (default:\n","    False)\n","  --fp16_full_eval [FP16_FULL_EVAL]\n","    Whether to\n","    use full\n","    float16\n","    evaluation\n","    instead of\n","    32-bit\n","    (default:\n","    False)\n","  --tf32 TF32\n","    Whether to\n","    enable tf32\n","    mode,\n","    available\n","    in Ampere\n","    and newer\n","    GPU archite\n","    ctures.\n","    This is an \n","    experimenta\n","    l API and\n","    it may\n","    change.\n","    (default:\n","    None)\n","  --local_rank LOCAL_RANK\n","    For\n","    distributed\n","    training:\n","    local_rank\n","    (default:\n","    -1)\n","  --xpu_backend {mpi,ccl,gloo}\n","    The backend\n","    to be used\n","    for\n","    distributed\n","    training on\n","    Intel XPU.\n","    (default:\n","    None)\n","  --tpu_num_cores TPU_NUM_CORES\n","    TPU: Number\n","    of TPU\n","    cores (auto\n","    matically\n","    passed by\n","    launcher\n","    script)\n","    (default:\n","    None)\n","  --tpu_metrics_debug [TPU_METRICS_DEBUG]\n","    Deprecated,\n","    the use of\n","    `--debug tp\n","    u_metrics_d\n","    ebug` is\n","    preferred.\n","    TPU:\n","    Whether to\n","    print debug\n","    metrics\n","    (default:\n","    False)\n","  --debug DEBUG\n","    Whether or\n","    not to\n","    enable\n","    debug mode.\n","    Current\n","    options: `u\n","    nderflow_ov\n","    erflow`\n","    (Detect\n","    underflow\n","    and\n","    overflow in\n","    activations\n","    and\n","    weights), `\n","    tpu_metrics\n","    _debug`\n","    (print\n","    debug\n","    metrics on\n","    TPU).\n","    (default: )\n","  --dataloader_drop_last [DATALOADER_DROP_LAST]\n","    Drop the\n","    last\n","    incomplete\n","    batch if it\n","    is not\n","    divisible\n","    by the\n","    batch size.\n","    (default:\n","    False)\n","  --eval_steps EVAL_STEPS\n","    Run an\n","    evaluation\n","    every X\n","    steps.\n","    (default:\n","    None)\n","  --dataloader_num_workers DATALOADER_NUM_WORKERS\n","    Number of s\n","    ubprocesses\n","    to use for\n","    data\n","    loading\n","    (PyTorch\n","    only). 0\n","    means that\n","    the data\n","    will be\n","    loaded in\n","    the main\n","    process.\n","    (default:\n","    0)\n","  --past_index PAST_INDEX\n","    If >=0,\n","    uses the co\n","    rresponding\n","    part of the\n","    output as\n","    the past\n","    state for\n","    next step.\n","    (default:\n","    -1)\n","  --run_name RUN_NAME\n","    An optional\n","    descriptor\n","    for the\n","    run.\n","    Notably\n","    used for\n","    wandb\n","    logging.\n","    (default:\n","    None)\n","  --disable_tqdm DISABLE_TQDM\n","    Whether or\n","    not to\n","    disable the\n","    tqdm\n","    progress\n","    bars.\n","    (default:\n","    None)\n","  --remove_unused_columns [REMOVE_UNUSED_COLUMNS]\n","    Remove\n","    columns not\n","    required by\n","    the model\n","    when using\n","    an nlp.Data\n","    set.\n","    (default:\n","    True)\n","  --no_remove_unused_columns\n","    Remove\n","    columns not\n","    required by\n","    the model\n","    when using\n","    an nlp.Data\n","    set.\n","    (default:\n","    False)\n","  --label_names LABEL_NAMES [LABEL_NAMES ...]\n","    The list of\n","    keys in\n","    your\n","    dictionary\n","    of inputs\n","    that\n","    correspond\n","    to the\n","    labels.\n","    (default:\n","    None)\n","  --load_best_model_at_end [LOAD_BEST_MODEL_AT_END]\n","    Whether or\n","    not to load\n","    the best\n","    model found\n","    during\n","    training at\n","    the end of\n","    training.\n","    (default:\n","    False)\n","  --metric_for_best_model METRIC_FOR_BEST_MODEL\n","    The metric\n","    to use to\n","    compare two\n","    different\n","    models.\n","    (default:\n","    None)\n","  --greater_is_better GREATER_IS_BETTER\n","    Whether the\n","    `metric_for\n","    _best_model\n","    ` should be\n","    maximized\n","    or not.\n","    (default:\n","    None)\n","  --ignore_data_skip [IGNORE_DATA_SKIP]\n","    When\n","    resuming\n","    training,\n","    whether or\n","    not to skip\n","    the first\n","    epochs and\n","    batches to\n","    get to the\n","    same\n","    training\n","    data.\n","    (default:\n","    False)\n","  --sharded_ddp SHARDED_DDP\n","    Whether or\n","    not to use\n","    sharded DDP\n","    training\n","    (in\n","    distributed\n","    training\n","    only). The\n","    base option\n","    should be\n","    `simple`,\n","    `zero_dp_2`\n","    or\n","    `zero_dp_3`\n","    and you can\n","    add CPU-\n","    offload to\n","    `zero_dp_2`\n","    or\n","    `zero_dp_3`\n","    like this:\n","    zero_dp_2\n","    offload` or\n","    `zero_dp_3\n","    offload`.\n","    You can add\n","    auto-wrap\n","    to\n","    `zero_dp_2`\n","    or\n","    `zero_dp_3`\n","    with the\n","    same\n","    syntax:\n","    zero_dp_2\n","    auto_wrap`\n","    or\n","    `zero_dp_3\n","    auto_wrap`.\n","    (default: )\n","  --fsdp FSDP\n","    Whether or\n","    not to use\n","    PyTorch\n","    Fully\n","    Sharded\n","    Data\n","    Parallel\n","    (FSDP)\n","    training\n","    (in\n","    distributed\n","    training\n","    only). The\n","    base option\n","    should be `\n","    full_shard`\n","    , `shard_gr\n","    ad_op` or\n","    `no_shard`\n","    and you can\n","    add CPU-\n","    offload to \n","    `full_shard\n","    ` or `shard\n","    _grad_op`\n","    like this:\n","    full_shard\n","    offload` or\n","    `shard_grad\n","    _op\n","    offload`.\n","    You can add\n","    auto-wrap\n","    to `full_sh\n","    ard` or `sh\n","    ard_grad_op\n","    ` with the\n","    same\n","    syntax:\n","    full_shard\n","    auto_wrap`\n","    or `shard_g\n","    rad_op\n","    auto_wrap`.\n","    (default: )\n","  --fsdp_min_num_params FSDP_MIN_NUM_PARAMS\n","    FSDP's\n","    minimum\n","    number of\n","    parameters\n","    for Default\n","    Auto\n","    Wrapping.\n","    (useful\n","    only when\n","    `fsdp`\n","    field is\n","    passed).\n","    (default:\n","    0)\n","  --fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP\n","    Transformer\n","    layer class\n","    name (case-\n","    sensitive)\n","    to wrap\n","    ,e.g, `Bert\n","    Layer`, `GP\n","    TJBlock`,\n","    `T5Block`\n","    ....\n","    (useful\n","    only when\n","    `fsdp` flag\n","    is passed).\n","    (default:\n","    None)\n","  --deepspeed DEEPSPEED\n","    Enable\n","    deepspeed\n","    and pass\n","    the path to\n","    deepspeed\n","    json config\n","    file (e.g. \n","    ds_config.j\n","    son) or an\n","    already\n","    loaded json\n","    file as a\n","    dict\n","    (default:\n","    None)\n","  --label_smoothing_factor LABEL_SMOOTHING_FACTOR\n","    The label\n","    smoothing\n","    epsilon to\n","    apply (zero\n","    means no\n","    label\n","    smoothing).\n","    (default:\n","    0.0)\n","  --optim {adamw_hf,adamw_torch,adamw_torch_xla,adamw_apex_fused,adafactor,adamw_bnb_8bit,adamw_anyprecision,sgd,adagrad}\n","    The\n","    optimizer\n","    to use.\n","    (default:\n","    adamw_hf)\n","  --optim_args OPTIM_ARGS\n","    Optional\n","    arguments\n","    to supply\n","    to\n","    optimizer.\n","    (default:\n","    None)\n","  --adafactor [ADAFACTOR]\n","    Whether or\n","    not to\n","    replace\n","    AdamW by\n","    Adafactor.\n","    (default:\n","    False)\n","  --group_by_length [GROUP_BY_LENGTH]\n","    Whether or\n","    not to\n","    group\n","    samples of\n","    roughly the\n","    same length\n","    together\n","    when\n","    batching.\n","    (default:\n","    False)\n","  --length_column_name LENGTH_COLUMN_NAME\n","    Column name\n","    with\n","    precomputed\n","    lengths to\n","    use when\n","    grouping by\n","    length.\n","    (default:\n","    length)\n","  --report_to REPORT_TO [REPORT_TO ...]\n","    The list of\n","    integration\n","    s to report\n","    the results\n","    and logs\n","    to.\n","    (default:\n","    None)\n","  --ddp_find_unused_parameters DDP_FIND_UNUSED_PARAMETERS\n","    When using\n","    distributed\n","    training,\n","    the value\n","    of the flag\n","    `find_unuse\n","    d_parameter\n","    s` passed\n","    to `Distrib\n","    utedDataPar\n","    allel`.\n","    (default:\n","    None)\n","  --ddp_bucket_cap_mb DDP_BUCKET_CAP_MB\n","    When using\n","    distributed\n","    training,\n","    the value\n","    of the flag\n","    `bucket_cap\n","    _mb` passed\n","    to `Distrib\n","    utedDataPar\n","    allel`.\n","    (default:\n","    None)\n","  --dataloader_pin_memory [DATALOADER_PIN_MEMORY]\n","    Whether or\n","    not to pin\n","    memory for\n","    DataLoader.\n","    (default:\n","    True)\n","  --no_dataloader_pin_memory\n","    Whether or\n","    not to pin\n","    memory for\n","    DataLoader.\n","    (default:\n","    False)\n","  --skip_memory_metrics [SKIP_MEMORY_METRICS]\n","    Whether or\n","    not to skip\n","    adding of\n","    memory\n","    profiler\n","    reports to\n","    metrics.\n","    (default:\n","    True)\n","  --no_skip_memory_metrics\n","    Whether or\n","    not to skip\n","    adding of\n","    memory\n","    profiler\n","    reports to\n","    metrics.\n","    (default:\n","    False)\n","  --use_legacy_prediction_loop [USE_LEGACY_PREDICTION_LOOP]\n","    Whether or\n","    not to use\n","    the legacy \n","    prediction_\n","    loop in the\n","    Trainer.\n","    (default:\n","    False)\n","  --push_to_hub [PUSH_TO_HUB]\n","    Whether or\n","    not to\n","    upload the\n","    trained\n","    model to\n","    the model\n","    hub after\n","    training.\n","    (default:\n","    False)\n","  --resume_from_checkpoint RESUME_FROM_CHECKPOINT\n","    The path to\n","    a folder\n","    with a\n","    valid\n","    checkpoint\n","    for your\n","    model.\n","    (default:\n","    None)\n","  --hub_model_id HUB_MODEL_ID\n","    The name of\n","    the\n","    repository\n","    to keep in\n","    sync with\n","    the local `\n","    output_dir`\n","    . (default:\n","    None)\n","  --hub_strategy {end,every_save,checkpoint,all_checkpoints}\n","    The hub\n","    strategy to\n","    use when `-\n","    -push_to_hu\n","    b` is\n","    activated.\n","    (default:\n","    every_save)\n","  --hub_token HUB_TOKEN\n","    The token\n","    to use to\n","    push to the\n","    Model Hub.\n","    (default:\n","    None)\n","  --hub_private_repo [HUB_PRIVATE_REPO]\n","    Whether the\n","    model\n","    repository\n","    is private\n","    or not.\n","    (default:\n","    False)\n","  --gradient_checkpointing [GRADIENT_CHECKPOINTING]\n","    If True,\n","    use\n","    gradient ch\n","    eckpointing\n","    to save\n","    memory at\n","    the expense\n","    of slower\n","    backward\n","    pass.\n","    (default:\n","    False)\n","  --include_inputs_for_metrics [INCLUDE_INPUTS_FOR_METRICS]\n","    Whether or\n","    not the\n","    inputs will\n","    be passed\n","    to the `com\n","    pute_metric\n","    s`\n","    function.\n","    (default:\n","    False)\n","  --fp16_backend {auto,cuda_amp,apex,cpu_amp}\n","    Deprecated.\n","    Use half_pr\n","    ecision_bac\n","    kend\n","    instead\n","    (default:\n","    auto)\n","  --push_to_hub_model_id PUSH_TO_HUB_MODEL_ID\n","    The name of\n","    the\n","    repository\n","    to which\n","    push the\n","    `Trainer`.\n","    (default:\n","    None)\n","  --push_to_hub_organization PUSH_TO_HUB_ORGANIZATION\n","    The name of\n","    the organiz\n","    ation in\n","    with to\n","    which push\n","    the\n","    `Trainer`.\n","    (default:\n","    None)\n","  --push_to_hub_token PUSH_TO_HUB_TOKEN\n","    The token\n","    to use to\n","    push to the\n","    Model Hub.\n","    (default:\n","    None)\n","  --mp_parameters MP_PARAMETERS\n","    Used by the\n","    SageMaker\n","    launcher to\n","    send mp-\n","    specific\n","    args.\n","    Ignored in\n","    Trainer\n","    (default: )\n","  --auto_find_batch_size [AUTO_FIND_BATCH_SIZE]\n","    Whether to \n","    automatical\n","    ly decrease\n","    the batch\n","    size in\n","    half and\n","    rerun the\n","    training\n","    loop again\n","    each time a\n","    CUDA Out-\n","    of-Memory\n","    was reached\n","    (default:\n","    False)\n","  --full_determinism [FULL_DETERMINISM]\n","    Whether to\n","    call enable\n","    _full_deter\n","    minism\n","    instead of\n","    set_seed\n","    for reprodu\n","    cibility in\n","    distributed\n","    training\n","    (default:\n","    False)\n","  --torchdynamo {eager,aot_eager,inductor,nvfuser,aot_nvfuser,aot_cudagraphs,ofi,fx2trt,onnxrt,ipex}\n","    This\n","    argument is\n","    deprecated,\n","    use `--torc\n","    h_compile_b\n","    ackend`\n","    instead.\n","    (default:\n","    None)\n","  --ray_scope RAY_SCOPE\n","    The scope\n","    to use when\n","    doing hyper\n","    parameter\n","    search with\n","    Ray. By\n","    default,\n","    `\"last\"`\n","    will be\n","    used. Ray\n","    will then\n","    use the\n","    last\n","    checkpoint\n","    of all\n","    trials,\n","    compare\n","    those, and\n","    select the\n","    best one.\n","    However,\n","    other\n","    options are\n","    also\n","    available.\n","    See the Ray\n","    documentati\n","    on (https:/\n","    /docs.ray.i\n","    o/en/latest\n","    /tune/api_d\n","    ocs/analysi\n","    s.html#ray.\n","    tune.Experi\n","    mentAnalysi\n","    s.get_best_\n","    trial) for\n","    more\n","    options.\n","    (default:\n","    last)\n","  --ddp_timeout DDP_TIMEOUT\n","    Overrides\n","    the default\n","    timeout for\n","    distributed\n","    training\n","    (value\n","    should be\n","    given in\n","    seconds).\n","    (default:\n","    1800)\n","  --torch_compile [TORCH_COMPILE]\n","    If set to\n","    `True`, the\n","    model will\n","    be wrapped\n","    in `torch.c\n","    ompile`.\n","    (default:\n","    False)\n","  --torch_compile_backend {eager,aot_eager,inductor,nvfuser,aot_nvfuser,aot_cudagraphs,ofi,fx2trt,onnxrt,ipex}\n","    Which\n","    backend to\n","    use with `t\n","    orch.compil\n","    e`, passing\n","    one will\n","    trigger a\n","    model compi\n","    lation.\n","    (default:\n","    None)\n","  --torch_compile_mode {default,reduce-overhead,max-autotune}\n","    Which mode\n","    to use with\n","    `torch.comp\n","    ile`,\n","    passing one\n","    will\n","    trigger a\n","    model compi\n","    lation.\n","    (default:\n","    None)\n","  --sortish_sampler [SORTISH_SAMPLER]\n","    Whether to\n","    use Sortish\n","    Sampler or\n","    not.\n","    (default:\n","    False)\n","  --predict_with_generate [PREDICT_WITH_GENERATE]\n","    Whether to\n","    use\n","    generate to\n","    calculate\n","    generative\n","    metrics\n","    (ROUGE,\n","    BLEU).\n","    (default:\n","    False)\n","  --generation_max_length GENERATION_MAX_LENGTH\n","    The `max_le\n","    ngth` to\n","    use on each\n","    evaluation\n","    loop when `\n","    predict_wit\n","    h_generate=\n","    True`. Will\n","    default to\n","    the `max_le\n","    ngth` value\n","    of the\n","    model confi\n","    guration.\n","    (default:\n","    None)\n","  --generation_num_beams GENERATION_NUM_BEAMS\n","    The\n","    `num_beams`\n","    to use on\n","    each\n","    evaluation\n","    loop when `\n","    predict_wit\n","    h_generate=\n","    True`. Will\n","    default to\n","    the\n","    `num_beams`\n","    value of\n","    the model c\n","    onfiguratio\n","    n.\n","    (default:\n","    None)\n"]}],"source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --help"]},{"cell_type":"markdown","metadata":{"id":"C7P5PuioHByl"},"source":["### Vào đây để lấy tên model mình thích nè\n","https://huggingface.co/models?pipeline_tag=summarization&sort=downloads"]},{"cell_type":"markdown","source":["## Đọc hướng dẫn sử dụng ở đây nè\n","- Muốn train và đánh giá kết quả trên tập val, test thì chạy code \n","```\n","!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"tên model lấy ở link trên\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"***dẫn đường link vào drive đồ án nhóm luôn để sau khỏi move vào nghen***\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --gradient_accumulation_steps 8 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 500 \\\n","    --lang \"vi_VN\"\n","  ```\n","\n","  * Batch size ở đây là bằng ***per_device_train_batch_size*** * ***gradient_accumulation_steps***\n","  * Cái ***per_device_train_batch_size*** nếu setup cao thì bị tràn ram, nên set up tầm vừa vừa khoản 2, 4 hay 8 gì thôi\n","  * ***per_device_eval_batch_size*** thì nên set up tầm 16, cái này là batch_size cho lúc đánh giá, set cao hơn cái kia tí chớ không time eval lâu như time train á\n","  * Còn cái ***gradient_accumulation_steps*** nó làm giảm số step khi train, cái này thì nên set tầm 8 hoặc 16\n","  * Còn cái ***--lang \"vi_VN\"*** thì  chỉ sử dụng cho mô hình muilti Language (mấy mô hình có chữ \"m\" phía trước như mBART á)"],"metadata":{"id":"PhBUHdqPFCJi"}},{"cell_type":"markdown","source":["- Giả sử theo cái code trên luôn, muốn train tiếp khi bị mất kết nối thì: \n","```\n","!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"Dẫn link checkpoint vào đây/checkpoint-6996/\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mBART-Large/\" \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --gradient_accumulation_steps 8 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 500 \\\n","    --lang \"vi_VN\"\n","```\n","\n","  * Phải xoá dòng ***--overwrite_output_dir***\n","  * Dẫn ***output_dir*** vào drive có chứa checkpoint\n","  * Dẫn checkpoint vào ***--model_name_or_path***"],"metadata":{"id":"kIjokvQbGgCQ"}},{"cell_type":"markdown","source":["- Đang train xong, chưa kịp đánh giá trên tập val và test thì có thể đánh giá sau với code:\n","```\n","!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"Dẫn link model vào đây (model chứ không phải checkpoint)\" \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Bart-large-cnn\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size=16 \\\n","    --per_device_eval_batch_size=16 \\\n","    --gradient_accumulation_steps 16 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 500 \n","```\n","\n"," * Nếu chưa đánh giá trên val, test luôn thì để \n"," ```\n","     --do_eval \\\n","     --do_predict \\\n"," ```\n","\n"," * Nếu đánh giá trên val rồi, test chưa thì để\n"," ```\n","     --do_predict \\\n"," ```\n","\n"," "],"metadata":{"id":"5spKaLJgJAlI"}},{"cell_type":"markdown","source":["- Muốn xem kết quả model in ra thì tìm cái file tên là ***generated_predictions.txt***\n","- Còn muốn xem từ custom data thì code dưới này:\n","\n","```\n","text_example = \"\"\"\n","  câu 1. câu 2. câu 3...\n","\"\"\"\n","\n","import torch\n","from transformers import pipeline\n","CHECKPOINT = \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mBART-Large\"\n","def get_pipeline(checkpoint=CHECKPOINT):\n","    model_checkpoint = checkpoint\n","    nlp = pipeline('summarization', model=model_checkpoint,\n","                      tokenizer=model_checkpoint)\n","    return nlp\n","nlp_pipeline = get_pipeline(CHECKPOINT)\n","with torch.no_grad():\n","    res = nlp_pipeline(text_example)\n","    print(res)\n","```"],"metadata":{"id":"N4EndJvrK9Po"}},{"cell_type":"markdown","source":["# Train mT5-small e3"],"metadata":{"id":"mQxAMeSBpUPA"}},{"cell_type":"code","source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"google/mt5-small\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-small-e3/\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size 4 \\\n","    --per_device_eval_batch_size 16 \\\n","    --gradient_accumulation_steps 8 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 450 \\\n","    --lang \"vi_VN\""],"metadata":{"id":"yufLUs0epT66"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JfQPQHHVF3S4"},"source":["# Train T5-small e3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1729800,"status":"ok","timestamp":1670665184917,"user":{"displayName":"Duy Ngọc Cao","userId":"09742726668153199714"},"user_tz":-420},"id":"2VWXQqynGaY1","outputId":"06ec332d-d2ce-457d-add3-97503a541197"},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=4,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=passive,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64/runs/Dec10_09-10-59_48144b9c5bc1,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=1.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=32,\n","per_device_train_batch_size=32,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64,\n","save_on_each_node=False,\n","save_steps=1500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","WARNING:datasets.builder:Using custom data configuration default-79d6ab05104a01e0\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n","INFO:datasets.builder:Overwrite dataset info from restored data version.\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","100% 3/3 [00:00<00:00, 245.50it/s]\n","[INFO|configuration_utils.py:654] 2022-12-10 09:11:03,059 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/config.json\n","[INFO|configuration_utils.py:706] 2022-12-10 09:11:03,063 >> Model config T5Config {\n","  \"_name_or_path\": \"t5-small\",\n","  \"architectures\": [\n","    \"T5WithLMHeadModel\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 6,\n","  \"num_heads\": 8,\n","  \"num_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","[INFO|tokenization_auto.py:450] 2022-12-10 09:11:03,978 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:654] 2022-12-10 09:11:04,929 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/config.json\n","[INFO|configuration_utils.py:706] 2022-12-10 09:11:04,930 >> Model config T5Config {\n","  \"_name_or_path\": \"t5-small\",\n","  \"architectures\": [\n","    \"T5WithLMHeadModel\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 6,\n","  \"num_heads\": 8,\n","  \"num_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 09:11:06,758 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/spiece.model\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 09:11:06,758 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/tokenizer.json\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 09:11:06,758 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 09:11:06,758 >> loading file special_tokens_map.json from cache at None\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 09:11:06,758 >> loading file tokenizer_config.json from cache at None\n","[INFO|configuration_utils.py:654] 2022-12-10 09:11:06,758 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/config.json\n","[INFO|configuration_utils.py:706] 2022-12-10 09:11:06,760 >> Model config T5Config {\n","  \"_name_or_path\": \"t5-small\",\n","  \"architectures\": [\n","    \"T5WithLMHeadModel\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 512,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"relu\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"relu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": false,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"t5\",\n","  \"n_positions\": 512,\n","  \"num_decoder_layers\": 6,\n","  \"num_heads\": 8,\n","  \"num_layers\": 6,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"max_length\": 200,\n","      \"min_length\": 30,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4,\n","      \"prefix\": \"summarize: \"\n","    },\n","    \"translation_en_to_de\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to German: \"\n","    },\n","    \"translation_en_to_fr\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to French: \"\n","    },\n","    \"translation_en_to_ro\": {\n","      \"early_stopping\": true,\n","      \"max_length\": 300,\n","      \"num_beams\": 4,\n","      \"prefix\": \"translate English to Romanian: \"\n","    }\n","  },\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32128\n","}\n","\n","/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n","For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n","- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n","- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n","- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n","  warnings.warn(\n","[INFO|modeling_utils.py:2238] 2022-12-10 09:11:06,841 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/d78aea13fa7ecd06c29e3e46195d6341255065d5/pytorch_model.bin\n","[INFO|modeling_utils.py:2742] 2022-12-10 09:11:07,936 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:2750] 2022-12-10 09:11:07,937 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n","Running tokenizer on train dataset:   0% 0/30 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ce0a9e08ba97c627.arrow\n","Running tokenizer on train dataset: 100% 30/30 [01:00<00:00,  2.02s/ba]\n","Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-5cd5f056f86f2886.arrow\n","Running tokenizer on validation dataset: 100% 4/4 [00:06<00:00,  1.66s/ba]\n","Running tokenizer on prediction dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c6b813a2e181ba96.arrow\n","Running tokenizer on prediction dataset: 100% 4/4 [00:06<00:00,  1.64s/ba]\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1642] 2022-12-10 09:12:25,205 >> ***** Running training *****\n","[INFO|trainer.py:1643] 2022-12-10 09:12:25,205 >>   Num examples = 29893\n","[INFO|trainer.py:1644] 2022-12-10 09:12:25,205 >>   Num Epochs = 1\n","[INFO|trainer.py:1645] 2022-12-10 09:12:25,205 >>   Instantaneous batch size per device = 32\n","[INFO|trainer.py:1646] 2022-12-10 09:12:25,206 >>   Total train batch size (w. parallel, distributed & accumulation) = 128\n","[INFO|trainer.py:1647] 2022-12-10 09:12:25,206 >>   Gradient Accumulation steps = 4\n","[INFO|trainer.py:1648] 2022-12-10 09:12:25,206 >>   Total optimization steps = 233\n","[INFO|trainer.py:1649] 2022-12-10 09:12:25,206 >>   Number of trainable parameters = 60506624\n","  0% 0/233 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-12-10 09:12:25,243 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","{'loss': 2.758, 'learning_rate': 2.8540772532188842e-05, 'epoch': 0.43}\n","{'loss': 2.476, 'learning_rate': 7.0815450643776825e-06, 'epoch': 0.86}\n","100% 233/233 [15:35<00:00,  4.02s/it][INFO|trainer.py:1893] 2022-12-10 09:28:00,649 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","{'train_runtime': 935.4497, 'train_samples_per_second': 31.956, 'train_steps_per_second': 0.249, 'train_loss': 2.5917057151958156, 'epoch': 1.0}\n","100% 233/233 [15:35<00:00,  4.01s/it]\n","[INFO|trainer.py:2701] 2022-12-10 09:28:00,671 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64\n","[INFO|configuration_utils.py:447] 2022-12-10 09:28:00,679 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64/config.json\n","[INFO|modeling_utils.py:1671] 2022-12-10 09:28:01,410 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-10 09:28:01,416 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-10 09:28:01,420 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-10 09:28:01,467 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64/spiece.model\n","***** train metrics *****\n","  epoch                    =        1.0\n","  train_loss               =     2.5917\n","  train_runtime            = 0:15:35.44\n","  train_samples            =      29893\n","  train_samples_per_second =     31.956\n","  train_steps_per_second   =      0.249\n","INFO:__main__:*** Evaluate ***\n","[INFO|trainer.py:2956] 2022-12-10 09:28:01,514 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2958] 2022-12-10 09:28:01,514 >>   Num examples = 3737\n","[INFO|trainer.py:2961] 2022-12-10 09:28:01,514 >>   Batch size = 32\n","100% 117/117 [05:42<00:00,  2.93s/it]\n","***** eval metrics *****\n","  epoch                   =        1.0\n","  eval_gen_len            =   248.1148\n","  eval_loss               =     2.2761\n","  eval_rouge1             =    31.0625\n","  eval_rouge2             =    10.7755\n","  eval_rougeL             =    19.9979\n","  eval_rougeLsum          =    23.9697\n","  eval_runtime            = 0:05:45.57\n","  eval_samples            =       3737\n","  eval_samples_per_second =     10.814\n","  eval_steps_per_second   =      0.339\n","INFO:__main__:*** Predict ***\n","[INFO|trainer.py:2956] 2022-12-10 09:33:47,108 >> ***** Running Prediction *****\n","[INFO|trainer.py:2958] 2022-12-10 09:33:47,108 >>   Num examples = 3737\n","[INFO|trainer.py:2961] 2022-12-10 09:33:47,109 >>   Batch size = 32\n","100% 117/117 [05:51<00:00,  3.00s/it]\n","***** predict metrics *****\n","  predict_gen_len            =   247.6604\n","  predict_loss               =     2.2764\n","  predict_rouge1             =     30.912\n","  predict_rouge2             =    10.6671\n","  predict_rougeL             =    19.8409\n","  predict_rougeLsum          =     23.786\n","  predict_runtime            = 0:05:54.15\n","  predict_samples            =       3737\n","  predict_samples_per_second =     10.552\n","  predict_steps_per_second   =       0.33\n","[INFO|modelcard.py:449] 2022-12-10 09:39:42,393 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 31.0625}]}\n"]}],"source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"t5-small\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/T5-small/s1025_t64\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size=32 \\\n","    --per_device_eval_batch_size=32 \\\n","    --gradient_accumulation_steps 4 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 1 \\\n","    --save_steps 1500 \\\n","    --max_source_length 1024 \\\n","    --max_target_length 256"]},{"cell_type":"markdown","source":["# Train mT5-base e3"],"metadata":{"id":"CfnOs6WTVh79"}},{"cell_type":"code","source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"google/mt5-base\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size=2 \\\n","    --per_device_eval_batch_size=8 \\\n","    --gradient_accumulation_steps 4 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 1000 \\\n","    --max_source_length 700 \\\n","    --max_target_length 64"],"metadata":{"id":"zaOO71WYVk47","colab":{"base_uri":"https://localhost:8080/"},"outputId":"64ff6f0a-c553-4ae5-fd1b-edf187091bd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=4,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=passive,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/runs/Dec14_13-31-08_affad6003026,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=2,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3,\n","save_on_each_node=False,\n","save_steps=1000,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","WARNING:datasets.builder:Using custom data configuration default-1d0dc81c08029efb\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n","INFO:datasets.builder:Overwrite dataset info from restored data version.\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","100% 3/3 [00:00<00:00, 850.08it/s]\n","[INFO|configuration_utils.py:654] 2022-12-14 13:31:12,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/config.json\n","[INFO|configuration_utils.py:706] 2022-12-14 13:31:12,062 >> Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-base\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","[INFO|configuration_utils.py:654] 2022-12-14 13:31:12,993 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/config.json\n","[INFO|configuration_utils.py:706] 2022-12-14 13:31:12,993 >> Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-base\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","[INFO|tokenization_utils_base.py:1799] 2022-12-14 13:31:12,994 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/spiece.model\n","[INFO|tokenization_utils_base.py:1799] 2022-12-14 13:31:12,994 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1799] 2022-12-14 13:31:12,994 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1799] 2022-12-14 13:31:12,994 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1799] 2022-12-14 13:31:12,994 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/tokenizer_config.json\n","[INFO|configuration_utils.py:654] 2022-12-14 13:31:12,995 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/config.json\n","[INFO|configuration_utils.py:706] 2022-12-14 13:31:12,995 >> Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-base\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","[INFO|configuration_utils.py:654] 2022-12-14 13:31:13,399 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/config.json\n","[INFO|configuration_utils.py:706] 2022-12-14 13:31:13,400 >> Model config MT5Config {\n","  \"_name_or_path\": \"google/mt5-base\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n","  warnings.warn(\n","[INFO|modeling_utils.py:2251] 2022-12-14 13:31:14,037 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/pytorch_model.bin\n","[INFO|modeling_utils.py:2806] 2022-12-14 13:31:21,368 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:2814] 2022-12-14 13:31:21,368 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-base.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-093294aa24510bf3.arrow\n","Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-507fbcd795234641.arrow\n","Running tokenizer on validation dataset: 100% 4/4 [00:06<00:00,  1.75s/ba]\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-8645f6fa2f73d07e.arrow\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1641] 2022-12-14 13:31:34,635 >> ***** Running training *****\n","[INFO|trainer.py:1642] 2022-12-14 13:31:34,635 >>   Num examples = 29893\n","[INFO|trainer.py:1643] 2022-12-14 13:31:34,635 >>   Num Epochs = 3\n","[INFO|trainer.py:1644] 2022-12-14 13:31:34,635 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1645] 2022-12-14 13:31:34,635 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1646] 2022-12-14 13:31:34,635 >>   Gradient Accumulation steps = 4\n","[INFO|trainer.py:1647] 2022-12-14 13:31:34,635 >>   Total optimization steps = 11208\n","[INFO|trainer.py:1648] 2022-12-14 13:31:34,637 >>   Number of trainable parameters = 582401280\n","  0% 0/11208 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-12-14 13:31:34,679 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","{'loss': 7.7574, 'learning_rate': 4.9553890078515346e-05, 'epoch': 0.03}\n","{'loss': 2.7746, 'learning_rate': 4.9107780157030696e-05, 'epoch': 0.05}\n","{'loss': 2.4388, 'learning_rate': 4.866167023554604e-05, 'epoch': 0.08}\n","{'loss': 2.3174, 'learning_rate': 4.821556031406139e-05, 'epoch': 0.11}\n","{'loss': 2.2347, 'learning_rate': 4.776945039257673e-05, 'epoch': 0.13}\n","{'loss': 2.215, 'learning_rate': 4.732334047109208e-05, 'epoch': 0.16}\n","{'loss': 2.2211, 'learning_rate': 4.6877230549607426e-05, 'epoch': 0.19}\n","{'loss': 2.1504, 'learning_rate': 4.643112062812277e-05, 'epoch': 0.21}\n","{'loss': 2.1671, 'learning_rate': 4.598501070663811e-05, 'epoch': 0.24}\n","{'loss': 2.1104, 'learning_rate': 4.553890078515346e-05, 'epoch': 0.27}\n","  9% 1000/11208 [38:36<6:35:23,  2.32s/it][INFO|trainer.py:2700] 2022-12-14 14:10:11,288 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-1000\n","[INFO|configuration_utils.py:447] 2022-12-14 14:10:11,296 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1680] 2022-12-14 14:10:21,535 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-14 14:10:21,542 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-14 14:10:21,547 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-1000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-14 14:10:21,890 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-1000/spiece.model\n","{'loss': 2.0856, 'learning_rate': 4.509279086366881e-05, 'epoch': 0.29}\n","{'loss': 2.0665, 'learning_rate': 4.4646680942184155e-05, 'epoch': 0.32}\n","{'loss': 2.0466, 'learning_rate': 4.4200571020699506e-05, 'epoch': 0.35}\n","{'loss': 2.0306, 'learning_rate': 4.375446109921485e-05, 'epoch': 0.37}\n","{'loss': 2.0271, 'learning_rate': 4.330835117773019e-05, 'epoch': 0.4}\n","{'loss': 2.0403, 'learning_rate': 4.286224125624554e-05, 'epoch': 0.43}\n","{'loss': 2.0501, 'learning_rate': 4.241613133476089e-05, 'epoch': 0.45}\n","{'loss': 1.9959, 'learning_rate': 4.1970021413276235e-05, 'epoch': 0.48}\n","{'loss': 1.9679, 'learning_rate': 4.152391149179158e-05, 'epoch': 0.51}\n","{'loss': 1.9641, 'learning_rate': 4.107780157030692e-05, 'epoch': 0.54}\n"," 18% 2000/11208 [1:17:42<5:54:45,  2.31s/it][INFO|trainer.py:2700] 2022-12-14 14:49:17,304 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-2000\n","[INFO|configuration_utils.py:447] 2022-12-14 14:49:17,313 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-2000/config.json\n","[INFO|modeling_utils.py:1680] 2022-12-14 14:49:27,169 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-2000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-14 14:49:27,180 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-2000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-14 14:49:27,189 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-2000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-14 14:49:27,772 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-2000/spiece.model\n","{'loss': 1.9542, 'learning_rate': 4.063169164882227e-05, 'epoch': 0.56}\n","{'loss': 1.9607, 'learning_rate': 4.0185581727337615e-05, 'epoch': 0.59}\n","{'loss': 1.9295, 'learning_rate': 3.9739471805852965e-05, 'epoch': 0.62}\n","{'loss': 1.9565, 'learning_rate': 3.9293361884368315e-05, 'epoch': 0.64}\n","{'loss': 1.9421, 'learning_rate': 3.884725196288366e-05, 'epoch': 0.67}\n","{'loss': 1.9035, 'learning_rate': 3.8401142041399e-05, 'epoch': 0.7}\n","{'loss': 1.8649, 'learning_rate': 3.7955032119914345e-05, 'epoch': 0.72}\n","{'loss': 1.9064, 'learning_rate': 3.7508922198429695e-05, 'epoch': 0.75}\n","{'loss': 1.8995, 'learning_rate': 3.706281227694504e-05, 'epoch': 0.78}\n","{'loss': 1.8988, 'learning_rate': 3.661670235546039e-05, 'epoch': 0.8}\n"," 27% 3000/11208 [1:56:46<5:15:56,  2.31s/it][INFO|trainer.py:2700] 2022-12-14 15:28:21,132 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-3000\n","[INFO|configuration_utils.py:447] 2022-12-14 15:28:21,139 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-3000/config.json\n","[INFO|modeling_utils.py:1680] 2022-12-14 15:28:31,165 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-3000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-14 15:28:31,171 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-3000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-14 15:28:31,177 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-3000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-14 15:28:31,595 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-3000/spiece.model\n","{'loss': 1.9125, 'learning_rate': 3.617059243397573e-05, 'epoch': 0.83}\n","{'loss': 1.8828, 'learning_rate': 3.572448251249108e-05, 'epoch': 0.86}\n","{'loss': 1.8877, 'learning_rate': 3.5278372591006425e-05, 'epoch': 0.88}\n","{'loss': 1.802, 'learning_rate': 3.483226266952177e-05, 'epoch': 0.91}\n","{'loss': 1.8623, 'learning_rate': 3.438615274803712e-05, 'epoch': 0.94}\n","{'loss': 1.857, 'learning_rate': 3.394004282655247e-05, 'epoch': 0.96}\n","{'loss': 1.869, 'learning_rate': 3.349393290506781e-05, 'epoch': 0.99}\n","{'loss': 1.813, 'learning_rate': 3.3047822983583155e-05, 'epoch': 1.02}\n","{'loss': 1.8396, 'learning_rate': 3.2601713062098505e-05, 'epoch': 1.04}\n","{'loss': 1.7933, 'learning_rate': 3.215560314061385e-05, 'epoch': 1.07}\n"," 36% 4000/11208 [2:35:50<4:38:09,  2.32s/it][INFO|trainer.py:2700] 2022-12-14 16:07:25,008 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-4000\n","[INFO|configuration_utils.py:447] 2022-12-14 16:07:25,018 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-4000/config.json\n","[INFO|modeling_utils.py:1680] 2022-12-14 16:07:34,798 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-4000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-14 16:07:34,804 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-4000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-14 16:07:34,811 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-4000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-14 16:07:35,169 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-4000/spiece.model\n","{'loss': 1.7986, 'learning_rate': 3.170949321912919e-05, 'epoch': 1.1}\n","{'loss': 1.824, 'learning_rate': 3.126338329764454e-05, 'epoch': 1.12}\n","{'loss': 1.8276, 'learning_rate': 3.081727337615989e-05, 'epoch': 1.15}\n","{'loss': 1.7748, 'learning_rate': 3.0371163454675235e-05, 'epoch': 1.18}\n","{'loss': 1.775, 'learning_rate': 2.9925053533190578e-05, 'epoch': 1.2}\n","{'loss': 1.808, 'learning_rate': 2.9478943611705928e-05, 'epoch': 1.23}\n","{'loss': 1.787, 'learning_rate': 2.903283369022127e-05, 'epoch': 1.26}\n","{'loss': 1.7876, 'learning_rate': 2.8586723768736618e-05, 'epoch': 1.28}\n","{'loss': 1.7718, 'learning_rate': 2.814061384725196e-05, 'epoch': 1.31}\n"," 44% 4961/11208 [3:13:23<4:02:17,  2.33s/it]"]}]},{"cell_type":"markdown","source":["# Resume train mT5-Base e3"],"metadata":{"id":"clobZm6DoEh2"}},{"cell_type":"code","source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-8000\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3\" \\\n","    --per_device_train_batch_size=2 \\\n","    --per_device_eval_batch_size=8 \\\n","    --gradient_accumulation_steps 4 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 1000 \\\n","    --max_source_length 700 \\\n","    --max_target_length 64"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wqc3e8HyoIzz","outputId":"8d46180b-7a26-4a46-c94a-63a16f33ddbb","executionInfo":{"status":"ok","timestamp":1671060607824,"user_tz":-420,"elapsed":10624009,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=4,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=passive,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/runs/Dec14_20-33-12_0f7f72f7c8de,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=2,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3,\n","save_on_each_node=False,\n","save_steps=1000,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","INFO:__main__:Checkpoint detected, resuming training at /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-8000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n","WARNING:datasets.builder:Using custom data configuration default-1d0dc81c08029efb\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n","INFO:datasets.builder:Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n","Downloading data files: 100% 3/3 [00:00<00:00, 617.23it/s]\n","INFO:datasets.download.download_manager:Downloading took 0.0 min\n","INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n","INFO:datasets.utils.py_utils:Spawning 3 processes for 3 objects in slices of [1, 1, 1]\n","Extracting data files #0:   0% 0/1 [00:00<?, ?obj/s]\n","\n","Extracting data files #2:   0% 0/1 [00:00<?, ?obj/s]\u001b[A\u001b[A\n","Extracting data files #2: 100% 1/1 [00:00<00:00, 52.12obj/s]\n","Extracting data files #0: 100% 1/1 [00:00<00:00, 14.87obj/s]\n","Extracting data files #1: 100% 1/1 [00:00<00:00, 17.26obj/s]\n","INFO:datasets.utils.py_utils:Finished 3 processes\n","INFO:datasets.utils.py_utils:Unpacked 3 objects\n","INFO:datasets.utils.info_utils:Unable to verify checksums.\n","INFO:datasets.builder:Generating train split\n","INFO:datasets.builder:Generating validation split\n","INFO:datasets.builder:Generating test split\n","INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n","Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n","100% 3/3 [00:00<00:00, 536.61it/s]\n","[INFO|configuration_utils.py:652] 2022-12-14 20:33:28,191 >> loading configuration file /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-8000/config.json\n","[INFO|configuration_utils.py:706] 2022-12-14 20:33:28,198 >> Model config MT5Config {\n","  \"_name_or_path\": \"/content/drive/MyDrive/\\u0110o\\u0302\\u0300 a\\u0301n DS310 - Xu\\u031b\\u0309 li\\u0301 ngo\\u0302n ngu\\u031b\\u0303 tu\\u031b\\u0323 nhie\\u0302n/Model/mT5-Base/epoch3/checkpoint-8000\",\n","  \"architectures\": [\n","    \"MT5ForConditionalGeneration\"\n","  ],\n","  \"d_ff\": 2048,\n","  \"d_kv\": 64,\n","  \"d_model\": 768,\n","  \"decoder_start_token_id\": 0,\n","  \"dense_act_fn\": \"gelu_new\",\n","  \"dropout_rate\": 0.1,\n","  \"eos_token_id\": 1,\n","  \"feed_forward_proj\": \"gated-gelu\",\n","  \"initializer_factor\": 1.0,\n","  \"is_encoder_decoder\": true,\n","  \"is_gated_act\": true,\n","  \"layer_norm_epsilon\": 1e-06,\n","  \"model_type\": \"mt5\",\n","  \"num_decoder_layers\": 12,\n","  \"num_heads\": 12,\n","  \"num_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 0,\n","  \"relative_attention_max_distance\": 128,\n","  \"relative_attention_num_buckets\": 32,\n","  \"tie_word_embeddings\": false,\n","  \"tokenizer_class\": \"T5Tokenizer\",\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 250112\n","}\n","\n","[INFO|tokenization_utils_base.py:1797] 2022-12-14 20:33:29,021 >> loading file spiece.model\n","[INFO|tokenization_utils_base.py:1797] 2022-12-14 20:33:29,021 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-14 20:33:29,022 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-14 20:33:29,022 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-14 20:33:29,022 >> loading file tokenizer_config.json\n","[INFO|modeling_utils.py:2248] 2022-12-14 20:33:33,293 >> loading weights file /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-8000/pytorch_model.bin\n","[INFO|modeling_utils.py:2806] 2022-12-14 20:34:23,397 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:2814] 2022-12-14 20:34:23,397 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-8000.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n","Running tokenizer on train dataset:   0% 0/30 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-82bcbce7383f2c38.arrow\n","Running tokenizer on train dataset: 100% 30/30 [00:54<00:00,  1.80s/ba]\n","Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9bc4a368ef5f2d3d.arrow\n","Running tokenizer on validation dataset: 100% 4/4 [00:06<00:00,  1.64s/ba]\n","Running tokenizer on prediction dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-4ae8ce11c8fcb2b9.arrow\n","Running tokenizer on prediction dataset: 100% 4/4 [00:09<00:00,  2.47s/ba]\n","Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 4.56MB/s]\n","[INFO|trainer.py:1963] 2022-12-14 20:35:45,609 >> Loading model from /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-8000.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1641] 2022-12-14 20:37:22,164 >> ***** Running training *****\n","[INFO|trainer.py:1642] 2022-12-14 20:37:22,164 >>   Num examples = 29893\n","[INFO|trainer.py:1643] 2022-12-14 20:37:22,164 >>   Num Epochs = 3\n","[INFO|trainer.py:1644] 2022-12-14 20:37:22,164 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1645] 2022-12-14 20:37:22,164 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1646] 2022-12-14 20:37:22,164 >>   Gradient Accumulation steps = 4\n","[INFO|trainer.py:1647] 2022-12-14 20:37:22,165 >>   Total optimization steps = 11208\n","[INFO|trainer.py:1648] 2022-12-14 20:37:22,167 >>   Number of trainable parameters = 582401280\n","[INFO|trainer.py:1670] 2022-12-14 20:37:22,961 >>   Continuing training from checkpoint, will skip to saved global_step\n","[INFO|trainer.py:1671] 2022-12-14 20:37:22,961 >>   Continuing training from epoch 2\n","[INFO|trainer.py:1672] 2022-12-14 20:37:22,961 >>   Continuing training from global step 8000\n","[INFO|trainer.py:1674] 2022-12-14 20:37:22,961 >>   Will skip the first 2 epochs then the first 2112 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n","Skipping the first batches:   0% 0/2112 [00:00<?, ?it/s]\n","  0% 0/11208 [00:00<?, ?it/s]\u001b[A[WARNING|logging.py:281] 2022-12-14 20:37:23,090 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Skipping the first batches: 100% 2112/2112 [00:05<00:00, 397.33it/s]\n","\n"," 71% 8001/11208 [00:10<00:04, 768.59it/s]\u001b[A\n"," 71% 8007/11208 [00:25<00:04, 768.59it/s]\u001b[A\n"," 71% 8008/11208 [00:25<00:12, 249.73it/s]\u001b[A\n"," 71% 8009/11208 [00:27<00:14, 219.21it/s]\u001b[A\n"," 71% 8010/11208 [00:29<00:17, 186.65it/s]\u001b[A\n"," 71% 8011/11208 [00:32<00:20, 153.89it/s]\u001b[A\n"," 71% 8012/11208 [00:34<00:26, 122.79it/s]\u001b[A\n"," 71% 8013/11208 [00:36<00:33, 95.93it/s] \u001b[A\n"," 72% 8014/11208 [00:38<00:43, 72.68it/s]\u001b[A\n"," 72% 8015/11208 [00:41<00:59, 53.89it/s]\u001b[A\n"," 72% 8016/11208 [00:43<01:21, 39.34it/s]\u001b[A\n"," 72% 8017/11208 [00:45<01:51, 28.55it/s]\u001b[A\n"," 72% 8018/11208 [00:47<02:35, 20.51it/s]\u001b[A\n"," 72% 8019/11208 [00:50<03:36, 14.70it/s]\u001b[A\n"," 72% 8020/11208 [00:52<05:04, 10.48it/s]\u001b[A\n"," 72% 8021/11208 [00:54<07:04,  7.51it/s]\u001b[A\n"," 72% 8022/11208 [00:56<09:50,  5.40it/s]\u001b[A\n"," 72% 8023/11208 [00:59<13:34,  3.91it/s]\u001b[A\n"," 72% 8024/11208 [01:01<18:29,  2.87it/s]\u001b[A\n"," 72% 8025/11208 [01:03<24:50,  2.14it/s]\u001b[A\n"," 72% 8026/11208 [01:06<32:43,  1.62it/s]\u001b[A\n"," 72% 8027/11208 [01:08<42:07,  1.26it/s]\u001b[A\n"," 72% 8028/11208 [01:10<52:39,  1.01it/s]\u001b[A\n"," 72% 8029/11208 [01:13<1:03:55,  1.21s/it]\u001b[A\n"," 72% 8030/11208 [01:15<1:15:03,  1.42s/it]\u001b[A\n"," 72% 8031/11208 [01:17<1:25:23,  1.61s/it]\u001b[A\n"," 72% 8032/11208 [01:20<1:33:26,  1.77s/it]\u001b[A\n"," 72% 8033/11208 [01:22<1:41:27,  1.92s/it]\u001b[A\n"," 72% 8034/11208 [01:24<1:47:55,  2.04s/it]\u001b[A\n"," 72% 8035/11208 [01:27<1:53:05,  2.14s/it]\u001b[A\n"," 72% 8036/11208 [01:29<1:56:51,  2.21s/it]\u001b[A\n"," 72% 8037/11208 [01:32<1:59:17,  2.26s/it]\u001b[A\n"," 72% 8038/11208 [01:34<1:58:31,  2.24s/it]\u001b[A\n"," 72% 8039/11208 [01:36<2:00:24,  2.28s/it]\u001b[A\n"," 72% 8040/11208 [01:39<2:01:55,  2.31s/it]\u001b[A\n"," 72% 8041/11208 [01:41<2:02:39,  2.32s/it]\u001b[A\n"," 72% 8042/11208 [01:43<2:03:00,  2.33s/it]\u001b[A\n"," 72% 8043/11208 [01:46<2:03:10,  2.33s/it]\u001b[A\n"," 72% 8044/11208 [01:48<2:03:04,  2.33s/it]\u001b[A\n"," 72% 8045/11208 [01:50<2:02:48,  2.33s/it]\u001b[A\n"," 72% 8046/11208 [01:53<2:02:36,  2.33s/it]\u001b[A\n"," 72% 8047/11208 [01:55<2:02:25,  2.32s/it]\u001b[A\n"," 72% 8048/11208 [01:57<2:02:09,  2.32s/it]\u001b[A\n"," 72% 8049/11208 [02:00<2:01:51,  2.31s/it]\u001b[A\n"," 72% 8050/11208 [02:02<2:01:42,  2.31s/it]\u001b[A\n"," 72% 8051/11208 [02:04<2:01:21,  2.31s/it]\u001b[A\n"," 72% 8052/11208 [02:06<2:01:24,  2.31s/it]\u001b[A\n"," 72% 8053/11208 [02:09<2:01:00,  2.30s/it]\u001b[A\n"," 72% 8054/11208 [02:11<2:00:57,  2.30s/it]\u001b[A\n"," 72% 8055/11208 [02:13<2:00:58,  2.30s/it]\u001b[A\n"," 72% 8056/11208 [02:15<1:57:32,  2.24s/it]\u001b[A\n"," 72% 8057/11208 [02:18<1:58:36,  2.26s/it]\u001b[A\n"," 72% 8058/11208 [02:20<1:59:14,  2.27s/it]\u001b[A\n"," 72% 8059/11208 [02:22<1:59:45,  2.28s/it]\u001b[A\n"," 72% 8060/11208 [02:25<2:00:09,  2.29s/it]\u001b[A\n"," 72% 8061/11208 [02:27<2:00:17,  2.29s/it]\u001b[A\n"," 72% 8062/11208 [02:29<2:00:27,  2.30s/it]\u001b[A\n"," 72% 8063/11208 [02:32<2:00:22,  2.30s/it]\u001b[A\n"," 72% 8064/11208 [02:34<2:00:46,  2.30s/it]\u001b[A\n"," 72% 8065/11208 [02:36<2:01:08,  2.31s/it]\u001b[A\n"," 72% 8066/11208 [02:39<2:01:22,  2.32s/it]\u001b[A\n"," 72% 8067/11208 [02:41<2:01:41,  2.32s/it]\u001b[A\n"," 72% 8068/11208 [02:43<2:01:56,  2.33s/it]\u001b[A\n"," 72% 8069/11208 [02:46<2:02:07,  2.33s/it]\u001b[A\n"," 72% 8070/11208 [02:48<2:02:08,  2.34s/it]\u001b[A\n"," 72% 8071/11208 [02:50<2:02:08,  2.34s/it]\u001b[A\n"," 72% 8072/11208 [02:53<2:01:24,  2.32s/it]\u001b[A\n"," 72% 8073/11208 [02:55<2:01:33,  2.33s/it]\u001b[A\n"," 72% 8074/11208 [02:57<2:01:42,  2.33s/it]\u001b[A\n"," 72% 8075/11208 [03:00<2:01:45,  2.33s/it]\u001b[A\n"," 72% 8076/11208 [03:02<2:01:30,  2.33s/it]\u001b[A\n"," 72% 8077/11208 [03:04<2:01:35,  2.33s/it]\u001b[A\n"," 72% 8078/11208 [03:06<2:01:25,  2.33s/it]\u001b[A\n"," 72% 8079/11208 [03:09<2:01:28,  2.33s/it]\u001b[A\n"," 72% 8080/11208 [03:11<2:01:27,  2.33s/it]\u001b[A\n"," 72% 8081/11208 [03:13<2:01:05,  2.32s/it]\u001b[A\n"," 72% 8082/11208 [03:16<1:59:51,  2.30s/it]\u001b[A\n"," 72% 8083/11208 [03:18<2:00:17,  2.31s/it]\u001b[A\n"," 72% 8084/11208 [03:20<2:00:30,  2.31s/it]\u001b[A\n"," 72% 8085/11208 [03:23<2:00:35,  2.32s/it]\u001b[A\n"," 72% 8086/11208 [03:25<2:00:43,  2.32s/it]\u001b[A\n"," 72% 8087/11208 [03:27<2:00:47,  2.32s/it]\u001b[A\n"," 72% 8088/11208 [03:30<2:00:47,  2.32s/it]\u001b[A\n"," 72% 8089/11208 [03:32<1:59:55,  2.31s/it]\u001b[A\n"," 72% 8090/11208 [03:34<2:00:07,  2.31s/it]\u001b[A\n"," 72% 8091/11208 [03:37<2:00:15,  2.31s/it]\u001b[A\n"," 72% 8092/11208 [03:39<2:00:24,  2.32s/it]\u001b[A\n"," 72% 8093/11208 [03:41<2:00:28,  2.32s/it]\u001b[A\n"," 72% 8094/11208 [03:43<1:59:11,  2.30s/it]\u001b[A\n"," 72% 8095/11208 [03:46<1:59:36,  2.31s/it]\u001b[A\n"," 72% 8096/11208 [03:48<2:01:17,  2.34s/it]\u001b[A\n"," 72% 8097/11208 [03:51<2:01:56,  2.35s/it]\u001b[A\n"," 72% 8098/11208 [03:53<2:01:23,  2.34s/it]\u001b[A\n"," 72% 8099/11208 [03:55<2:00:55,  2.33s/it]\u001b[A\n"," 72% 8100/11208 [03:58<2:00:41,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.7022, 'learning_rate': 1.386509635974304e-05, 'epoch': 2.17}\n","\n"," 72% 8100/11208 [03:58<2:00:41,  2.33s/it]\u001b[A\n"," 72% 8101/11208 [04:00<2:00:39,  2.33s/it]\u001b[A\n"," 72% 8102/11208 [04:02<2:00:26,  2.33s/it]\u001b[A\n"," 72% 8103/11208 [04:05<2:00:08,  2.32s/it]\u001b[A\n"," 72% 8104/11208 [04:07<2:00:12,  2.32s/it]\u001b[A\n"," 72% 8105/11208 [04:09<2:00:06,  2.32s/it]\u001b[A\n"," 72% 8106/11208 [04:12<2:00:14,  2.33s/it]\u001b[A\n"," 72% 8107/11208 [04:14<2:00:33,  2.33s/it]\u001b[A\n"," 72% 8108/11208 [04:16<2:00:32,  2.33s/it]\u001b[A\n"," 72% 8109/11208 [04:19<2:00:23,  2.33s/it]\u001b[A\n"," 72% 8110/11208 [04:21<2:00:14,  2.33s/it]\u001b[A\n"," 72% 8111/11208 [04:23<1:59:48,  2.32s/it]\u001b[A\n"," 72% 8112/11208 [04:25<1:59:47,  2.32s/it]\u001b[A\n"," 72% 8113/11208 [04:28<1:59:47,  2.32s/it]\u001b[A\n"," 72% 8114/11208 [04:30<1:59:39,  2.32s/it]\u001b[A\n"," 72% 8115/11208 [04:32<1:59:30,  2.32s/it]\u001b[A\n"," 72% 8116/11208 [04:35<1:56:32,  2.26s/it]\u001b[A\n"," 72% 8117/11208 [04:37<1:57:28,  2.28s/it]\u001b[A\n"," 72% 8118/11208 [04:39<1:57:50,  2.29s/it]\u001b[A\n"," 72% 8119/11208 [04:41<1:58:12,  2.30s/it]\u001b[A\n"," 72% 8120/11208 [04:44<1:58:38,  2.31s/it]\u001b[A\n"," 72% 8121/11208 [04:46<1:58:47,  2.31s/it]\u001b[A\n"," 72% 8122/11208 [04:48<1:58:54,  2.31s/it]\u001b[A\n"," 72% 8123/11208 [04:51<1:59:02,  2.32s/it]\u001b[A\n"," 72% 8124/11208 [04:53<1:59:18,  2.32s/it]\u001b[A\n"," 72% 8125/11208 [04:55<1:58:58,  2.32s/it]\u001b[A\n"," 73% 8126/11208 [04:58<1:59:51,  2.33s/it]\u001b[A\n"," 73% 8127/11208 [05:00<1:59:48,  2.33s/it]\u001b[A\n"," 73% 8128/11208 [05:02<1:59:37,  2.33s/it]\u001b[A\n"," 73% 8129/11208 [05:05<1:59:38,  2.33s/it]\u001b[A\n"," 73% 8130/11208 [05:07<1:59:43,  2.33s/it]\u001b[A\n"," 73% 8131/11208 [05:09<1:59:15,  2.33s/it]\u001b[A\n"," 73% 8132/11208 [05:12<1:59:26,  2.33s/it]\u001b[A\n"," 73% 8133/11208 [05:14<1:59:27,  2.33s/it]\u001b[A\n"," 73% 8134/11208 [05:16<1:59:34,  2.33s/it]\u001b[A\n"," 73% 8135/11208 [05:19<1:59:24,  2.33s/it]\u001b[A\n"," 73% 8136/11208 [05:21<1:59:20,  2.33s/it]\u001b[A\n"," 73% 8137/11208 [05:23<1:59:31,  2.34s/it]\u001b[A\n"," 73% 8138/11208 [05:26<1:59:25,  2.33s/it]\u001b[A\n"," 73% 8139/11208 [05:28<1:59:37,  2.34s/it]\u001b[A\n"," 73% 8140/11208 [05:30<1:59:39,  2.34s/it]\u001b[A\n"," 73% 8141/11208 [05:33<1:59:35,  2.34s/it]\u001b[A\n"," 73% 8142/11208 [05:35<1:59:27,  2.34s/it]\u001b[A\n"," 73% 8143/11208 [05:37<1:58:01,  2.31s/it]\u001b[A\n"," 73% 8144/11208 [05:40<1:58:01,  2.31s/it]\u001b[A\n"," 73% 8145/11208 [05:42<1:58:24,  2.32s/it]\u001b[A\n"," 73% 8146/11208 [05:44<1:58:31,  2.32s/it]\u001b[A\n"," 73% 8147/11208 [05:47<1:56:58,  2.29s/it]\u001b[A\n"," 73% 8148/11208 [05:49<1:57:23,  2.30s/it]\u001b[A\n"," 73% 8149/11208 [05:51<1:57:28,  2.30s/it]\u001b[A\n"," 73% 8150/11208 [05:54<1:57:50,  2.31s/it]\u001b[A\n"," 73% 8151/11208 [05:56<1:58:01,  2.32s/it]\u001b[A\n"," 73% 8152/11208 [05:58<1:58:07,  2.32s/it]\u001b[A\n"," 73% 8153/11208 [06:00<1:56:03,  2.28s/it]\u001b[A\n"," 73% 8154/11208 [06:03<1:56:43,  2.29s/it]\u001b[A\n"," 73% 8155/11208 [06:05<1:55:53,  2.28s/it]\u001b[A\n"," 73% 8156/11208 [06:07<1:56:38,  2.29s/it]\u001b[A\n"," 73% 8157/11208 [06:10<1:57:05,  2.30s/it]\u001b[A\n"," 73% 8158/11208 [06:12<1:57:10,  2.30s/it]\u001b[A\n"," 73% 8159/11208 [06:14<1:57:33,  2.31s/it]\u001b[A\n"," 73% 8160/11208 [06:17<1:57:38,  2.32s/it]\u001b[A\n"," 73% 8161/11208 [06:19<1:57:45,  2.32s/it]\u001b[A\n"," 73% 8162/11208 [06:21<1:57:53,  2.32s/it]\u001b[A\n"," 73% 8163/11208 [06:24<1:57:38,  2.32s/it]\u001b[A\n"," 73% 8164/11208 [06:26<1:57:52,  2.32s/it]\u001b[A\n"," 73% 8165/11208 [06:28<1:57:51,  2.32s/it]\u001b[A\n"," 73% 8166/11208 [06:31<1:57:48,  2.32s/it]\u001b[A\n"," 73% 8167/11208 [06:33<1:57:45,  2.32s/it]\u001b[A\n"," 73% 8168/11208 [06:35<1:57:30,  2.32s/it]\u001b[A\n"," 73% 8169/11208 [06:37<1:57:29,  2.32s/it]\u001b[A\n"," 73% 8170/11208 [06:40<1:57:37,  2.32s/it]\u001b[A\n"," 73% 8171/11208 [06:42<1:57:28,  2.32s/it]\u001b[A\n"," 73% 8172/11208 [06:44<1:57:36,  2.32s/it]\u001b[A\n"," 73% 8173/11208 [06:47<1:56:42,  2.31s/it]\u001b[A\n"," 73% 8174/11208 [06:49<1:57:03,  2.31s/it]\u001b[A\n"," 73% 8175/11208 [06:51<1:57:09,  2.32s/it]\u001b[A\n"," 73% 8176/11208 [06:54<1:57:20,  2.32s/it]\u001b[A\n"," 73% 8177/11208 [06:56<1:57:23,  2.32s/it]\u001b[A\n"," 73% 8178/11208 [06:58<1:57:29,  2.33s/it]\u001b[A\n"," 73% 8179/11208 [07:01<1:57:32,  2.33s/it]\u001b[A\n"," 73% 8180/11208 [07:03<1:57:15,  2.32s/it]\u001b[A\n"," 73% 8181/11208 [07:05<1:57:01,  2.32s/it]\u001b[A\n"," 73% 8182/11208 [07:08<1:56:39,  2.31s/it]\u001b[A\n"," 73% 8183/11208 [07:10<1:56:46,  2.32s/it]\u001b[A\n"," 73% 8184/11208 [07:12<1:56:52,  2.32s/it]\u001b[A\n"," 73% 8185/11208 [07:15<1:56:48,  2.32s/it]\u001b[A\n"," 73% 8186/11208 [07:17<1:57:05,  2.32s/it]\u001b[A\n"," 73% 8187/11208 [07:19<1:57:09,  2.33s/it]\u001b[A\n"," 73% 8188/11208 [07:22<1:57:19,  2.33s/it]\u001b[A\n"," 73% 8189/11208 [07:24<1:54:56,  2.28s/it]\u001b[A\n"," 73% 8190/11208 [07:26<1:55:28,  2.30s/it]\u001b[A\n"," 73% 8191/11208 [07:28<1:55:14,  2.29s/it]\u001b[A\n"," 73% 8192/11208 [07:31<1:54:08,  2.27s/it]\u001b[A\n"," 73% 8193/11208 [07:33<1:55:04,  2.29s/it]\u001b[A\n"," 73% 8194/11208 [07:35<1:55:39,  2.30s/it]\u001b[A\n"," 73% 8195/11208 [07:38<1:55:54,  2.31s/it]\u001b[A\n"," 73% 8196/11208 [07:40<1:56:06,  2.31s/it]\u001b[A\n"," 73% 8197/11208 [07:42<1:56:20,  2.32s/it]\u001b[A\n"," 73% 8198/11208 [07:45<1:56:24,  2.32s/it]\u001b[A\n"," 73% 8199/11208 [07:47<1:56:31,  2.32s/it]\u001b[A\n"," 73% 8200/11208 [07:49<1:56:36,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6749, 'learning_rate': 1.3418986438258387e-05, 'epoch': 2.19}\n","\n"," 73% 8200/11208 [07:49<1:56:36,  2.33s/it]\u001b[A\n"," 73% 8201/11208 [07:52<1:56:32,  2.33s/it]\u001b[A\n"," 73% 8202/11208 [07:54<1:55:15,  2.30s/it]\u001b[A\n"," 73% 8203/11208 [07:56<1:54:29,  2.29s/it]\u001b[A\n"," 73% 8204/11208 [07:58<1:55:03,  2.30s/it]\u001b[A\n"," 73% 8205/11208 [08:01<1:55:29,  2.31s/it]\u001b[A\n"," 73% 8206/11208 [08:03<1:54:28,  2.29s/it]\u001b[A\n"," 73% 8207/11208 [08:05<1:54:48,  2.30s/it]\u001b[A\n"," 73% 8208/11208 [08:08<1:55:11,  2.30s/it]\u001b[A\n"," 73% 8209/11208 [08:10<1:55:10,  2.30s/it]\u001b[A\n"," 73% 8210/11208 [08:12<1:52:56,  2.26s/it]\u001b[A\n"," 73% 8211/11208 [08:14<1:53:58,  2.28s/it]\u001b[A\n"," 73% 8212/11208 [08:17<1:54:42,  2.30s/it]\u001b[A\n"," 73% 8213/11208 [08:19<1:55:09,  2.31s/it]\u001b[A\n"," 73% 8214/11208 [08:21<1:55:32,  2.32s/it]\u001b[A\n"," 73% 8215/11208 [08:24<1:55:43,  2.32s/it]\u001b[A\n"," 73% 8216/11208 [08:26<1:54:20,  2.29s/it]\u001b[A\n"," 73% 8217/11208 [08:28<1:54:40,  2.30s/it]\u001b[A\n"," 73% 8218/11208 [08:31<1:55:12,  2.31s/it]\u001b[A\n"," 73% 8219/11208 [08:33<1:55:06,  2.31s/it]\u001b[A\n"," 73% 8220/11208 [08:35<1:55:14,  2.31s/it]\u001b[A\n"," 73% 8221/11208 [08:38<1:55:34,  2.32s/it]\u001b[A\n"," 73% 8222/11208 [08:40<1:55:24,  2.32s/it]\u001b[A\n"," 73% 8223/11208 [08:42<1:54:44,  2.31s/it]\u001b[A\n"," 73% 8224/11208 [08:44<1:54:42,  2.31s/it]\u001b[A\n"," 73% 8225/11208 [08:47<1:55:04,  2.31s/it]\u001b[A\n"," 73% 8226/11208 [08:49<1:55:16,  2.32s/it]\u001b[A\n"," 73% 8227/11208 [08:51<1:54:24,  2.30s/it]\u001b[A\n"," 73% 8228/11208 [08:54<1:51:44,  2.25s/it]\u001b[A\n"," 73% 8229/11208 [08:56<1:52:51,  2.27s/it]\u001b[A\n"," 73% 8230/11208 [08:58<1:53:29,  2.29s/it]\u001b[A\n"," 73% 8231/11208 [09:00<1:54:07,  2.30s/it]\u001b[A\n"," 73% 8232/11208 [09:03<1:54:26,  2.31s/it]\u001b[A\n"," 73% 8233/11208 [09:05<1:53:24,  2.29s/it]\u001b[A\n"," 73% 8234/11208 [09:07<1:54:09,  2.30s/it]\u001b[A\n"," 73% 8235/11208 [09:10<1:54:19,  2.31s/it]\u001b[A\n"," 73% 8236/11208 [09:12<1:54:39,  2.31s/it]\u001b[A\n"," 73% 8237/11208 [09:14<1:54:57,  2.32s/it]\u001b[A\n"," 74% 8238/11208 [09:17<1:55:10,  2.33s/it]\u001b[A\n"," 74% 8239/11208 [09:19<1:55:22,  2.33s/it]\u001b[A\n"," 74% 8240/11208 [09:21<1:55:23,  2.33s/it]\u001b[A\n"," 74% 8241/11208 [09:24<1:55:23,  2.33s/it]\u001b[A\n"," 74% 8242/11208 [09:26<1:54:40,  2.32s/it]\u001b[A\n"," 74% 8243/11208 [09:28<1:54:42,  2.32s/it]\u001b[A\n"," 74% 8244/11208 [09:31<1:54:38,  2.32s/it]\u001b[A\n"," 74% 8245/11208 [09:33<1:54:47,  2.32s/it]\u001b[A\n"," 74% 8246/11208 [09:35<1:54:48,  2.33s/it]\u001b[A\n"," 74% 8247/11208 [09:38<1:54:56,  2.33s/it]\u001b[A\n"," 74% 8248/11208 [09:40<1:53:59,  2.31s/it]\u001b[A\n"," 74% 8249/11208 [09:42<1:54:19,  2.32s/it]\u001b[A\n"," 74% 8250/11208 [09:45<1:54:26,  2.32s/it]\u001b[A\n"," 74% 8251/11208 [09:47<1:54:36,  2.33s/it]\u001b[A\n"," 74% 8252/11208 [09:49<1:54:49,  2.33s/it]\u001b[A\n"," 74% 8253/11208 [09:52<1:54:48,  2.33s/it]\u001b[A\n"," 74% 8254/11208 [09:54<1:54:46,  2.33s/it]\u001b[A\n"," 74% 8255/11208 [09:56<1:54:29,  2.33s/it]\u001b[A\n"," 74% 8256/11208 [09:59<1:54:34,  2.33s/it]\u001b[A\n"," 74% 8257/11208 [10:01<1:54:20,  2.32s/it]\u001b[A\n"," 74% 8258/11208 [10:03<1:54:29,  2.33s/it]\u001b[A\n"," 74% 8259/11208 [10:06<1:54:34,  2.33s/it]\u001b[A\n"," 74% 8260/11208 [10:08<1:54:25,  2.33s/it]\u001b[A\n"," 74% 8261/11208 [10:10<1:52:22,  2.29s/it]\u001b[A\n"," 74% 8262/11208 [10:12<1:52:46,  2.30s/it]\u001b[A\n"," 74% 8263/11208 [10:15<1:52:53,  2.30s/it]\u001b[A\n"," 74% 8264/11208 [10:17<1:50:19,  2.25s/it]\u001b[A\n"," 74% 8265/11208 [10:19<1:51:50,  2.28s/it]\u001b[A\n"," 74% 8266/11208 [10:22<1:52:47,  2.30s/it]\u001b[A\n"," 74% 8267/11208 [10:24<1:50:57,  2.26s/it]\u001b[A\n"," 74% 8268/11208 [10:26<1:50:08,  2.25s/it]\u001b[A\n"," 74% 8269/11208 [10:28<1:51:06,  2.27s/it]\u001b[A\n"," 74% 8270/11208 [10:31<1:51:50,  2.28s/it]\u001b[A\n"," 74% 8271/11208 [10:33<1:52:24,  2.30s/it]\u001b[A\n"," 74% 8272/11208 [10:35<1:52:50,  2.31s/it]\u001b[A\n"," 74% 8273/11208 [10:38<1:53:14,  2.32s/it]\u001b[A\n"," 74% 8274/11208 [10:40<1:53:06,  2.31s/it]\u001b[A\n"," 74% 8275/11208 [10:42<1:53:06,  2.31s/it]\u001b[A\n"," 74% 8276/11208 [10:45<1:53:23,  2.32s/it]\u001b[A\n"," 74% 8277/11208 [10:47<1:53:22,  2.32s/it]\u001b[A\n"," 74% 8278/11208 [10:49<1:53:39,  2.33s/it]\u001b[A\n"," 74% 8279/11208 [10:52<1:53:40,  2.33s/it]\u001b[A\n"," 74% 8280/11208 [10:54<1:51:20,  2.28s/it]\u001b[A\n"," 74% 8281/11208 [10:56<1:52:12,  2.30s/it]\u001b[A\n"," 74% 8282/11208 [10:58<1:52:41,  2.31s/it]\u001b[A\n"," 74% 8283/11208 [11:01<1:52:58,  2.32s/it]\u001b[A\n"," 74% 8284/11208 [11:03<1:53:15,  2.32s/it]\u001b[A\n"," 74% 8285/11208 [11:05<1:53:07,  2.32s/it]\u001b[A\n"," 74% 8286/11208 [11:08<1:53:06,  2.32s/it]\u001b[A\n"," 74% 8287/11208 [11:10<1:52:47,  2.32s/it]\u001b[A\n"," 74% 8288/11208 [11:12<1:52:48,  2.32s/it]\u001b[A\n"," 74% 8289/11208 [11:15<1:52:50,  2.32s/it]\u001b[A\n"," 74% 8290/11208 [11:17<1:52:55,  2.32s/it]\u001b[A\n"," 74% 8291/11208 [11:19<1:53:01,  2.32s/it]\u001b[A\n"," 74% 8292/11208 [11:22<1:52:47,  2.32s/it]\u001b[A\n"," 74% 8293/11208 [11:24<1:52:51,  2.32s/it]\u001b[A\n"," 74% 8294/11208 [11:26<1:51:18,  2.29s/it]\u001b[A\n"," 74% 8295/11208 [11:28<1:50:25,  2.27s/it]\u001b[A\n"," 74% 8296/11208 [11:31<1:49:10,  2.25s/it]\u001b[A\n"," 74% 8297/11208 [11:33<1:50:15,  2.27s/it]\u001b[A\n"," 74% 8298/11208 [11:35<1:50:58,  2.29s/it]\u001b[A\n"," 74% 8299/11208 [11:38<1:51:07,  2.29s/it]\u001b[A\n"," 74% 8300/11208 [11:40<1:51:38,  2.30s/it]\u001b[A\n","\u001b[A{'loss': 1.7288, 'learning_rate': 1.2972876516773732e-05, 'epoch': 2.22}\n","\n"," 74% 8300/11208 [11:40<1:51:38,  2.30s/it]\u001b[A\n"," 74% 8301/11208 [11:42<1:51:55,  2.31s/it]\u001b[A\n"," 74% 8302/11208 [11:45<1:52:16,  2.32s/it]\u001b[A\n"," 74% 8303/11208 [11:47<1:52:27,  2.32s/it]\u001b[A\n"," 74% 8304/11208 [11:49<1:52:31,  2.32s/it]\u001b[A\n"," 74% 8305/11208 [11:52<1:52:44,  2.33s/it]\u001b[A\n"," 74% 8306/11208 [11:54<1:52:48,  2.33s/it]\u001b[A\n"," 74% 8307/11208 [11:56<1:52:53,  2.33s/it]\u001b[A\n"," 74% 8308/11208 [11:59<1:52:41,  2.33s/it]\u001b[A\n"," 74% 8309/11208 [12:01<1:52:45,  2.33s/it]\u001b[A\n"," 74% 8310/11208 [12:03<1:52:53,  2.34s/it]\u001b[A\n"," 74% 8311/11208 [12:06<1:52:47,  2.34s/it]\u001b[A\n"," 74% 8312/11208 [12:08<1:52:39,  2.33s/it]\u001b[A\n"," 74% 8313/11208 [12:10<1:52:43,  2.34s/it]\u001b[A\n"," 74% 8314/11208 [12:13<1:52:33,  2.33s/it]\u001b[A\n"," 74% 8315/11208 [12:15<1:52:38,  2.34s/it]\u001b[A\n"," 74% 8316/11208 [12:17<1:52:21,  2.33s/it]\u001b[A\n"," 74% 8317/11208 [12:20<1:52:31,  2.34s/it]\u001b[A\n"," 74% 8318/11208 [12:22<1:52:29,  2.34s/it]\u001b[A\n"," 74% 8319/11208 [12:24<1:52:44,  2.34s/it]\u001b[A\n"," 74% 8320/11208 [12:27<1:52:33,  2.34s/it]\u001b[A\n"," 74% 8321/11208 [12:29<1:52:35,  2.34s/it]\u001b[A\n"," 74% 8322/11208 [12:31<1:52:32,  2.34s/it]\u001b[A\n"," 74% 8323/11208 [12:34<1:52:07,  2.33s/it]\u001b[A\n"," 74% 8324/11208 [12:36<1:52:04,  2.33s/it]\u001b[A\n"," 74% 8325/11208 [12:38<1:51:57,  2.33s/it]\u001b[A\n"," 74% 8326/11208 [12:41<1:51:54,  2.33s/it]\u001b[A\n"," 74% 8327/11208 [12:43<1:51:48,  2.33s/it]\u001b[A\n"," 74% 8328/11208 [12:45<1:51:59,  2.33s/it]\u001b[A\n"," 74% 8329/11208 [12:48<1:51:54,  2.33s/it]\u001b[A\n"," 74% 8330/11208 [12:50<1:51:48,  2.33s/it]\u001b[A\n"," 74% 8331/11208 [12:52<1:51:41,  2.33s/it]\u001b[A\n"," 74% 8332/11208 [12:54<1:50:59,  2.32s/it]\u001b[A\n"," 74% 8333/11208 [12:57<1:51:03,  2.32s/it]\u001b[A\n"," 74% 8334/11208 [12:59<1:51:01,  2.32s/it]\u001b[A\n"," 74% 8335/11208 [13:01<1:51:13,  2.32s/it]\u001b[A\n"," 74% 8336/11208 [13:04<1:51:17,  2.32s/it]\u001b[A\n"," 74% 8337/11208 [13:06<1:51:08,  2.32s/it]\u001b[A\n"," 74% 8338/11208 [13:08<1:50:58,  2.32s/it]\u001b[A\n"," 74% 8339/11208 [13:11<1:49:53,  2.30s/it]\u001b[A\n"," 74% 8340/11208 [13:13<1:50:09,  2.30s/it]\u001b[A\n"," 74% 8341/11208 [13:15<1:50:18,  2.31s/it]\u001b[A\n"," 74% 8342/11208 [13:18<1:50:18,  2.31s/it]\u001b[A\n"," 74% 8343/11208 [13:20<1:50:42,  2.32s/it]\u001b[A\n"," 74% 8344/11208 [13:22<1:51:07,  2.33s/it]\u001b[A\n"," 74% 8345/11208 [13:25<1:51:14,  2.33s/it]\u001b[A\n"," 74% 8346/11208 [13:27<1:50:11,  2.31s/it]\u001b[A\n"," 74% 8347/11208 [13:29<1:50:30,  2.32s/it]\u001b[A\n"," 74% 8348/11208 [13:32<1:50:35,  2.32s/it]\u001b[A\n"," 74% 8349/11208 [13:34<1:50:41,  2.32s/it]\u001b[A\n"," 75% 8350/11208 [13:36<1:50:47,  2.33s/it]\u001b[A\n"," 75% 8351/11208 [13:39<1:50:59,  2.33s/it]\u001b[A\n"," 75% 8352/11208 [13:41<1:50:50,  2.33s/it]\u001b[A\n"," 75% 8353/11208 [13:43<1:50:49,  2.33s/it]\u001b[A\n"," 75% 8354/11208 [13:46<1:50:50,  2.33s/it]\u001b[A\n"," 75% 8355/11208 [13:48<1:51:01,  2.33s/it]\u001b[A\n"," 75% 8356/11208 [13:50<1:50:51,  2.33s/it]\u001b[A\n"," 75% 8357/11208 [13:53<1:50:55,  2.33s/it]\u001b[A\n"," 75% 8358/11208 [13:55<1:50:58,  2.34s/it]\u001b[A\n"," 75% 8359/11208 [13:57<1:50:49,  2.33s/it]\u001b[A\n"," 75% 8360/11208 [14:00<1:50:48,  2.33s/it]\u001b[A\n"," 75% 8361/11208 [14:02<1:50:58,  2.34s/it]\u001b[A\n"," 75% 8362/11208 [14:04<1:50:39,  2.33s/it]\u001b[A\n"," 75% 8363/11208 [14:07<1:50:55,  2.34s/it]\u001b[A\n"," 75% 8364/11208 [14:09<1:50:48,  2.34s/it]\u001b[A\n"," 75% 8365/11208 [14:11<1:50:55,  2.34s/it]\u001b[A\n"," 75% 8366/11208 [14:14<1:50:46,  2.34s/it]\u001b[A\n"," 75% 8367/11208 [14:16<1:50:52,  2.34s/it]\u001b[A\n"," 75% 8368/11208 [14:18<1:50:45,  2.34s/it]\u001b[A\n"," 75% 8369/11208 [14:21<1:50:50,  2.34s/it]\u001b[A\n"," 75% 8370/11208 [14:23<1:50:56,  2.35s/it]\u001b[A\n"," 75% 8371/11208 [14:25<1:48:17,  2.29s/it]\u001b[A\n"," 75% 8372/11208 [14:28<1:49:02,  2.31s/it]\u001b[A\n"," 75% 8373/11208 [14:30<1:49:36,  2.32s/it]\u001b[A\n"," 75% 8374/11208 [14:32<1:49:55,  2.33s/it]\u001b[A\n"," 75% 8375/11208 [14:35<1:50:03,  2.33s/it]\u001b[A\n"," 75% 8376/11208 [14:37<1:49:55,  2.33s/it]\u001b[A\n"," 75% 8377/11208 [14:39<1:49:42,  2.33s/it]\u001b[A\n"," 75% 8378/11208 [14:42<1:49:50,  2.33s/it]\u001b[A\n"," 75% 8379/11208 [14:44<1:49:52,  2.33s/it]\u001b[A\n"," 75% 8380/11208 [14:46<1:49:46,  2.33s/it]\u001b[A\n"," 75% 8381/11208 [14:49<1:49:55,  2.33s/it]\u001b[A\n"," 75% 8382/11208 [14:51<1:49:52,  2.33s/it]\u001b[A\n"," 75% 8383/11208 [14:53<1:49:41,  2.33s/it]\u001b[A\n"," 75% 8384/11208 [14:56<1:49:47,  2.33s/it]\u001b[A\n"," 75% 8385/11208 [14:58<1:49:34,  2.33s/it]\u001b[A\n"," 75% 8386/11208 [15:00<1:49:25,  2.33s/it]\u001b[A\n"," 75% 8387/11208 [15:02<1:49:22,  2.33s/it]\u001b[A\n"," 75% 8388/11208 [15:05<1:49:23,  2.33s/it]\u001b[A\n"," 75% 8389/11208 [15:07<1:49:31,  2.33s/it]\u001b[A\n"," 75% 8390/11208 [15:09<1:48:56,  2.32s/it]\u001b[A\n"," 75% 8391/11208 [15:12<1:49:01,  2.32s/it]\u001b[A\n"," 75% 8392/11208 [15:14<1:48:56,  2.32s/it]\u001b[A\n"," 75% 8393/11208 [15:16<1:48:50,  2.32s/it]\u001b[A\n"," 75% 8394/11208 [15:19<1:49:00,  2.32s/it]\u001b[A\n"," 75% 8395/11208 [15:21<1:48:49,  2.32s/it]\u001b[A\n"," 75% 8396/11208 [15:23<1:48:45,  2.32s/it]\u001b[A\n"," 75% 8397/11208 [15:26<1:48:47,  2.32s/it]\u001b[A\n"," 75% 8398/11208 [15:28<1:48:56,  2.33s/it]\u001b[A\n"," 75% 8399/11208 [15:30<1:48:50,  2.32s/it]\u001b[A\n"," 75% 8400/11208 [15:33<1:48:49,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6928, 'learning_rate': 1.252676659528908e-05, 'epoch': 2.25}\n","\n"," 75% 8400/11208 [15:33<1:48:49,  2.33s/it]\u001b[A\n"," 75% 8401/11208 [15:35<1:48:56,  2.33s/it]\u001b[A\n"," 75% 8402/11208 [15:37<1:48:48,  2.33s/it]\u001b[A\n"," 75% 8403/11208 [15:40<1:47:59,  2.31s/it]\u001b[A\n"," 75% 8404/11208 [15:42<1:48:02,  2.31s/it]\u001b[A\n"," 75% 8405/11208 [15:44<1:48:04,  2.31s/it]\u001b[A\n"," 75% 8406/11208 [15:47<1:48:01,  2.31s/it]\u001b[A\n"," 75% 8407/11208 [15:49<1:48:04,  2.32s/it]\u001b[A\n"," 75% 8408/11208 [15:51<1:48:22,  2.32s/it]\u001b[A\n"," 75% 8409/11208 [15:54<1:48:24,  2.32s/it]\u001b[A\n"," 75% 8410/11208 [15:56<1:48:29,  2.33s/it]\u001b[A\n"," 75% 8411/11208 [15:58<1:47:27,  2.31s/it]\u001b[A\n"," 75% 8412/11208 [16:00<1:47:54,  2.32s/it]\u001b[A\n"," 75% 8413/11208 [16:03<1:48:05,  2.32s/it]\u001b[A\n"," 75% 8414/11208 [16:05<1:48:07,  2.32s/it]\u001b[A\n"," 75% 8415/11208 [16:07<1:48:07,  2.32s/it]\u001b[A\n"," 75% 8416/11208 [16:10<1:48:04,  2.32s/it]\u001b[A\n"," 75% 8417/11208 [16:12<1:48:06,  2.32s/it]\u001b[A\n"," 75% 8418/11208 [16:14<1:48:09,  2.33s/it]\u001b[A\n"," 75% 8419/11208 [16:17<1:48:14,  2.33s/it]\u001b[A\n"," 75% 8420/11208 [16:19<1:47:43,  2.32s/it]\u001b[A\n"," 75% 8421/11208 [16:21<1:47:56,  2.32s/it]\u001b[A\n"," 75% 8422/11208 [16:24<1:48:17,  2.33s/it]\u001b[A\n"," 75% 8423/11208 [16:26<1:47:14,  2.31s/it]\u001b[A\n"," 75% 8424/11208 [16:28<1:47:33,  2.32s/it]\u001b[A\n"," 75% 8425/11208 [16:31<1:47:41,  2.32s/it]\u001b[A\n"," 75% 8426/11208 [16:33<1:47:53,  2.33s/it]\u001b[A\n"," 75% 8427/11208 [16:35<1:48:02,  2.33s/it]\u001b[A\n"," 75% 8428/11208 [16:38<1:47:51,  2.33s/it]\u001b[A\n"," 75% 8429/11208 [16:40<1:47:51,  2.33s/it]\u001b[A\n"," 75% 8430/11208 [16:42<1:47:53,  2.33s/it]\u001b[A\n"," 75% 8431/11208 [16:45<1:47:37,  2.33s/it]\u001b[A\n"," 75% 8432/11208 [16:47<1:47:26,  2.32s/it]\u001b[A\n"," 75% 8433/11208 [16:49<1:47:39,  2.33s/it]\u001b[A\n"," 75% 8434/11208 [16:52<1:47:35,  2.33s/it]\u001b[A\n"," 75% 8435/11208 [16:54<1:47:42,  2.33s/it]\u001b[A\n"," 75% 8436/11208 [16:56<1:47:37,  2.33s/it]\u001b[A\n"," 75% 8437/11208 [16:59<1:47:30,  2.33s/it]\u001b[A\n"," 75% 8438/11208 [17:01<1:47:34,  2.33s/it]\u001b[A\n"," 75% 8439/11208 [17:03<1:47:23,  2.33s/it]\u001b[A\n"," 75% 8440/11208 [17:06<1:47:25,  2.33s/it]\u001b[A\n"," 75% 8441/11208 [17:08<1:44:16,  2.26s/it]\u001b[A\n"," 75% 8442/11208 [17:10<1:45:06,  2.28s/it]\u001b[A\n"," 75% 8443/11208 [17:12<1:42:49,  2.23s/it]\u001b[A\n"," 75% 8444/11208 [17:15<1:44:10,  2.26s/it]\u001b[A\n"," 75% 8445/11208 [17:17<1:44:12,  2.26s/it]\u001b[A\n"," 75% 8446/11208 [17:19<1:44:45,  2.28s/it]\u001b[A\n"," 75% 8447/11208 [17:21<1:45:30,  2.29s/it]\u001b[A\n"," 75% 8448/11208 [17:24<1:45:57,  2.30s/it]\u001b[A\n"," 75% 8449/11208 [17:26<1:46:24,  2.31s/it]\u001b[A\n"," 75% 8450/11208 [17:28<1:46:30,  2.32s/it]\u001b[A\n"," 75% 8451/11208 [17:31<1:46:38,  2.32s/it]\u001b[A\n"," 75% 8452/11208 [17:33<1:46:51,  2.33s/it]\u001b[A\n"," 75% 8453/11208 [17:35<1:47:01,  2.33s/it]\u001b[A\n"," 75% 8454/11208 [17:38<1:47:00,  2.33s/it]\u001b[A\n"," 75% 8455/11208 [17:40<1:46:50,  2.33s/it]\u001b[A\n"," 75% 8456/11208 [17:42<1:46:47,  2.33s/it]\u001b[A\n"," 75% 8457/11208 [17:45<1:46:53,  2.33s/it]\u001b[A\n"," 75% 8458/11208 [17:47<1:46:47,  2.33s/it]\u001b[A\n"," 75% 8459/11208 [17:49<1:46:26,  2.32s/it]\u001b[A\n"," 75% 8460/11208 [17:52<1:46:23,  2.32s/it]\u001b[A\n"," 75% 8461/11208 [17:54<1:42:29,  2.24s/it]\u001b[A\n"," 75% 8462/11208 [17:56<1:43:44,  2.27s/it]\u001b[A\n"," 76% 8463/11208 [17:58<1:44:34,  2.29s/it]\u001b[A\n"," 76% 8464/11208 [18:01<1:45:09,  2.30s/it]\u001b[A\n"," 76% 8465/11208 [18:03<1:45:07,  2.30s/it]\u001b[A\n"," 76% 8466/11208 [18:05<1:45:35,  2.31s/it]\u001b[A\n"," 76% 8467/11208 [18:08<1:45:56,  2.32s/it]\u001b[A\n"," 76% 8468/11208 [18:10<1:46:09,  2.32s/it]\u001b[A\n"," 76% 8469/11208 [18:12<1:45:32,  2.31s/it]\u001b[A\n"," 76% 8470/11208 [18:15<1:45:36,  2.31s/it]\u001b[A\n"," 76% 8471/11208 [18:17<1:45:54,  2.32s/it]\u001b[A\n"," 76% 8472/11208 [18:19<1:45:49,  2.32s/it]\u001b[A\n"," 76% 8473/11208 [18:22<1:46:04,  2.33s/it]\u001b[A\n"," 76% 8474/11208 [18:24<1:46:04,  2.33s/it]\u001b[A\n"," 76% 8475/11208 [18:26<1:46:14,  2.33s/it]\u001b[A\n"," 76% 8476/11208 [18:29<1:46:20,  2.34s/it]\u001b[A\n"," 76% 8477/11208 [18:31<1:46:15,  2.33s/it]\u001b[A\n"," 76% 8478/11208 [18:33<1:46:24,  2.34s/it]\u001b[A\n"," 76% 8479/11208 [18:36<1:46:26,  2.34s/it]\u001b[A\n"," 76% 8480/11208 [18:38<1:46:29,  2.34s/it]\u001b[A\n"," 76% 8481/11208 [18:40<1:46:23,  2.34s/it]\u001b[A\n"," 76% 8482/11208 [18:43<1:46:22,  2.34s/it]\u001b[A\n"," 76% 8483/11208 [18:45<1:45:22,  2.32s/it]\u001b[A\n"," 76% 8484/11208 [18:47<1:45:02,  2.31s/it]\u001b[A\n"," 76% 8485/11208 [18:50<1:45:14,  2.32s/it]\u001b[A\n"," 76% 8486/11208 [18:52<1:45:24,  2.32s/it]\u001b[A\n"," 76% 8487/11208 [18:54<1:45:33,  2.33s/it]\u001b[A\n"," 76% 8488/11208 [18:57<1:45:42,  2.33s/it]\u001b[A\n"," 76% 8489/11208 [18:59<1:45:37,  2.33s/it]\u001b[A\n"," 76% 8490/11208 [19:01<1:45:41,  2.33s/it]\u001b[A\n"," 76% 8491/11208 [19:04<1:45:49,  2.34s/it]\u001b[A\n"," 76% 8492/11208 [19:06<1:45:43,  2.34s/it]\u001b[A\n"," 76% 8493/11208 [19:08<1:45:44,  2.34s/it]\u001b[A\n"," 76% 8494/11208 [19:11<1:45:27,  2.33s/it]\u001b[A\n"," 76% 8495/11208 [19:13<1:43:33,  2.29s/it]\u001b[A\n"," 76% 8496/11208 [19:15<1:42:48,  2.27s/it]\u001b[A\n"," 76% 8497/11208 [19:17<1:43:47,  2.30s/it]\u001b[A\n"," 76% 8498/11208 [19:20<1:44:20,  2.31s/it]\u001b[A\n"," 76% 8499/11208 [19:22<1:44:44,  2.32s/it]\u001b[A\n"," 76% 8500/11208 [19:24<1:44:58,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.7019, 'learning_rate': 1.2080656673804426e-05, 'epoch': 2.28}\n","\n"," 76% 8500/11208 [19:25<1:44:58,  2.33s/it]\u001b[A\n"," 76% 8501/11208 [19:27<1:45:08,  2.33s/it]\u001b[A\n"," 76% 8502/11208 [19:29<1:44:55,  2.33s/it]\u001b[A\n"," 76% 8503/11208 [19:31<1:44:09,  2.31s/it]\u001b[A\n"," 76% 8504/11208 [19:34<1:44:19,  2.31s/it]\u001b[A\n"," 76% 8505/11208 [19:36<1:44:25,  2.32s/it]\u001b[A\n"," 76% 8506/11208 [19:38<1:44:39,  2.32s/it]\u001b[A\n"," 76% 8507/11208 [19:41<1:43:33,  2.30s/it]\u001b[A\n"," 76% 8508/11208 [19:43<1:43:54,  2.31s/it]\u001b[A\n"," 76% 8509/11208 [19:45<1:44:06,  2.31s/it]\u001b[A\n"," 76% 8510/11208 [19:48<1:44:16,  2.32s/it]\u001b[A\n"," 76% 8511/11208 [19:50<1:43:29,  2.30s/it]\u001b[A\n"," 76% 8512/11208 [19:52<1:43:37,  2.31s/it]\u001b[A\n"," 76% 8513/11208 [19:55<1:44:03,  2.32s/it]\u001b[A\n"," 76% 8514/11208 [19:57<1:44:10,  2.32s/it]\u001b[A\n"," 76% 8515/11208 [19:59<1:44:12,  2.32s/it]\u001b[A\n"," 76% 8516/11208 [20:01<1:43:10,  2.30s/it]\u001b[A\n"," 76% 8517/11208 [20:04<1:43:15,  2.30s/it]\u001b[A\n"," 76% 8518/11208 [20:06<1:43:34,  2.31s/it]\u001b[A\n"," 76% 8519/11208 [20:08<1:42:49,  2.29s/it]\u001b[A\n"," 76% 8520/11208 [20:11<1:43:09,  2.30s/it]\u001b[A\n"," 76% 8521/11208 [20:13<1:43:31,  2.31s/it]\u001b[A\n"," 76% 8522/11208 [20:15<1:43:44,  2.32s/it]\u001b[A\n"," 76% 8523/11208 [20:18<1:42:25,  2.29s/it]\u001b[A\n"," 76% 8524/11208 [20:20<1:41:39,  2.27s/it]\u001b[A\n"," 76% 8525/11208 [20:22<1:42:17,  2.29s/it]\u001b[A\n"," 76% 8526/11208 [20:24<1:42:49,  2.30s/it]\u001b[A\n"," 76% 8527/11208 [20:27<1:41:24,  2.27s/it]\u001b[A\n"," 76% 8528/11208 [20:29<1:42:12,  2.29s/it]\u001b[A\n"," 76% 8529/11208 [20:31<1:42:35,  2.30s/it]\u001b[A\n"," 76% 8530/11208 [20:34<1:43:01,  2.31s/it]\u001b[A\n"," 76% 8531/11208 [20:36<1:43:20,  2.32s/it]\u001b[A\n"," 76% 8532/11208 [20:38<1:43:34,  2.32s/it]\u001b[A\n"," 76% 8533/11208 [20:41<1:43:44,  2.33s/it]\u001b[A\n"," 76% 8534/11208 [20:43<1:41:59,  2.29s/it]\u001b[A\n"," 76% 8535/11208 [20:45<1:42:28,  2.30s/it]\u001b[A\n"," 76% 8536/11208 [20:47<1:42:43,  2.31s/it]\u001b[A\n"," 76% 8537/11208 [20:50<1:42:02,  2.29s/it]\u001b[A\n"," 76% 8538/11208 [20:52<1:42:08,  2.30s/it]\u001b[A\n"," 76% 8539/11208 [20:54<1:42:39,  2.31s/it]\u001b[A\n"," 76% 8540/11208 [20:57<1:42:54,  2.31s/it]\u001b[A\n"," 76% 8541/11208 [20:59<1:43:00,  2.32s/it]\u001b[A\n"," 76% 8542/11208 [21:01<1:43:04,  2.32s/it]\u001b[A\n"," 76% 8543/11208 [21:04<1:43:14,  2.32s/it]\u001b[A\n"," 76% 8544/11208 [21:06<1:43:12,  2.32s/it]\u001b[A\n"," 76% 8545/11208 [21:08<1:40:53,  2.27s/it]\u001b[A\n"," 76% 8546/11208 [21:10<1:41:33,  2.29s/it]\u001b[A\n"," 76% 8547/11208 [21:13<1:42:00,  2.30s/it]\u001b[A\n"," 76% 8548/11208 [21:15<1:42:18,  2.31s/it]\u001b[A\n"," 76% 8549/11208 [21:17<1:42:31,  2.31s/it]\u001b[A\n"," 76% 8550/11208 [21:20<1:42:42,  2.32s/it]\u001b[A\n"," 76% 8551/11208 [21:22<1:42:27,  2.31s/it]\u001b[A\n"," 76% 8552/11208 [21:24<1:42:40,  2.32s/it]\u001b[A\n"," 76% 8553/11208 [21:27<1:41:42,  2.30s/it]\u001b[A\n"," 76% 8554/11208 [21:29<1:42:07,  2.31s/it]\u001b[A\n"," 76% 8555/11208 [21:31<1:42:18,  2.31s/it]\u001b[A\n"," 76% 8556/11208 [21:34<1:42:35,  2.32s/it]\u001b[A\n"," 76% 8557/11208 [21:36<1:42:40,  2.32s/it]\u001b[A\n"," 76% 8558/11208 [21:38<1:42:45,  2.33s/it]\u001b[A\n"," 76% 8559/11208 [21:41<1:42:45,  2.33s/it]\u001b[A\n"," 76% 8560/11208 [21:43<1:43:01,  2.33s/it]\u001b[A\n"," 76% 8561/11208 [21:45<1:43:07,  2.34s/it]\u001b[A\n"," 76% 8562/11208 [21:48<1:43:10,  2.34s/it]\u001b[A\n"," 76% 8563/11208 [21:50<1:42:04,  2.32s/it]\u001b[A\n"," 76% 8564/11208 [21:52<1:42:19,  2.32s/it]\u001b[A\n"," 76% 8565/11208 [21:55<1:42:22,  2.32s/it]\u001b[A\n"," 76% 8566/11208 [21:57<1:42:32,  2.33s/it]\u001b[A\n"," 76% 8567/11208 [21:59<1:42:27,  2.33s/it]\u001b[A\n"," 76% 8568/11208 [22:02<1:41:40,  2.31s/it]\u001b[A\n"," 76% 8569/11208 [22:04<1:41:01,  2.30s/it]\u001b[A\n"," 76% 8570/11208 [22:06<1:41:00,  2.30s/it]\u001b[A\n"," 76% 8571/11208 [22:08<1:41:05,  2.30s/it]\u001b[A\n"," 76% 8572/11208 [22:11<1:41:31,  2.31s/it]\u001b[A\n"," 76% 8573/11208 [22:13<1:41:54,  2.32s/it]\u001b[A\n"," 76% 8574/11208 [22:15<1:41:51,  2.32s/it]\u001b[A\n"," 77% 8575/11208 [22:18<1:42:05,  2.33s/it]\u001b[A\n"," 77% 8576/11208 [22:20<1:41:56,  2.32s/it]\u001b[A\n"," 77% 8577/11208 [22:22<1:41:53,  2.32s/it]\u001b[A\n"," 77% 8578/11208 [22:25<1:41:49,  2.32s/it]\u001b[A\n"," 77% 8579/11208 [22:27<1:41:28,  2.32s/it]\u001b[A\n"," 77% 8580/11208 [22:29<1:41:32,  2.32s/it]\u001b[A\n"," 77% 8581/11208 [22:32<1:40:10,  2.29s/it]\u001b[A\n"," 77% 8582/11208 [22:34<1:39:34,  2.28s/it]\u001b[A\n"," 77% 8583/11208 [22:36<1:40:06,  2.29s/it]\u001b[A\n"," 77% 8584/11208 [22:38<1:40:09,  2.29s/it]\u001b[A\n"," 77% 8585/11208 [22:41<1:40:35,  2.30s/it]\u001b[A\n"," 77% 8586/11208 [22:43<1:39:44,  2.28s/it]\u001b[A\n"," 77% 8587/11208 [22:45<1:40:00,  2.29s/it]\u001b[A\n"," 77% 8588/11208 [22:48<1:40:29,  2.30s/it]\u001b[A\n"," 77% 8589/11208 [22:50<1:40:38,  2.31s/it]\u001b[A\n"," 77% 8590/11208 [22:52<1:40:38,  2.31s/it]\u001b[A\n"," 77% 8591/11208 [22:54<1:39:34,  2.28s/it]\u001b[A\n"," 77% 8592/11208 [22:57<1:40:04,  2.30s/it]\u001b[A\n"," 77% 8593/11208 [22:59<1:40:26,  2.30s/it]\u001b[A\n"," 77% 8594/11208 [23:01<1:40:34,  2.31s/it]\u001b[A\n"," 77% 8595/11208 [23:04<1:40:44,  2.31s/it]\u001b[A\n"," 77% 8596/11208 [23:06<1:40:37,  2.31s/it]\u001b[A\n"," 77% 8597/11208 [23:08<1:40:50,  2.32s/it]\u001b[A\n"," 77% 8598/11208 [23:11<1:40:46,  2.32s/it]\u001b[A\n"," 77% 8599/11208 [23:13<1:39:42,  2.29s/it]\u001b[A\n"," 77% 8600/11208 [23:15<1:39:39,  2.29s/it]\u001b[A\n","\u001b[A{'loss': 1.673, 'learning_rate': 1.1634546752319772e-05, 'epoch': 2.3}\n","\n"," 77% 8600/11208 [23:15<1:39:39,  2.29s/it]\u001b[A\n"," 77% 8601/11208 [23:18<1:39:58,  2.30s/it]\u001b[A\n"," 77% 8602/11208 [23:20<1:40:10,  2.31s/it]\u001b[A\n"," 77% 8603/11208 [23:22<1:40:13,  2.31s/it]\u001b[A\n"," 77% 8604/11208 [23:25<1:40:19,  2.31s/it]\u001b[A\n"," 77% 8605/11208 [23:27<1:40:15,  2.31s/it]\u001b[A\n"," 77% 8606/11208 [23:29<1:40:20,  2.31s/it]\u001b[A\n"," 77% 8607/11208 [23:31<1:40:24,  2.32s/it]\u001b[A\n"," 77% 8608/11208 [23:34<1:40:20,  2.32s/it]\u001b[A\n"," 77% 8609/11208 [23:36<1:40:15,  2.31s/it]\u001b[A\n"," 77% 8610/11208 [23:38<1:40:09,  2.31s/it]\u001b[A\n"," 77% 8611/11208 [23:41<1:40:11,  2.31s/it]\u001b[A\n"," 77% 8612/11208 [23:43<1:39:56,  2.31s/it]\u001b[A\n"," 77% 8613/11208 [23:45<1:40:16,  2.32s/it]\u001b[A\n"," 77% 8614/11208 [23:48<1:40:24,  2.32s/it]\u001b[A\n"," 77% 8615/11208 [23:50<1:39:59,  2.31s/it]\u001b[A\n"," 77% 8616/11208 [23:52<1:38:53,  2.29s/it]\u001b[A\n"," 77% 8617/11208 [23:55<1:39:22,  2.30s/it]\u001b[A\n"," 77% 8618/11208 [23:57<1:39:32,  2.31s/it]\u001b[A\n"," 77% 8619/11208 [23:59<1:38:29,  2.28s/it]\u001b[A\n"," 77% 8620/11208 [24:01<1:39:04,  2.30s/it]\u001b[A\n"," 77% 8621/11208 [24:04<1:39:31,  2.31s/it]\u001b[A\n"," 77% 8622/11208 [24:06<1:39:26,  2.31s/it]\u001b[A\n"," 77% 8623/11208 [24:08<1:38:14,  2.28s/it]\u001b[A\n"," 77% 8624/11208 [24:11<1:38:52,  2.30s/it]\u001b[A\n"," 77% 8625/11208 [24:13<1:39:20,  2.31s/it]\u001b[A\n"," 77% 8626/11208 [24:15<1:39:39,  2.32s/it]\u001b[A\n"," 77% 8627/11208 [24:18<1:39:48,  2.32s/it]\u001b[A\n"," 77% 8628/11208 [24:20<1:39:48,  2.32s/it]\u001b[A\n"," 77% 8629/11208 [24:22<1:38:29,  2.29s/it]\u001b[A\n"," 77% 8630/11208 [24:24<1:38:47,  2.30s/it]\u001b[A\n"," 77% 8631/11208 [24:27<1:39:02,  2.31s/it]\u001b[A\n"," 77% 8632/11208 [24:29<1:38:53,  2.30s/it]\u001b[A\n"," 77% 8633/11208 [24:31<1:39:13,  2.31s/it]\u001b[A\n"," 77% 8634/11208 [24:34<1:36:30,  2.25s/it]\u001b[A\n"," 77% 8635/11208 [24:36<1:37:22,  2.27s/it]\u001b[A\n"," 77% 8636/11208 [24:38<1:38:03,  2.29s/it]\u001b[A\n"," 77% 8637/11208 [24:40<1:37:14,  2.27s/it]\u001b[A\n"," 77% 8638/11208 [24:43<1:37:56,  2.29s/it]\u001b[A\n"," 77% 8639/11208 [24:45<1:38:32,  2.30s/it]\u001b[A\n"," 77% 8640/11208 [24:47<1:38:42,  2.31s/it]\u001b[A\n"," 77% 8641/11208 [24:50<1:38:59,  2.31s/it]\u001b[A\n"," 77% 8642/11208 [24:52<1:39:05,  2.32s/it]\u001b[A\n"," 77% 8643/11208 [24:54<1:39:15,  2.32s/it]\u001b[A\n"," 77% 8644/11208 [24:57<1:39:12,  2.32s/it]\u001b[A\n"," 77% 8645/11208 [24:59<1:39:25,  2.33s/it]\u001b[A\n"," 77% 8646/11208 [25:01<1:39:25,  2.33s/it]\u001b[A\n"," 77% 8647/11208 [25:04<1:39:21,  2.33s/it]\u001b[A\n"," 77% 8648/11208 [25:06<1:39:27,  2.33s/it]\u001b[A\n"," 77% 8649/11208 [25:08<1:39:38,  2.34s/it]\u001b[A\n"," 77% 8650/11208 [25:11<1:39:48,  2.34s/it]\u001b[A\n"," 77% 8651/11208 [25:13<1:39:37,  2.34s/it]\u001b[A\n"," 77% 8652/11208 [25:15<1:39:31,  2.34s/it]\u001b[A\n"," 77% 8653/11208 [25:18<1:37:48,  2.30s/it]\u001b[A\n"," 77% 8654/11208 [25:20<1:38:04,  2.30s/it]\u001b[A\n"," 77% 8655/11208 [25:22<1:38:29,  2.31s/it]\u001b[A\n"," 77% 8656/11208 [25:24<1:35:29,  2.25s/it]\u001b[A\n"," 77% 8657/11208 [25:27<1:34:45,  2.23s/it]\u001b[A\n"," 77% 8658/11208 [25:29<1:36:03,  2.26s/it]\u001b[A\n"," 77% 8659/11208 [25:31<1:36:42,  2.28s/it]\u001b[A\n"," 77% 8660/11208 [25:33<1:36:44,  2.28s/it]\u001b[A\n"," 77% 8661/11208 [25:36<1:37:23,  2.29s/it]\u001b[A\n"," 77% 8662/11208 [25:38<1:37:52,  2.31s/it]\u001b[A\n"," 77% 8663/11208 [25:40<1:38:11,  2.31s/it]\u001b[A\n"," 77% 8664/11208 [25:43<1:38:20,  2.32s/it]\u001b[A\n"," 77% 8665/11208 [25:45<1:36:13,  2.27s/it]\u001b[A\n"," 77% 8666/11208 [25:47<1:37:01,  2.29s/it]\u001b[A\n"," 77% 8667/11208 [25:50<1:37:35,  2.30s/it]\u001b[A\n"," 77% 8668/11208 [25:52<1:35:38,  2.26s/it]\u001b[A\n"," 77% 8669/11208 [25:54<1:35:38,  2.26s/it]\u001b[A\n"," 77% 8670/11208 [25:56<1:36:27,  2.28s/it]\u001b[A\n"," 77% 8671/11208 [25:59<1:36:10,  2.27s/it]\u001b[A\n"," 77% 8672/11208 [26:01<1:36:40,  2.29s/it]\u001b[A\n"," 77% 8673/11208 [26:03<1:37:15,  2.30s/it]\u001b[A\n"," 77% 8674/11208 [26:06<1:37:29,  2.31s/it]\u001b[A\n"," 77% 8675/11208 [26:08<1:37:49,  2.32s/it]\u001b[A\n"," 77% 8676/11208 [26:10<1:36:55,  2.30s/it]\u001b[A\n"," 77% 8677/11208 [26:13<1:37:23,  2.31s/it]\u001b[A\n"," 77% 8678/11208 [26:15<1:37:45,  2.32s/it]\u001b[A\n"," 77% 8679/11208 [26:17<1:37:56,  2.32s/it]\u001b[A\n"," 77% 8680/11208 [26:20<1:38:04,  2.33s/it]\u001b[A\n"," 77% 8681/11208 [26:22<1:38:09,  2.33s/it]\u001b[A\n"," 77% 8682/11208 [26:24<1:38:08,  2.33s/it]\u001b[A\n"," 77% 8683/11208 [26:27<1:38:06,  2.33s/it]\u001b[A\n"," 77% 8684/11208 [26:29<1:37:52,  2.33s/it]\u001b[A\n"," 77% 8685/11208 [26:31<1:37:47,  2.33s/it]\u001b[A\n"," 77% 8686/11208 [26:33<1:37:42,  2.32s/it]\u001b[A\n"," 78% 8687/11208 [26:36<1:36:35,  2.30s/it]\u001b[A\n"," 78% 8688/11208 [26:38<1:36:44,  2.30s/it]\u001b[A\n"," 78% 8689/11208 [26:40<1:35:53,  2.28s/it]\u001b[A\n"," 78% 8690/11208 [26:43<1:36:27,  2.30s/it]\u001b[A\n"," 78% 8691/11208 [26:45<1:36:45,  2.31s/it]\u001b[A\n"," 78% 8692/11208 [26:47<1:36:41,  2.31s/it]\u001b[A\n"," 78% 8693/11208 [26:50<1:36:35,  2.30s/it]\u001b[A\n"," 78% 8694/11208 [26:52<1:36:45,  2.31s/it]\u001b[A\n"," 78% 8695/11208 [26:54<1:36:51,  2.31s/it]\u001b[A\n"," 78% 8696/11208 [26:57<1:37:08,  2.32s/it]\u001b[A\n"," 78% 8697/11208 [26:59<1:37:08,  2.32s/it]\u001b[A\n"," 78% 8698/11208 [27:01<1:37:04,  2.32s/it]\u001b[A\n"," 78% 8699/11208 [27:04<1:37:13,  2.33s/it]\u001b[A\n"," 78% 8700/11208 [27:06<1:37:07,  2.32s/it]\u001b[A\n","\u001b[A{'loss': 1.6742, 'learning_rate': 1.1188436830835119e-05, 'epoch': 2.33}\n","\n"," 78% 8700/11208 [27:06<1:37:07,  2.32s/it]\u001b[A\n"," 78% 8701/11208 [27:08<1:37:13,  2.33s/it]\u001b[A\n"," 78% 8702/11208 [27:10<1:37:06,  2.32s/it]\u001b[A\n"," 78% 8703/11208 [27:13<1:36:28,  2.31s/it]\u001b[A\n"," 78% 8704/11208 [27:15<1:36:37,  2.32s/it]\u001b[A\n"," 78% 8705/11208 [27:17<1:36:52,  2.32s/it]\u001b[A\n"," 78% 8706/11208 [27:20<1:35:08,  2.28s/it]\u001b[A\n"," 78% 8707/11208 [27:22<1:35:34,  2.29s/it]\u001b[A\n"," 78% 8708/11208 [27:24<1:36:00,  2.30s/it]\u001b[A\n"," 78% 8709/11208 [27:27<1:36:03,  2.31s/it]\u001b[A\n"," 78% 8710/11208 [27:29<1:36:31,  2.32s/it]\u001b[A\n"," 78% 8711/11208 [27:31<1:36:36,  2.32s/it]\u001b[A\n"," 78% 8712/11208 [27:34<1:36:52,  2.33s/it]\u001b[A\n"," 78% 8713/11208 [27:36<1:36:55,  2.33s/it]\u001b[A\n"," 78% 8714/11208 [27:38<1:36:57,  2.33s/it]\u001b[A\n"," 78% 8715/11208 [27:41<1:37:04,  2.34s/it]\u001b[A\n"," 78% 8716/11208 [27:43<1:37:01,  2.34s/it]\u001b[A\n"," 78% 8717/11208 [27:45<1:37:04,  2.34s/it]\u001b[A\n"," 78% 8718/11208 [27:48<1:37:04,  2.34s/it]\u001b[A\n"," 78% 8719/11208 [27:50<1:37:03,  2.34s/it]\u001b[A\n"," 78% 8720/11208 [27:52<1:37:06,  2.34s/it]\u001b[A\n"," 78% 8721/11208 [27:55<1:37:00,  2.34s/it]\u001b[A\n"," 78% 8722/11208 [27:57<1:36:56,  2.34s/it]\u001b[A\n"," 78% 8723/11208 [27:59<1:36:40,  2.33s/it]\u001b[A\n"," 78% 8724/11208 [28:02<1:36:42,  2.34s/it]\u001b[A\n"," 78% 8725/11208 [28:04<1:36:42,  2.34s/it]\u001b[A\n"," 78% 8726/11208 [28:06<1:35:06,  2.30s/it]\u001b[A\n"," 78% 8727/11208 [28:09<1:35:51,  2.32s/it]\u001b[A\n"," 78% 8728/11208 [28:11<1:33:56,  2.27s/it]\u001b[A\n"," 78% 8729/11208 [28:13<1:34:46,  2.29s/it]\u001b[A\n"," 78% 8730/11208 [28:15<1:35:10,  2.30s/it]\u001b[A\n"," 78% 8731/11208 [28:18<1:35:42,  2.32s/it]\u001b[A\n"," 78% 8732/11208 [28:20<1:35:50,  2.32s/it]\u001b[A\n"," 78% 8733/11208 [28:22<1:35:43,  2.32s/it]\u001b[A\n"," 78% 8734/11208 [28:25<1:35:40,  2.32s/it]\u001b[A\n"," 78% 8735/11208 [28:27<1:35:49,  2.32s/it]\u001b[A\n"," 78% 8736/11208 [28:29<1:36:00,  2.33s/it]\u001b[A\n"," 78% 8737/11208 [28:32<1:36:00,  2.33s/it]\u001b[A\n"," 78% 8738/11208 [28:34<1:36:03,  2.33s/it]\u001b[A\n"," 78% 8739/11208 [28:36<1:35:57,  2.33s/it]\u001b[A\n"," 78% 8740/11208 [28:39<1:35:59,  2.33s/it]\u001b[A\n"," 78% 8741/11208 [28:41<1:35:57,  2.33s/it]\u001b[A\n"," 78% 8742/11208 [28:43<1:35:45,  2.33s/it]\u001b[A\n"," 78% 8743/11208 [28:46<1:35:37,  2.33s/it]\u001b[A\n"," 78% 8744/11208 [28:48<1:35:35,  2.33s/it]\u001b[A\n"," 78% 8745/11208 [28:50<1:35:42,  2.33s/it]\u001b[A\n"," 78% 8746/11208 [28:53<1:35:43,  2.33s/it]\u001b[A\n"," 78% 8747/11208 [28:55<1:35:27,  2.33s/it]\u001b[A\n"," 78% 8748/11208 [28:57<1:35:16,  2.32s/it]\u001b[A\n"," 78% 8749/11208 [29:00<1:35:23,  2.33s/it]\u001b[A\n"," 78% 8750/11208 [29:02<1:35:09,  2.32s/it]\u001b[A\n"," 78% 8751/11208 [29:04<1:35:12,  2.32s/it]\u001b[A\n"," 78% 8752/11208 [29:07<1:33:59,  2.30s/it]\u001b[A\n"," 78% 8753/11208 [29:09<1:34:19,  2.31s/it]\u001b[A\n"," 78% 8754/11208 [29:11<1:34:30,  2.31s/it]\u001b[A\n"," 78% 8755/11208 [29:14<1:34:38,  2.31s/it]\u001b[A\n"," 78% 8756/11208 [29:16<1:34:47,  2.32s/it]\u001b[A\n"," 78% 8757/11208 [29:18<1:33:08,  2.28s/it]\u001b[A\n"," 78% 8758/11208 [29:20<1:33:39,  2.29s/it]\u001b[A\n"," 78% 8759/11208 [29:23<1:33:46,  2.30s/it]\u001b[A\n"," 78% 8760/11208 [29:25<1:34:08,  2.31s/it]\u001b[A\n"," 78% 8761/11208 [29:27<1:34:20,  2.31s/it]\u001b[A\n"," 78% 8762/11208 [29:30<1:34:27,  2.32s/it]\u001b[A\n"," 78% 8763/11208 [29:32<1:34:34,  2.32s/it]\u001b[A\n"," 78% 8764/11208 [29:34<1:34:25,  2.32s/it]\u001b[A\n"," 78% 8765/11208 [29:37<1:34:35,  2.32s/it]\u001b[A\n"," 78% 8766/11208 [29:39<1:33:59,  2.31s/it]\u001b[A\n"," 78% 8767/11208 [29:41<1:34:12,  2.32s/it]\u001b[A\n"," 78% 8768/11208 [29:44<1:34:17,  2.32s/it]\u001b[A\n"," 78% 8769/11208 [29:46<1:34:12,  2.32s/it]\u001b[A\n"," 78% 8770/11208 [29:48<1:34:16,  2.32s/it]\u001b[A\n"," 78% 8771/11208 [29:51<1:34:11,  2.32s/it]\u001b[A\n"," 78% 8772/11208 [29:53<1:34:21,  2.32s/it]\u001b[A\n"," 78% 8773/11208 [29:55<1:34:20,  2.32s/it]\u001b[A\n"," 78% 8774/11208 [29:58<1:34:11,  2.32s/it]\u001b[A\n"," 78% 8775/11208 [30:00<1:34:16,  2.33s/it]\u001b[A\n"," 78% 8776/11208 [30:02<1:34:21,  2.33s/it]\u001b[A\n"," 78% 8777/11208 [30:04<1:33:51,  2.32s/it]\u001b[A\n"," 78% 8778/11208 [30:07<1:33:58,  2.32s/it]\u001b[A\n"," 78% 8779/11208 [30:09<1:34:06,  2.32s/it]\u001b[A\n"," 78% 8780/11208 [30:11<1:34:01,  2.32s/it]\u001b[A\n"," 78% 8781/11208 [30:14<1:34:08,  2.33s/it]\u001b[A\n"," 78% 8782/11208 [30:16<1:33:57,  2.32s/it]\u001b[A\n"," 78% 8783/11208 [30:18<1:34:03,  2.33s/it]\u001b[A\n"," 78% 8784/11208 [30:21<1:33:56,  2.33s/it]\u001b[A\n"," 78% 8785/11208 [30:23<1:34:01,  2.33s/it]\u001b[A\n"," 78% 8786/11208 [30:25<1:34:03,  2.33s/it]\u001b[A\n"," 78% 8787/11208 [30:28<1:34:09,  2.33s/it]\u001b[A\n"," 78% 8788/11208 [30:30<1:34:15,  2.34s/it]\u001b[A\n"," 78% 8789/11208 [30:32<1:34:10,  2.34s/it]\u001b[A\n"," 78% 8790/11208 [30:35<1:34:05,  2.33s/it]\u001b[A\n"," 78% 8791/11208 [30:37<1:34:03,  2.34s/it]\u001b[A\n"," 78% 8792/11208 [30:39<1:34:00,  2.33s/it]\u001b[A\n"," 78% 8793/11208 [30:42<1:33:49,  2.33s/it]\u001b[A\n"," 78% 8794/11208 [30:44<1:32:24,  2.30s/it]\u001b[A\n"," 78% 8795/11208 [30:46<1:32:25,  2.30s/it]\u001b[A\n"," 78% 8796/11208 [30:49<1:32:50,  2.31s/it]\u001b[A\n"," 78% 8797/11208 [30:51<1:32:20,  2.30s/it]\u001b[A\n"," 78% 8798/11208 [30:53<1:32:46,  2.31s/it]\u001b[A\n"," 79% 8799/11208 [30:56<1:33:02,  2.32s/it]\u001b[A\n"," 79% 8800/11208 [30:58<1:33:24,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6705, 'learning_rate': 1.0742326909350464e-05, 'epoch': 2.36}\n","\n"," 79% 8800/11208 [30:58<1:33:24,  2.33s/it]\u001b[A\n"," 79% 8801/11208 [31:00<1:33:45,  2.34s/it]\u001b[A\n"," 79% 8802/11208 [31:03<1:33:55,  2.34s/it]\u001b[A\n"," 79% 8803/11208 [31:05<1:33:49,  2.34s/it]\u001b[A\n"," 79% 8804/11208 [31:07<1:33:45,  2.34s/it]\u001b[A\n"," 79% 8805/11208 [31:10<1:33:39,  2.34s/it]\u001b[A\n"," 79% 8806/11208 [31:12<1:33:32,  2.34s/it]\u001b[A\n"," 79% 8807/11208 [31:14<1:33:32,  2.34s/it]\u001b[A\n"," 79% 8808/11208 [31:17<1:33:28,  2.34s/it]\u001b[A\n"," 79% 8809/11208 [31:19<1:33:21,  2.34s/it]\u001b[A\n"," 79% 8810/11208 [31:21<1:33:03,  2.33s/it]\u001b[A\n"," 79% 8811/11208 [31:24<1:33:03,  2.33s/it]\u001b[A\n"," 79% 8812/11208 [31:26<1:33:05,  2.33s/it]\u001b[A\n"," 79% 8813/11208 [31:28<1:32:24,  2.31s/it]\u001b[A\n"," 79% 8814/11208 [31:31<1:32:36,  2.32s/it]\u001b[A\n"," 79% 8815/11208 [31:33<1:32:52,  2.33s/it]\u001b[A\n"," 79% 8816/11208 [31:35<1:33:03,  2.33s/it]\u001b[A\n"," 79% 8817/11208 [31:38<1:32:03,  2.31s/it]\u001b[A\n"," 79% 8818/11208 [31:40<1:32:08,  2.31s/it]\u001b[A\n"," 79% 8819/11208 [31:42<1:32:16,  2.32s/it]\u001b[A\n"," 79% 8820/11208 [31:45<1:32:34,  2.33s/it]\u001b[A\n"," 79% 8821/11208 [31:47<1:32:24,  2.32s/it]\u001b[A\n"," 79% 8822/11208 [31:49<1:32:32,  2.33s/it]\u001b[A\n"," 79% 8823/11208 [31:52<1:32:41,  2.33s/it]\u001b[A\n"," 79% 8824/11208 [31:54<1:32:44,  2.33s/it]\u001b[A\n"," 79% 8825/11208 [31:56<1:32:47,  2.34s/it]\u001b[A\n"," 79% 8826/11208 [31:59<1:32:40,  2.33s/it]\u001b[A\n"," 79% 8827/11208 [32:01<1:32:29,  2.33s/it]\u001b[A\n"," 79% 8828/11208 [32:03<1:32:22,  2.33s/it]\u001b[A\n"," 79% 8829/11208 [32:06<1:32:26,  2.33s/it]\u001b[A\n"," 79% 8830/11208 [32:08<1:32:30,  2.33s/it]\u001b[A\n"," 79% 8831/11208 [32:10<1:32:23,  2.33s/it]\u001b[A\n"," 79% 8832/11208 [32:13<1:32:24,  2.33s/it]\u001b[A\n"," 79% 8833/11208 [32:15<1:32:10,  2.33s/it]\u001b[A\n"," 79% 8834/11208 [32:17<1:32:03,  2.33s/it]\u001b[A\n"," 79% 8835/11208 [32:19<1:32:01,  2.33s/it]\u001b[A\n"," 79% 8836/11208 [32:22<1:32:07,  2.33s/it]\u001b[A\n"," 79% 8837/11208 [32:24<1:32:12,  2.33s/it]\u001b[A\n"," 79% 8838/11208 [32:26<1:32:11,  2.33s/it]\u001b[A\n"," 79% 8839/11208 [32:29<1:32:07,  2.33s/it]\u001b[A\n"," 79% 8840/11208 [32:31<1:31:36,  2.32s/it]\u001b[A\n"," 79% 8841/11208 [32:33<1:31:29,  2.32s/it]\u001b[A\n"," 79% 8842/11208 [32:36<1:29:47,  2.28s/it]\u001b[A\n"," 79% 8843/11208 [32:38<1:30:31,  2.30s/it]\u001b[A\n"," 79% 8844/11208 [32:40<1:30:46,  2.30s/it]\u001b[A\n"," 79% 8845/11208 [32:43<1:31:02,  2.31s/it]\u001b[A\n"," 79% 8846/11208 [32:45<1:31:04,  2.31s/it]\u001b[A\n"," 79% 8847/11208 [32:47<1:31:14,  2.32s/it]\u001b[A\n"," 79% 8848/11208 [32:50<1:31:30,  2.33s/it]\u001b[A\n"," 79% 8849/11208 [32:52<1:31:29,  2.33s/it]\u001b[A\n"," 79% 8850/11208 [32:54<1:31:32,  2.33s/it]\u001b[A\n"," 79% 8851/11208 [32:57<1:31:27,  2.33s/it]\u001b[A\n"," 79% 8852/11208 [32:59<1:31:35,  2.33s/it]\u001b[A\n"," 79% 8853/11208 [33:01<1:31:42,  2.34s/it]\u001b[A\n"," 79% 8854/11208 [33:04<1:31:34,  2.33s/it]\u001b[A\n"," 79% 8855/11208 [33:06<1:31:36,  2.34s/it]\u001b[A\n"," 79% 8856/11208 [33:08<1:31:38,  2.34s/it]\u001b[A\n"," 79% 8857/11208 [33:11<1:31:34,  2.34s/it]\u001b[A\n"," 79% 8858/11208 [33:13<1:31:29,  2.34s/it]\u001b[A\n"," 79% 8859/11208 [33:15<1:31:22,  2.33s/it]\u001b[A\n"," 79% 8860/11208 [33:18<1:31:24,  2.34s/it]\u001b[A\n"," 79% 8861/11208 [33:20<1:31:30,  2.34s/it]\u001b[A\n"," 79% 8862/11208 [33:22<1:31:25,  2.34s/it]\u001b[A\n"," 79% 8863/11208 [33:25<1:31:26,  2.34s/it]\u001b[A\n"," 79% 8864/11208 [33:27<1:31:23,  2.34s/it]\u001b[A\n"," 79% 8865/11208 [33:29<1:31:15,  2.34s/it]\u001b[A\n"," 79% 8866/11208 [33:32<1:30:24,  2.32s/it]\u001b[A\n"," 79% 8867/11208 [33:34<1:29:49,  2.30s/it]\u001b[A\n"," 79% 8868/11208 [33:36<1:30:17,  2.31s/it]\u001b[A\n"," 79% 8869/11208 [33:39<1:30:30,  2.32s/it]\u001b[A\n"," 79% 8870/11208 [33:41<1:30:24,  2.32s/it]\u001b[A\n"," 79% 8871/11208 [33:43<1:30:23,  2.32s/it]\u001b[A\n"," 79% 8872/11208 [33:46<1:30:36,  2.33s/it]\u001b[A\n"," 79% 8873/11208 [33:48<1:30:36,  2.33s/it]\u001b[A\n"," 79% 8874/11208 [33:50<1:30:50,  2.34s/it]\u001b[A\n"," 79% 8875/11208 [33:53<1:30:55,  2.34s/it]\u001b[A\n"," 79% 8876/11208 [33:55<1:31:03,  2.34s/it]\u001b[A\n"," 79% 8877/11208 [33:57<1:30:56,  2.34s/it]\u001b[A\n"," 79% 8878/11208 [34:00<1:30:45,  2.34s/it]\u001b[A\n"," 79% 8879/11208 [34:02<1:30:40,  2.34s/it]\u001b[A\n"," 79% 8880/11208 [34:04<1:30:46,  2.34s/it]\u001b[A\n"," 79% 8881/11208 [34:07<1:30:46,  2.34s/it]\u001b[A\n"," 79% 8882/11208 [34:09<1:30:46,  2.34s/it]\u001b[A\n"," 79% 8883/11208 [34:11<1:30:39,  2.34s/it]\u001b[A\n"," 79% 8884/11208 [34:14<1:30:19,  2.33s/it]\u001b[A\n"," 79% 8885/11208 [34:16<1:30:20,  2.33s/it]\u001b[A\n"," 79% 8886/11208 [34:18<1:30:18,  2.33s/it]\u001b[A\n"," 79% 8887/11208 [34:21<1:30:17,  2.33s/it]\u001b[A\n"," 79% 8888/11208 [34:23<1:30:22,  2.34s/it]\u001b[A\n"," 79% 8889/11208 [34:25<1:30:19,  2.34s/it]\u001b[A\n"," 79% 8890/11208 [34:28<1:30:18,  2.34s/it]\u001b[A\n"," 79% 8891/11208 [34:30<1:30:17,  2.34s/it]\u001b[A\n"," 79% 8892/11208 [34:32<1:29:19,  2.31s/it]\u001b[A\n"," 79% 8893/11208 [34:35<1:29:24,  2.32s/it]\u001b[A\n"," 79% 8894/11208 [34:37<1:29:21,  2.32s/it]\u001b[A\n"," 79% 8895/11208 [34:39<1:28:03,  2.28s/it]\u001b[A\n"," 79% 8896/11208 [34:41<1:28:39,  2.30s/it]\u001b[A\n"," 79% 8897/11208 [34:44<1:29:05,  2.31s/it]\u001b[A\n"," 79% 8898/11208 [34:46<1:29:23,  2.32s/it]\u001b[A\n"," 79% 8899/11208 [34:48<1:29:31,  2.33s/it]\u001b[A\n"," 79% 8900/11208 [34:51<1:28:11,  2.29s/it]\u001b[A\n","\u001b[A{'loss': 1.6326, 'learning_rate': 1.029621698786581e-05, 'epoch': 2.38}\n","\n"," 79% 8900/11208 [34:51<1:28:11,  2.29s/it]\u001b[A\n"," 79% 8901/11208 [34:53<1:28:37,  2.30s/it]\u001b[A\n"," 79% 8902/11208 [34:55<1:28:42,  2.31s/it]\u001b[A\n"," 79% 8903/11208 [34:58<1:28:48,  2.31s/it]\u001b[A\n"," 79% 8904/11208 [35:00<1:28:54,  2.32s/it]\u001b[A\n"," 79% 8905/11208 [35:02<1:29:08,  2.32s/it]\u001b[A\n"," 79% 8906/11208 [35:05<1:29:14,  2.33s/it]\u001b[A\n"," 79% 8907/11208 [35:07<1:29:04,  2.32s/it]\u001b[A\n"," 79% 8908/11208 [35:09<1:29:05,  2.32s/it]\u001b[A\n"," 79% 8909/11208 [35:11<1:26:09,  2.25s/it]\u001b[A\n"," 79% 8910/11208 [35:14<1:26:54,  2.27s/it]\u001b[A\n"," 80% 8911/11208 [35:16<1:27:18,  2.28s/it]\u001b[A\n"," 80% 8912/11208 [35:18<1:27:49,  2.29s/it]\u001b[A\n"," 80% 8913/11208 [35:21<1:28:06,  2.30s/it]\u001b[A\n"," 80% 8914/11208 [35:23<1:28:14,  2.31s/it]\u001b[A\n"," 80% 8915/11208 [35:25<1:28:19,  2.31s/it]\u001b[A\n"," 80% 8916/11208 [35:28<1:28:28,  2.32s/it]\u001b[A\n"," 80% 8917/11208 [35:30<1:28:30,  2.32s/it]\u001b[A\n"," 80% 8918/11208 [35:32<1:28:37,  2.32s/it]\u001b[A\n"," 80% 8919/11208 [35:35<1:28:36,  2.32s/it]\u001b[A\n"," 80% 8920/11208 [35:37<1:28:41,  2.33s/it]\u001b[A\n"," 80% 8921/11208 [35:39<1:28:16,  2.32s/it]\u001b[A\n"," 80% 8922/11208 [35:41<1:28:12,  2.32s/it]\u001b[A\n"," 80% 8923/11208 [35:44<1:28:24,  2.32s/it]\u001b[A\n"," 80% 8924/11208 [35:46<1:28:34,  2.33s/it]\u001b[A\n"," 80% 8925/11208 [35:48<1:28:31,  2.33s/it]\u001b[A\n"," 80% 8926/11208 [35:51<1:28:33,  2.33s/it]\u001b[A\n"," 80% 8927/11208 [35:53<1:28:19,  2.32s/it]\u001b[A\n"," 80% 8928/11208 [35:55<1:28:28,  2.33s/it]\u001b[A\n"," 80% 8929/11208 [35:58<1:28:10,  2.32s/it]\u001b[A\n"," 80% 8930/11208 [36:00<1:28:09,  2.32s/it]\u001b[A\n"," 80% 8931/11208 [36:02<1:28:13,  2.32s/it]\u001b[A\n"," 80% 8932/11208 [36:05<1:28:05,  2.32s/it]\u001b[A\n"," 80% 8933/11208 [36:07<1:27:18,  2.30s/it]\u001b[A\n"," 80% 8934/11208 [36:09<1:27:24,  2.31s/it]\u001b[A\n"," 80% 8935/11208 [36:12<1:27:34,  2.31s/it]\u001b[A\n"," 80% 8936/11208 [36:14<1:27:24,  2.31s/it]\u001b[A\n"," 80% 8937/11208 [36:16<1:27:03,  2.30s/it]\u001b[A\n"," 80% 8938/11208 [36:18<1:25:48,  2.27s/it]\u001b[A\n"," 80% 8939/11208 [36:21<1:26:22,  2.28s/it]\u001b[A\n"," 80% 8940/11208 [36:23<1:26:38,  2.29s/it]\u001b[A\n"," 80% 8941/11208 [36:25<1:27:05,  2.30s/it]\u001b[A\n"," 80% 8942/11208 [36:28<1:27:24,  2.31s/it]\u001b[A\n"," 80% 8943/11208 [36:30<1:27:19,  2.31s/it]\u001b[A\n"," 80% 8944/11208 [36:32<1:27:24,  2.32s/it]\u001b[A\n"," 80% 8945/11208 [36:35<1:27:35,  2.32s/it]\u001b[A\n"," 80% 8946/11208 [36:37<1:26:31,  2.30s/it]\u001b[A\n"," 80% 8947/11208 [36:39<1:26:54,  2.31s/it]\u001b[A\n"," 80% 8948/11208 [36:42<1:27:24,  2.32s/it]\u001b[A\n"," 80% 8949/11208 [36:44<1:27:36,  2.33s/it]\u001b[A\n"," 80% 8950/11208 [36:46<1:27:44,  2.33s/it]\u001b[A\n"," 80% 8951/11208 [36:49<1:27:39,  2.33s/it]\u001b[A\n"," 80% 8952/11208 [36:51<1:27:36,  2.33s/it]\u001b[A\n"," 80% 8953/11208 [36:53<1:27:17,  2.32s/it]\u001b[A\n"," 80% 8954/11208 [36:56<1:27:06,  2.32s/it]\u001b[A\n"," 80% 8955/11208 [36:58<1:27:03,  2.32s/it]\u001b[A\n"," 80% 8956/11208 [37:00<1:27:04,  2.32s/it]\u001b[A\n"," 80% 8957/11208 [37:02<1:26:42,  2.31s/it]\u001b[A\n"," 80% 8958/11208 [37:05<1:26:48,  2.32s/it]\u001b[A\n"," 80% 8959/11208 [37:07<1:26:29,  2.31s/it]\u001b[A\n"," 80% 8960/11208 [37:09<1:25:11,  2.27s/it]\u001b[A\n"," 80% 8961/11208 [37:12<1:25:07,  2.27s/it]\u001b[A\n"," 80% 8962/11208 [37:14<1:25:46,  2.29s/it]\u001b[A\n"," 80% 8963/11208 [37:16<1:26:07,  2.30s/it]\u001b[A\n"," 80% 8964/11208 [37:19<1:26:10,  2.30s/it]\u001b[A\n"," 80% 8965/11208 [37:21<1:26:10,  2.31s/it]\u001b[A\n"," 80% 8966/11208 [37:23<1:26:20,  2.31s/it]\u001b[A\n"," 80% 8967/11208 [37:25<1:26:28,  2.32s/it]\u001b[A\n"," 80% 8968/11208 [37:28<1:26:35,  2.32s/it]\u001b[A\n"," 80% 8969/11208 [37:30<1:26:45,  2.33s/it]\u001b[A\n"," 80% 8970/11208 [37:32<1:26:44,  2.33s/it]\u001b[A\n"," 80% 8971/11208 [37:35<1:26:49,  2.33s/it]\u001b[A\n"," 80% 8972/11208 [37:37<1:26:53,  2.33s/it]\u001b[A\n"," 80% 8973/11208 [37:39<1:26:45,  2.33s/it]\u001b[A\n"," 80% 8974/11208 [37:42<1:26:49,  2.33s/it]\u001b[A\n"," 80% 8975/11208 [37:44<1:26:47,  2.33s/it]\u001b[A\n"," 80% 8976/11208 [37:46<1:26:43,  2.33s/it]\u001b[A\n"," 80% 8977/11208 [37:49<1:26:20,  2.32s/it]\u001b[A\n"," 80% 8978/11208 [37:51<1:26:17,  2.32s/it]\u001b[A\n"," 80% 8979/11208 [37:53<1:25:51,  2.31s/it]\u001b[A\n"," 80% 8980/11208 [37:56<1:26:02,  2.32s/it]\u001b[A\n"," 80% 8981/11208 [37:58<1:26:05,  2.32s/it]\u001b[A\n"," 80% 8982/11208 [38:00<1:26:04,  2.32s/it]\u001b[A\n"," 80% 8983/11208 [38:02<1:23:36,  2.25s/it]\u001b[A\n"," 80% 8984/11208 [38:05<1:24:27,  2.28s/it]\u001b[A\n"," 80% 8985/11208 [38:07<1:24:56,  2.29s/it]\u001b[A\n"," 80% 8986/11208 [38:09<1:25:17,  2.30s/it]\u001b[A\n"," 80% 8987/11208 [38:12<1:25:40,  2.31s/it]\u001b[A\n"," 80% 8988/11208 [38:14<1:25:53,  2.32s/it]\u001b[A\n"," 80% 8989/11208 [38:16<1:25:51,  2.32s/it]\u001b[A\n"," 80% 8990/11208 [38:19<1:25:47,  2.32s/it]\u001b[A\n"," 80% 8991/11208 [38:21<1:25:49,  2.32s/it]\u001b[A\n"," 80% 8992/11208 [38:23<1:25:49,  2.32s/it]\u001b[A\n"," 80% 8993/11208 [38:26<1:25:46,  2.32s/it]\u001b[A\n"," 80% 8994/11208 [38:28<1:25:38,  2.32s/it]\u001b[A\n"," 80% 8995/11208 [38:30<1:24:01,  2.28s/it]\u001b[A\n"," 80% 8996/11208 [38:33<1:24:35,  2.29s/it]\u001b[A\n"," 80% 8997/11208 [38:35<1:24:46,  2.30s/it]\u001b[A\n"," 80% 8998/11208 [38:37<1:25:08,  2.31s/it]\u001b[A\n"," 80% 8999/11208 [38:40<1:25:18,  2.32s/it]\u001b[A\n"," 80% 9000/11208 [38:42<1:25:12,  2.32s/it]\u001b[A\n","\u001b[A{'loss': 1.6704, 'learning_rate': 9.850107066381155e-06, 'epoch': 2.41}\n","\n"," 80% 9000/11208 [38:42<1:25:12,  2.32s/it]\u001b[A[INFO|trainer.py:2700] 2022-12-14 21:16:05,467 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-9000\n","[INFO|configuration_utils.py:447] 2022-12-14 21:16:05,475 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-9000/config.json\n","[INFO|modeling_utils.py:1680] 2022-12-14 21:16:15,091 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-9000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-14 21:16:15,098 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-9000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-14 21:16:15,105 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-9000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-14 21:16:16,956 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-9000/spiece.model\n","\n"," 80% 9001/11208 [39:16<7:20:30, 11.98s/it]\u001b[A\n"," 80% 9002/11208 [39:19<5:33:38,  9.07s/it]\u001b[A\n"," 80% 9003/11208 [39:21<4:18:52,  7.04s/it]\u001b[A\n"," 80% 9004/11208 [39:23<3:27:33,  5.65s/it]\u001b[A\n"," 80% 9005/11208 [39:26<2:51:41,  4.68s/it]\u001b[A\n"," 80% 9006/11208 [39:28<2:26:36,  3.99s/it]\u001b[A\n"," 80% 9007/11208 [39:31<2:08:21,  3.50s/it]\u001b[A\n"," 80% 9008/11208 [39:33<1:56:24,  3.17s/it]\u001b[A\n"," 80% 9009/11208 [39:35<1:48:35,  2.96s/it]\u001b[A\n"," 80% 9010/11208 [39:38<1:43:32,  2.83s/it]\u001b[A\n"," 80% 9011/11208 [39:40<1:39:29,  2.72s/it]\u001b[A\n"," 80% 9012/11208 [39:43<1:36:20,  2.63s/it]\u001b[A\n"," 80% 9013/11208 [39:45<1:33:46,  2.56s/it]\u001b[A\n"," 80% 9014/11208 [39:48<1:31:41,  2.51s/it]\u001b[A\n"," 80% 9015/11208 [39:50<1:30:26,  2.47s/it]\u001b[A\n"," 80% 9016/11208 [39:52<1:28:13,  2.41s/it]\u001b[A\n"," 80% 9017/11208 [39:55<1:27:32,  2.40s/it]\u001b[A\n"," 80% 9018/11208 [39:57<1:26:56,  2.38s/it]\u001b[A\n"," 80% 9019/11208 [39:59<1:26:20,  2.37s/it]\u001b[A\n"," 80% 9020/11208 [40:02<1:26:04,  2.36s/it]\u001b[A\n"," 80% 9021/11208 [40:04<1:25:56,  2.36s/it]\u001b[A\n"," 80% 9022/11208 [40:06<1:24:32,  2.32s/it]\u001b[A\n"," 81% 9023/11208 [40:09<1:24:30,  2.32s/it]\u001b[A\n"," 81% 9024/11208 [40:11<1:24:20,  2.32s/it]\u001b[A\n"," 81% 9025/11208 [40:13<1:24:20,  2.32s/it]\u001b[A\n"," 81% 9026/11208 [40:16<1:24:08,  2.31s/it]\u001b[A\n"," 81% 9027/11208 [40:18<1:24:20,  2.32s/it]\u001b[A\n"," 81% 9028/11208 [40:20<1:24:36,  2.33s/it]\u001b[A\n"," 81% 9029/11208 [40:23<1:24:31,  2.33s/it]\u001b[A\n"," 81% 9030/11208 [40:25<1:24:08,  2.32s/it]\u001b[A\n"," 81% 9031/11208 [40:27<1:23:46,  2.31s/it]\u001b[A\n"," 81% 9032/11208 [40:29<1:23:28,  2.30s/it]\u001b[A\n"," 81% 9033/11208 [40:32<1:23:48,  2.31s/it]\u001b[A\n"," 81% 9034/11208 [40:34<1:24:13,  2.32s/it]\u001b[A\n"," 81% 9035/11208 [40:36<1:24:12,  2.33s/it]\u001b[A\n"," 81% 9036/11208 [40:39<1:24:12,  2.33s/it]\u001b[A\n"," 81% 9037/11208 [40:41<1:24:34,  2.34s/it]\u001b[A\n"," 81% 9038/11208 [40:43<1:24:53,  2.35s/it]\u001b[A\n"," 81% 9039/11208 [40:46<1:23:14,  2.30s/it]\u001b[A\n"," 81% 9040/11208 [40:48<1:22:51,  2.29s/it]\u001b[A\n"," 81% 9041/11208 [40:50<1:23:33,  2.31s/it]\u001b[A\n"," 81% 9042/11208 [40:53<1:23:33,  2.31s/it]\u001b[A\n"," 81% 9043/11208 [40:55<1:24:15,  2.34s/it]\u001b[A\n"," 81% 9044/11208 [40:57<1:24:27,  2.34s/it]\u001b[A\n"," 81% 9045/11208 [41:00<1:24:40,  2.35s/it]\u001b[A\n"," 81% 9046/11208 [41:02<1:25:00,  2.36s/it]\u001b[A\n"," 81% 9047/11208 [41:04<1:23:56,  2.33s/it]\u001b[A\n"," 81% 9048/11208 [41:07<1:24:15,  2.34s/it]\u001b[A\n"," 81% 9049/11208 [41:09<1:24:11,  2.34s/it]\u001b[A\n"," 81% 9050/11208 [41:11<1:24:19,  2.34s/it]\u001b[A\n"," 81% 9051/11208 [41:14<1:23:52,  2.33s/it]\u001b[A\n"," 81% 9052/11208 [41:16<1:23:59,  2.34s/it]\u001b[A\n"," 81% 9053/11208 [41:18<1:21:58,  2.28s/it]\u001b[A\n"," 81% 9054/11208 [41:21<1:22:28,  2.30s/it]\u001b[A\n"," 81% 9055/11208 [41:23<1:22:50,  2.31s/it]\u001b[A\n"," 81% 9056/11208 [41:25<1:23:12,  2.32s/it]\u001b[A\n"," 81% 9057/11208 [41:28<1:23:21,  2.33s/it]\u001b[A\n"," 81% 9058/11208 [41:30<1:23:29,  2.33s/it]\u001b[A\n"," 81% 9059/11208 [41:32<1:23:36,  2.33s/it]\u001b[A\n"," 81% 9060/11208 [41:35<1:23:32,  2.33s/it]\u001b[A\n"," 81% 9061/11208 [41:37<1:23:07,  2.32s/it]\u001b[A\n"," 81% 9062/11208 [41:39<1:23:18,  2.33s/it]\u001b[A\n"," 81% 9063/11208 [41:42<1:23:12,  2.33s/it]\u001b[A\n"," 81% 9064/11208 [41:44<1:22:02,  2.30s/it]\u001b[A\n"," 81% 9065/11208 [41:46<1:22:17,  2.30s/it]\u001b[A\n"," 81% 9066/11208 [41:48<1:22:34,  2.31s/it]\u001b[A\n"," 81% 9067/11208 [41:51<1:22:48,  2.32s/it]\u001b[A\n"," 81% 9068/11208 [41:53<1:22:02,  2.30s/it]\u001b[A\n"," 81% 9069/11208 [41:55<1:22:08,  2.30s/it]\u001b[A\n"," 81% 9070/11208 [41:58<1:22:11,  2.31s/it]\u001b[A\n"," 81% 9071/11208 [42:00<1:22:19,  2.31s/it]\u001b[A\n"," 81% 9072/11208 [42:02<1:22:31,  2.32s/it]\u001b[A\n"," 81% 9073/11208 [42:05<1:22:38,  2.32s/it]\u001b[A\n"," 81% 9074/11208 [42:07<1:20:44,  2.27s/it]\u001b[A\n"," 81% 9075/11208 [42:09<1:21:24,  2.29s/it]\u001b[A\n"," 81% 9076/11208 [42:11<1:21:51,  2.30s/it]\u001b[A\n"," 81% 9077/11208 [42:14<1:22:06,  2.31s/it]\u001b[A\n"," 81% 9078/11208 [42:16<1:22:15,  2.32s/it]\u001b[A\n"," 81% 9079/11208 [42:18<1:22:16,  2.32s/it]\u001b[A\n"," 81% 9080/11208 [42:21<1:22:24,  2.32s/it]\u001b[A\n"," 81% 9081/11208 [42:23<1:22:25,  2.33s/it]\u001b[A\n"," 81% 9082/11208 [42:25<1:22:36,  2.33s/it]\u001b[A\n"," 81% 9083/11208 [42:28<1:22:31,  2.33s/it]\u001b[A\n"," 81% 9084/11208 [42:30<1:22:45,  2.34s/it]\u001b[A\n"," 81% 9085/11208 [42:32<1:22:30,  2.33s/it]\u001b[A\n"," 81% 9086/11208 [42:35<1:21:54,  2.32s/it]\u001b[A\n"," 81% 9087/11208 [42:37<1:22:01,  2.32s/it]\u001b[A\n"," 81% 9088/11208 [42:39<1:22:10,  2.33s/it]\u001b[A\n"," 81% 9089/11208 [42:42<1:21:56,  2.32s/it]\u001b[A\n"," 81% 9090/11208 [42:44<1:22:01,  2.32s/it]\u001b[A\n"," 81% 9091/11208 [42:46<1:22:13,  2.33s/it]\u001b[A\n"," 81% 9092/11208 [42:49<1:22:12,  2.33s/it]\u001b[A\n"," 81% 9093/11208 [42:51<1:21:16,  2.31s/it]\u001b[A\n"," 81% 9094/11208 [42:53<1:21:40,  2.32s/it]\u001b[A\n"," 81% 9095/11208 [42:56<1:21:45,  2.32s/it]\u001b[A\n"," 81% 9096/11208 [42:58<1:20:59,  2.30s/it]\u001b[A\n"," 81% 9097/11208 [43:00<1:21:07,  2.31s/it]\u001b[A\n"," 81% 9098/11208 [43:03<1:21:06,  2.31s/it]\u001b[A\n"," 81% 9099/11208 [43:05<1:20:03,  2.28s/it]\u001b[A\n"," 81% 9100/11208 [43:07<1:20:06,  2.28s/it]\u001b[A\n","\u001b[A{'loss': 1.6813, 'learning_rate': 9.403997144896504e-06, 'epoch': 2.44}\n","\n"," 81% 9100/11208 [43:07<1:20:06,  2.28s/it]\u001b[A\n"," 81% 9101/11208 [43:09<1:20:38,  2.30s/it]\u001b[A\n"," 81% 9102/11208 [43:12<1:19:46,  2.27s/it]\u001b[A\n"," 81% 9103/11208 [43:14<1:20:20,  2.29s/it]\u001b[A\n"," 81% 9104/11208 [43:16<1:20:36,  2.30s/it]\u001b[A\n"," 81% 9105/11208 [43:19<1:20:32,  2.30s/it]\u001b[A\n"," 81% 9106/11208 [43:21<1:20:51,  2.31s/it]\u001b[A\n"," 81% 9107/11208 [43:23<1:20:47,  2.31s/it]\u001b[A\n"," 81% 9108/11208 [43:25<1:20:35,  2.30s/it]\u001b[A\n"," 81% 9109/11208 [43:28<1:20:46,  2.31s/it]\u001b[A\n"," 81% 9110/11208 [43:30<1:20:59,  2.32s/it]\u001b[A\n"," 81% 9111/11208 [43:32<1:17:59,  2.23s/it]\u001b[A\n"," 81% 9112/11208 [43:34<1:19:05,  2.26s/it]\u001b[A\n"," 81% 9113/11208 [43:37<1:19:44,  2.28s/it]\u001b[A\n"," 81% 9114/11208 [43:39<1:20:15,  2.30s/it]\u001b[A\n"," 81% 9115/11208 [43:41<1:20:33,  2.31s/it]\u001b[A\n"," 81% 9116/11208 [43:44<1:20:47,  2.32s/it]\u001b[A\n"," 81% 9117/11208 [43:46<1:20:56,  2.32s/it]\u001b[A\n"," 81% 9118/11208 [43:48<1:20:58,  2.32s/it]\u001b[A\n"," 81% 9119/11208 [43:51<1:20:55,  2.32s/it]\u001b[A\n"," 81% 9120/11208 [43:53<1:18:35,  2.26s/it]\u001b[A\n"," 81% 9121/11208 [43:55<1:19:26,  2.28s/it]\u001b[A\n"," 81% 9122/11208 [43:57<1:18:51,  2.27s/it]\u001b[A\n"," 81% 9123/11208 [44:00<1:19:27,  2.29s/it]\u001b[A\n"," 81% 9124/11208 [44:02<1:19:50,  2.30s/it]\u001b[A\n"," 81% 9125/11208 [44:04<1:20:16,  2.31s/it]\u001b[A\n"," 81% 9126/11208 [44:07<1:20:31,  2.32s/it]\u001b[A\n"," 81% 9127/11208 [44:09<1:20:43,  2.33s/it]\u001b[A\n"," 81% 9128/11208 [44:11<1:20:40,  2.33s/it]\u001b[A\n"," 81% 9129/11208 [44:14<1:20:36,  2.33s/it]\u001b[A\n"," 81% 9130/11208 [44:16<1:19:39,  2.30s/it]\u001b[A\n"," 81% 9131/11208 [44:18<1:19:46,  2.30s/it]\u001b[A\n"," 81% 9132/11208 [44:21<1:19:54,  2.31s/it]\u001b[A\n"," 81% 9133/11208 [44:23<1:20:06,  2.32s/it]\u001b[A\n"," 81% 9134/11208 [44:25<1:20:17,  2.32s/it]\u001b[A\n"," 82% 9135/11208 [44:28<1:20:15,  2.32s/it]\u001b[A\n"," 82% 9136/11208 [44:30<1:20:25,  2.33s/it]\u001b[A\n"," 82% 9137/11208 [44:32<1:20:19,  2.33s/it]\u001b[A\n"," 82% 9138/11208 [44:35<1:20:17,  2.33s/it]\u001b[A\n"," 82% 9139/11208 [44:37<1:20:21,  2.33s/it]\u001b[A\n"," 82% 9140/11208 [44:39<1:20:21,  2.33s/it]\u001b[A\n"," 82% 9141/11208 [44:42<1:20:22,  2.33s/it]\u001b[A\n"," 82% 9142/11208 [44:44<1:20:25,  2.34s/it]\u001b[A\n"," 82% 9143/11208 [44:46<1:20:26,  2.34s/it]\u001b[A\n"," 82% 9144/11208 [44:49<1:20:08,  2.33s/it]\u001b[A\n"," 82% 9145/11208 [44:51<1:20:10,  2.33s/it]\u001b[A\n"," 82% 9146/11208 [44:53<1:20:11,  2.33s/it]\u001b[A\n"," 82% 9147/11208 [44:56<1:20:09,  2.33s/it]\u001b[A\n"," 82% 9148/11208 [44:58<1:19:30,  2.32s/it]\u001b[A\n"," 82% 9149/11208 [45:00<1:19:34,  2.32s/it]\u001b[A\n"," 82% 9150/11208 [45:03<1:19:39,  2.32s/it]\u001b[A\n"," 82% 9151/11208 [45:05<1:19:46,  2.33s/it]\u001b[A\n"," 82% 9152/11208 [45:07<1:19:50,  2.33s/it]\u001b[A\n"," 82% 9153/11208 [45:10<1:19:48,  2.33s/it]\u001b[A\n"," 82% 9154/11208 [45:12<1:19:41,  2.33s/it]\u001b[A\n"," 82% 9155/11208 [45:14<1:19:49,  2.33s/it]\u001b[A\n"," 82% 9156/11208 [45:17<1:19:52,  2.34s/it]\u001b[A\n"," 82% 9157/11208 [45:19<1:19:56,  2.34s/it]\u001b[A\n"," 82% 9158/11208 [45:21<1:19:50,  2.34s/it]\u001b[A\n"," 82% 9159/11208 [45:24<1:19:48,  2.34s/it]\u001b[A\n"," 82% 9160/11208 [45:26<1:19:44,  2.34s/it]\u001b[A\n"," 82% 9161/11208 [45:28<1:19:18,  2.32s/it]\u001b[A\n"," 82% 9162/11208 [45:31<1:19:15,  2.32s/it]\u001b[A\n"," 82% 9163/11208 [45:33<1:19:22,  2.33s/it]\u001b[A\n"," 82% 9164/11208 [45:35<1:19:29,  2.33s/it]\u001b[A\n"," 82% 9165/11208 [45:38<1:19:24,  2.33s/it]\u001b[A\n"," 82% 9166/11208 [45:40<1:19:20,  2.33s/it]\u001b[A\n"," 82% 9167/11208 [45:42<1:19:24,  2.33s/it]\u001b[A\n"," 82% 9168/11208 [45:45<1:19:18,  2.33s/it]\u001b[A\n"," 82% 9169/11208 [45:47<1:19:18,  2.33s/it]\u001b[A\n"," 82% 9170/11208 [45:49<1:19:11,  2.33s/it]\u001b[A\n"," 82% 9171/11208 [45:52<1:18:18,  2.31s/it]\u001b[A\n"," 82% 9172/11208 [45:54<1:18:31,  2.31s/it]\u001b[A\n"," 82% 9173/11208 [45:56<1:18:42,  2.32s/it]\u001b[A\n"," 82% 9174/11208 [45:59<1:18:49,  2.33s/it]\u001b[A\n"," 82% 9175/11208 [46:01<1:18:40,  2.32s/it]\u001b[A\n"," 82% 9176/11208 [46:03<1:18:39,  2.32s/it]\u001b[A\n"," 82% 9177/11208 [46:06<1:18:52,  2.33s/it]\u001b[A\n"," 82% 9178/11208 [46:08<1:18:56,  2.33s/it]\u001b[A\n"," 82% 9179/11208 [46:10<1:19:02,  2.34s/it]\u001b[A\n"," 82% 9180/11208 [46:13<1:19:08,  2.34s/it]\u001b[A\n"," 82% 9181/11208 [46:15<1:19:10,  2.34s/it]\u001b[A\n"," 82% 9182/11208 [46:17<1:18:50,  2.33s/it]\u001b[A\n"," 82% 9183/11208 [46:20<1:18:45,  2.33s/it]\u001b[A\n"," 82% 9184/11208 [46:22<1:17:28,  2.30s/it]\u001b[A\n"," 82% 9185/11208 [46:24<1:17:43,  2.31s/it]\u001b[A\n"," 82% 9186/11208 [46:26<1:18:01,  2.32s/it]\u001b[A\n"," 82% 9187/11208 [46:29<1:18:04,  2.32s/it]\u001b[A\n"," 82% 9188/11208 [46:31<1:18:05,  2.32s/it]\u001b[A\n"," 82% 9189/11208 [46:33<1:17:22,  2.30s/it]\u001b[A\n"," 82% 9190/11208 [46:36<1:17:21,  2.30s/it]\u001b[A\n"," 82% 9191/11208 [46:38<1:16:32,  2.28s/it]\u001b[A\n"," 82% 9192/11208 [46:40<1:17:09,  2.30s/it]\u001b[A\n"," 82% 9193/11208 [46:43<1:17:27,  2.31s/it]\u001b[A\n"," 82% 9194/11208 [46:45<1:17:39,  2.31s/it]\u001b[A\n"," 82% 9195/11208 [46:47<1:17:42,  2.32s/it]\u001b[A\n"," 82% 9196/11208 [46:50<1:17:47,  2.32s/it]\u001b[A\n"," 82% 9197/11208 [46:52<1:16:29,  2.28s/it]\u001b[A\n"," 82% 9198/11208 [46:54<1:16:56,  2.30s/it]\u001b[A\n"," 82% 9199/11208 [46:56<1:17:04,  2.30s/it]\u001b[A\n"," 82% 9200/11208 [46:59<1:17:19,  2.31s/it]\u001b[A\n","\u001b[A{'loss': 1.6764, 'learning_rate': 8.95788722341185e-06, 'epoch': 2.46}\n","\n"," 82% 9200/11208 [46:59<1:17:19,  2.31s/it]\u001b[A\n"," 82% 9201/11208 [47:01<1:17:27,  2.32s/it]\u001b[A\n"," 82% 9202/11208 [47:03<1:17:25,  2.32s/it]\u001b[A\n"," 82% 9203/11208 [47:06<1:16:50,  2.30s/it]\u001b[A\n"," 82% 9204/11208 [47:08<1:16:54,  2.30s/it]\u001b[A\n"," 82% 9205/11208 [47:10<1:17:05,  2.31s/it]\u001b[A\n"," 82% 9206/11208 [47:13<1:16:55,  2.31s/it]\u001b[A\n"," 82% 9207/11208 [47:15<1:17:10,  2.31s/it]\u001b[A\n"," 82% 9208/11208 [47:17<1:17:02,  2.31s/it]\u001b[A\n"," 82% 9209/11208 [47:19<1:17:12,  2.32s/it]\u001b[A\n"," 82% 9210/11208 [47:22<1:17:11,  2.32s/it]\u001b[A\n"," 82% 9211/11208 [47:24<1:17:12,  2.32s/it]\u001b[A\n"," 82% 9212/11208 [47:26<1:17:12,  2.32s/it]\u001b[A\n"," 82% 9213/11208 [47:29<1:17:13,  2.32s/it]\u001b[A\n"," 82% 9214/11208 [47:31<1:17:16,  2.33s/it]\u001b[A\n"," 82% 9215/11208 [47:33<1:17:16,  2.33s/it]\u001b[A\n"," 82% 9216/11208 [47:36<1:17:04,  2.32s/it]\u001b[A\n"," 82% 9217/11208 [47:38<1:17:04,  2.32s/it]\u001b[A\n"," 82% 9218/11208 [47:40<1:16:52,  2.32s/it]\u001b[A\n"," 82% 9219/11208 [47:43<1:17:05,  2.33s/it]\u001b[A\n"," 82% 9220/11208 [47:45<1:16:49,  2.32s/it]\u001b[A\n"," 82% 9221/11208 [47:47<1:17:00,  2.33s/it]\u001b[A\n"," 82% 9222/11208 [47:50<1:16:50,  2.32s/it]\u001b[A\n"," 82% 9223/11208 [47:52<1:16:57,  2.33s/it]\u001b[A\n"," 82% 9224/11208 [47:54<1:17:02,  2.33s/it]\u001b[A\n"," 82% 9225/11208 [47:57<1:17:00,  2.33s/it]\u001b[A\n"," 82% 9226/11208 [47:59<1:17:01,  2.33s/it]\u001b[A\n"," 82% 9227/11208 [48:01<1:17:01,  2.33s/it]\u001b[A\n"," 82% 9228/11208 [48:04<1:16:49,  2.33s/it]\u001b[A\n"," 82% 9229/11208 [48:06<1:16:58,  2.33s/it]\u001b[A\n"," 82% 9230/11208 [48:08<1:16:56,  2.33s/it]\u001b[A\n"," 82% 9231/11208 [48:11<1:16:55,  2.33s/it]\u001b[A\n"," 82% 9232/11208 [48:13<1:16:57,  2.34s/it]\u001b[A\n"," 82% 9233/11208 [48:15<1:16:20,  2.32s/it]\u001b[A\n"," 82% 9234/11208 [48:17<1:14:40,  2.27s/it]\u001b[A\n"," 82% 9235/11208 [48:20<1:15:22,  2.29s/it]\u001b[A\n"," 82% 9236/11208 [48:22<1:15:35,  2.30s/it]\u001b[A\n"," 82% 9237/11208 [48:24<1:15:47,  2.31s/it]\u001b[A\n"," 82% 9238/11208 [48:27<1:16:04,  2.32s/it]\u001b[A\n"," 82% 9239/11208 [48:29<1:16:16,  2.32s/it]\u001b[A\n"," 82% 9240/11208 [48:31<1:16:21,  2.33s/it]\u001b[A\n"," 82% 9241/11208 [48:34<1:16:17,  2.33s/it]\u001b[A\n"," 82% 9242/11208 [48:36<1:16:21,  2.33s/it]\u001b[A\n"," 82% 9243/11208 [48:38<1:16:21,  2.33s/it]\u001b[A\n"," 82% 9244/11208 [48:41<1:16:19,  2.33s/it]\u001b[A\n"," 82% 9245/11208 [48:43<1:16:25,  2.34s/it]\u001b[A\n"," 82% 9246/11208 [48:45<1:15:37,  2.31s/it]\u001b[A\n"," 83% 9247/11208 [48:48<1:15:54,  2.32s/it]\u001b[A\n"," 83% 9248/11208 [48:50<1:16:01,  2.33s/it]\u001b[A\n"," 83% 9249/11208 [48:52<1:16:09,  2.33s/it]\u001b[A\n"," 83% 9250/11208 [48:55<1:15:59,  2.33s/it]\u001b[A\n"," 83% 9251/11208 [48:57<1:15:55,  2.33s/it]\u001b[A\n"," 83% 9252/11208 [48:59<1:15:57,  2.33s/it]\u001b[A\n"," 83% 9253/11208 [49:02<1:16:04,  2.33s/it]\u001b[A\n"," 83% 9254/11208 [49:04<1:16:04,  2.34s/it]\u001b[A\n"," 83% 9255/11208 [49:06<1:15:16,  2.31s/it]\u001b[A\n"," 83% 9256/11208 [49:09<1:15:32,  2.32s/it]\u001b[A\n"," 83% 9257/11208 [49:11<1:15:43,  2.33s/it]\u001b[A\n"," 83% 9258/11208 [49:13<1:15:40,  2.33s/it]\u001b[A\n"," 83% 9259/11208 [49:16<1:15:45,  2.33s/it]\u001b[A\n"," 83% 9260/11208 [49:18<1:14:40,  2.30s/it]\u001b[A\n"," 83% 9261/11208 [49:20<1:15:00,  2.31s/it]\u001b[A\n"," 83% 9262/11208 [49:23<1:15:09,  2.32s/it]\u001b[A\n"," 83% 9263/11208 [49:25<1:15:24,  2.33s/it]\u001b[A\n"," 83% 9264/11208 [49:27<1:15:23,  2.33s/it]\u001b[A\n"," 83% 9265/11208 [49:30<1:14:47,  2.31s/it]\u001b[A\n"," 83% 9266/11208 [49:32<1:14:58,  2.32s/it]\u001b[A\n"," 83% 9267/11208 [49:34<1:15:12,  2.32s/it]\u001b[A\n"," 83% 9268/11208 [49:37<1:15:13,  2.33s/it]\u001b[A\n"," 83% 9269/11208 [49:39<1:15:20,  2.33s/it]\u001b[A\n"," 83% 9270/11208 [49:41<1:14:56,  2.32s/it]\u001b[A\n"," 83% 9271/11208 [49:44<1:15:00,  2.32s/it]\u001b[A\n"," 83% 9272/11208 [49:46<1:14:59,  2.32s/it]\u001b[A\n"," 83% 9273/11208 [49:48<1:15:03,  2.33s/it]\u001b[A\n"," 83% 9274/11208 [49:50<1:14:46,  2.32s/it]\u001b[A\n"," 83% 9275/11208 [49:53<1:14:51,  2.32s/it]\u001b[A\n"," 83% 9276/11208 [49:55<1:14:56,  2.33s/it]\u001b[A\n"," 83% 9277/11208 [49:57<1:14:53,  2.33s/it]\u001b[A\n"," 83% 9278/11208 [50:00<1:14:41,  2.32s/it]\u001b[A\n"," 83% 9279/11208 [50:02<1:14:04,  2.30s/it]\u001b[A\n"," 83% 9280/11208 [50:04<1:14:15,  2.31s/it]\u001b[A\n"," 83% 9281/11208 [50:07<1:14:26,  2.32s/it]\u001b[A\n"," 83% 9282/11208 [50:09<1:14:35,  2.32s/it]\u001b[A\n"," 83% 9283/11208 [50:11<1:14:34,  2.32s/it]\u001b[A\n"," 83% 9284/11208 [50:14<1:14:30,  2.32s/it]\u001b[A\n"," 83% 9285/11208 [50:16<1:14:27,  2.32s/it]\u001b[A\n"," 83% 9286/11208 [50:18<1:14:29,  2.33s/it]\u001b[A\n"," 83% 9287/11208 [50:21<1:14:31,  2.33s/it]\u001b[A\n"," 83% 9288/11208 [50:23<1:14:26,  2.33s/it]\u001b[A\n"," 83% 9289/11208 [50:25<1:14:18,  2.32s/it]\u001b[A\n"," 83% 9290/11208 [50:28<1:14:12,  2.32s/it]\u001b[A\n"," 83% 9291/11208 [50:30<1:14:07,  2.32s/it]\u001b[A\n"," 83% 9292/11208 [50:32<1:14:12,  2.32s/it]\u001b[A\n"," 83% 9293/11208 [50:35<1:14:00,  2.32s/it]\u001b[A\n"," 83% 9294/11208 [50:37<1:14:04,  2.32s/it]\u001b[A\n"," 83% 9295/11208 [50:39<1:14:08,  2.33s/it]\u001b[A\n"," 83% 9296/11208 [50:42<1:13:57,  2.32s/it]\u001b[A\n"," 83% 9297/11208 [50:44<1:13:58,  2.32s/it]\u001b[A\n"," 83% 9298/11208 [50:46<1:13:58,  2.32s/it]\u001b[A\n"," 83% 9299/11208 [50:49<1:13:58,  2.32s/it]\u001b[A\n"," 83% 9300/11208 [50:51<1:14:00,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.7025, 'learning_rate': 8.511777301927195e-06, 'epoch': 2.49}\n","\n"," 83% 9300/11208 [50:51<1:14:00,  2.33s/it]\u001b[A\n"," 83% 9301/11208 [50:53<1:13:57,  2.33s/it]\u001b[A\n"," 83% 9302/11208 [50:56<1:14:00,  2.33s/it]\u001b[A\n"," 83% 9303/11208 [50:58<1:13:13,  2.31s/it]\u001b[A\n"," 83% 9304/11208 [51:00<1:13:37,  2.32s/it]\u001b[A\n"," 83% 9305/11208 [51:02<1:13:34,  2.32s/it]\u001b[A\n"," 83% 9306/11208 [51:05<1:13:36,  2.32s/it]\u001b[A\n"," 83% 9307/11208 [51:07<1:13:22,  2.32s/it]\u001b[A\n"," 83% 9308/11208 [51:09<1:13:32,  2.32s/it]\u001b[A\n"," 83% 9309/11208 [51:12<1:13:28,  2.32s/it]\u001b[A\n"," 83% 9310/11208 [51:14<1:13:24,  2.32s/it]\u001b[A\n"," 83% 9311/11208 [51:16<1:13:27,  2.32s/it]\u001b[A\n"," 83% 9312/11208 [51:19<1:13:27,  2.32s/it]\u001b[A\n"," 83% 9313/11208 [51:21<1:13:24,  2.32s/it]\u001b[A\n"," 83% 9314/11208 [51:23<1:13:18,  2.32s/it]\u001b[A\n"," 83% 9315/11208 [51:26<1:12:26,  2.30s/it]\u001b[A\n"," 83% 9316/11208 [51:28<1:12:16,  2.29s/it]\u001b[A\n"," 83% 9317/11208 [51:30<1:12:37,  2.30s/it]\u001b[A\n"," 83% 9318/11208 [51:33<1:12:49,  2.31s/it]\u001b[A\n"," 83% 9319/11208 [51:35<1:12:45,  2.31s/it]\u001b[A\n"," 83% 9320/11208 [51:37<1:10:59,  2.26s/it]\u001b[A\n"," 83% 9321/11208 [51:39<1:11:43,  2.28s/it]\u001b[A\n"," 83% 9322/11208 [51:41<1:10:09,  2.23s/it]\u001b[A\n"," 83% 9323/11208 [51:44<1:11:02,  2.26s/it]\u001b[A\n"," 83% 9324/11208 [51:46<1:11:37,  2.28s/it]\u001b[A\n"," 83% 9325/11208 [51:48<1:12:00,  2.29s/it]\u001b[A\n"," 83% 9326/11208 [51:51<1:12:18,  2.31s/it]\u001b[A\n"," 83% 9327/11208 [51:53<1:12:24,  2.31s/it]\u001b[A\n"," 83% 9328/11208 [51:55<1:12:33,  2.32s/it]\u001b[A\n"," 83% 9329/11208 [51:58<1:12:32,  2.32s/it]\u001b[A\n"," 83% 9330/11208 [52:00<1:12:39,  2.32s/it]\u001b[A\n"," 83% 9331/11208 [52:02<1:12:41,  2.32s/it]\u001b[A\n"," 83% 9332/11208 [52:05<1:12:35,  2.32s/it]\u001b[A\n"," 83% 9333/11208 [52:07<1:12:32,  2.32s/it]\u001b[A\n"," 83% 9334/11208 [52:09<1:11:46,  2.30s/it]\u001b[A\n"," 83% 9335/11208 [52:12<1:12:00,  2.31s/it]\u001b[A\n"," 83% 9336/11208 [52:14<1:12:12,  2.31s/it]\u001b[A\n"," 83% 9337/11208 [52:16<1:12:19,  2.32s/it]\u001b[A\n"," 83% 9338/11208 [52:19<1:12:02,  2.31s/it]\u001b[A\n"," 83% 9339/11208 [52:21<1:12:09,  2.32s/it]\u001b[A\n"," 83% 9340/11208 [52:23<1:12:10,  2.32s/it]\u001b[A\n"," 83% 9341/11208 [52:26<1:12:18,  2.32s/it]\u001b[A\n"," 83% 9342/11208 [52:28<1:12:26,  2.33s/it]\u001b[A\n"," 83% 9343/11208 [52:30<1:12:22,  2.33s/it]\u001b[A\n"," 83% 9344/11208 [52:33<1:12:15,  2.33s/it]\u001b[A\n"," 83% 9345/11208 [52:35<1:12:13,  2.33s/it]\u001b[A\n"," 83% 9346/11208 [52:37<1:11:29,  2.30s/it]\u001b[A\n"," 83% 9347/11208 [52:39<1:11:35,  2.31s/it]\u001b[A\n"," 83% 9348/11208 [52:42<1:10:45,  2.28s/it]\u001b[A\n"," 83% 9349/11208 [52:44<1:11:03,  2.29s/it]\u001b[A\n"," 83% 9350/11208 [52:46<1:11:16,  2.30s/it]\u001b[A\n"," 83% 9351/11208 [52:49<1:11:14,  2.30s/it]\u001b[A\n"," 83% 9352/11208 [52:51<1:11:17,  2.30s/it]\u001b[A\n"," 83% 9353/11208 [52:53<1:11:23,  2.31s/it]\u001b[A\n"," 83% 9354/11208 [52:56<1:11:20,  2.31s/it]\u001b[A\n"," 83% 9355/11208 [52:58<1:10:28,  2.28s/it]\u001b[A\n"," 83% 9356/11208 [53:00<1:10:52,  2.30s/it]\u001b[A\n"," 83% 9357/11208 [53:02<1:11:04,  2.30s/it]\u001b[A\n"," 83% 9358/11208 [53:05<1:11:10,  2.31s/it]\u001b[A\n"," 84% 9359/11208 [53:07<1:11:12,  2.31s/it]\u001b[A\n"," 84% 9360/11208 [53:09<1:11:21,  2.32s/it]\u001b[A\n"," 84% 9361/11208 [53:12<1:11:25,  2.32s/it]\u001b[A\n"," 84% 9362/11208 [53:14<1:09:50,  2.27s/it]\u001b[A\n"," 84% 9363/11208 [53:16<1:10:23,  2.29s/it]\u001b[A\n"," 84% 9364/11208 [53:19<1:10:40,  2.30s/it]\u001b[A\n"," 84% 9365/11208 [53:21<1:10:55,  2.31s/it]\u001b[A\n"," 84% 9366/11208 [53:23<1:10:52,  2.31s/it]\u001b[A\n"," 84% 9367/11208 [53:25<1:10:56,  2.31s/it]\u001b[A\n"," 84% 9368/11208 [53:28<1:10:59,  2.32s/it]\u001b[A\n"," 84% 9369/11208 [53:30<1:10:29,  2.30s/it]\u001b[A\n"," 84% 9370/11208 [53:32<1:10:42,  2.31s/it]\u001b[A\n"," 84% 9371/11208 [53:35<1:10:43,  2.31s/it]\u001b[A\n"," 84% 9372/11208 [53:37<1:10:44,  2.31s/it]\u001b[A\n"," 84% 9373/11208 [53:39<1:10:46,  2.31s/it]\u001b[A\n"," 84% 9374/11208 [53:42<1:09:41,  2.28s/it]\u001b[A\n"," 84% 9375/11208 [53:44<1:10:09,  2.30s/it]\u001b[A\n"," 84% 9376/11208 [53:46<1:09:59,  2.29s/it]\u001b[A\n"," 84% 9377/11208 [53:48<1:10:24,  2.31s/it]\u001b[A\n"," 84% 9378/11208 [53:51<1:10:34,  2.31s/it]\u001b[A\n"," 84% 9379/11208 [53:53<1:10:39,  2.32s/it]\u001b[A\n"," 84% 9380/11208 [53:55<1:10:39,  2.32s/it]\u001b[A\n"," 84% 9381/11208 [53:58<1:10:35,  2.32s/it]\u001b[A\n"," 84% 9382/11208 [54:00<1:10:37,  2.32s/it]\u001b[A\n"," 84% 9383/11208 [54:02<1:10:41,  2.32s/it]\u001b[A\n"," 84% 9384/11208 [54:05<1:10:38,  2.32s/it]\u001b[A\n"," 84% 9385/11208 [54:07<1:10:39,  2.33s/it]\u001b[A\n"," 84% 9386/11208 [54:09<1:10:36,  2.33s/it]\u001b[A\n"," 84% 9387/11208 [54:12<1:10:31,  2.32s/it]\u001b[A\n"," 84% 9388/11208 [54:14<1:10:33,  2.33s/it]\u001b[A\n"," 84% 9389/11208 [54:16<1:10:25,  2.32s/it]\u001b[A\n"," 84% 9390/11208 [54:19<1:10:21,  2.32s/it]\u001b[A\n"," 84% 9391/11208 [54:21<1:10:18,  2.32s/it]\u001b[A\n"," 84% 9392/11208 [54:23<1:10:19,  2.32s/it]\u001b[A\n"," 84% 9393/11208 [54:26<1:10:20,  2.33s/it]\u001b[A\n"," 84% 9394/11208 [54:28<1:10:20,  2.33s/it]\u001b[A\n"," 84% 9395/11208 [54:30<1:09:50,  2.31s/it]\u001b[A\n"," 84% 9396/11208 [54:33<1:09:51,  2.31s/it]\u001b[A\n"," 84% 9397/11208 [54:35<1:10:00,  2.32s/it]\u001b[A\n"," 84% 9398/11208 [54:37<1:09:26,  2.30s/it]\u001b[A\n"," 84% 9399/11208 [54:40<1:09:40,  2.31s/it]\u001b[A\n"," 84% 9400/11208 [54:42<1:09:43,  2.31s/it]\u001b[A\n","\u001b[A{'loss': 1.66, 'learning_rate': 8.065667380442542e-06, 'epoch': 2.52}\n","\n"," 84% 9400/11208 [54:42<1:09:43,  2.31s/it]\u001b[A\n"," 84% 9401/11208 [54:44<1:09:42,  2.31s/it]\u001b[A\n"," 84% 9402/11208 [54:47<1:09:53,  2.32s/it]\u001b[A\n"," 84% 9403/11208 [54:49<1:09:55,  2.32s/it]\u001b[A\n"," 84% 9404/11208 [54:51<1:09:51,  2.32s/it]\u001b[A\n"," 84% 9405/11208 [54:53<1:08:19,  2.27s/it]\u001b[A\n"," 84% 9406/11208 [54:56<1:08:39,  2.29s/it]\u001b[A\n"," 84% 9407/11208 [54:58<1:08:57,  2.30s/it]\u001b[A\n"," 84% 9408/11208 [55:00<1:09:12,  2.31s/it]\u001b[A\n"," 84% 9409/11208 [55:03<1:09:21,  2.31s/it]\u001b[A\n"," 84% 9410/11208 [55:05<1:09:34,  2.32s/it]\u001b[A\n"," 84% 9411/11208 [55:07<1:09:25,  2.32s/it]\u001b[A\n"," 84% 9412/11208 [55:10<1:09:24,  2.32s/it]\u001b[A\n"," 84% 9413/11208 [55:12<1:09:29,  2.32s/it]\u001b[A\n"," 84% 9414/11208 [55:14<1:09:35,  2.33s/it]\u001b[A\n"," 84% 9415/11208 [55:17<1:09:32,  2.33s/it]\u001b[A\n"," 84% 9416/11208 [55:19<1:09:32,  2.33s/it]\u001b[A\n"," 84% 9417/11208 [55:21<1:09:23,  2.32s/it]\u001b[A\n"," 84% 9418/11208 [55:23<1:08:25,  2.29s/it]\u001b[A\n"," 84% 9419/11208 [55:26<1:08:39,  2.30s/it]\u001b[A\n"," 84% 9420/11208 [55:28<1:08:04,  2.28s/it]\u001b[A\n"," 84% 9421/11208 [55:30<1:08:28,  2.30s/it]\u001b[A\n"," 84% 9422/11208 [55:33<1:08:41,  2.31s/it]\u001b[A\n"," 84% 9423/11208 [55:35<1:08:34,  2.30s/it]\u001b[A\n"," 84% 9424/11208 [55:37<1:07:17,  2.26s/it]\u001b[A\n"," 84% 9425/11208 [55:39<1:07:42,  2.28s/it]\u001b[A\n"," 84% 9426/11208 [55:42<1:08:07,  2.29s/it]\u001b[A\n"," 84% 9427/11208 [55:44<1:08:23,  2.30s/it]\u001b[A\n"," 84% 9428/11208 [55:46<1:08:36,  2.31s/it]\u001b[A\n"," 84% 9429/11208 [55:49<1:08:33,  2.31s/it]\u001b[A\n"," 84% 9430/11208 [55:51<1:06:36,  2.25s/it]\u001b[A\n"," 84% 9431/11208 [55:53<1:07:12,  2.27s/it]\u001b[A\n"," 84% 9432/11208 [55:55<1:06:55,  2.26s/it]\u001b[A\n"," 84% 9433/11208 [55:58<1:06:57,  2.26s/it]\u001b[A\n"," 84% 9434/11208 [56:00<1:07:27,  2.28s/it]\u001b[A\n"," 84% 9435/11208 [56:02<1:07:45,  2.29s/it]\u001b[A\n"," 84% 9436/11208 [56:05<1:07:54,  2.30s/it]\u001b[A\n"," 84% 9437/11208 [56:07<1:08:05,  2.31s/it]\u001b[A\n"," 84% 9438/11208 [56:09<1:07:21,  2.28s/it]\u001b[A\n"," 84% 9439/11208 [56:12<1:07:36,  2.29s/it]\u001b[A\n"," 84% 9440/11208 [56:14<1:07:48,  2.30s/it]\u001b[A\n"," 84% 9441/11208 [56:16<1:08:07,  2.31s/it]\u001b[A\n"," 84% 9442/11208 [56:18<1:08:11,  2.32s/it]\u001b[A\n"," 84% 9443/11208 [56:21<1:08:15,  2.32s/it]\u001b[A\n"," 84% 9444/11208 [56:23<1:08:12,  2.32s/it]\u001b[A\n"," 84% 9445/11208 [56:25<1:07:40,  2.30s/it]\u001b[A\n"," 84% 9446/11208 [56:28<1:07:52,  2.31s/it]\u001b[A\n"," 84% 9447/11208 [56:30<1:08:00,  2.32s/it]\u001b[A\n"," 84% 9448/11208 [56:32<1:07:58,  2.32s/it]\u001b[A\n"," 84% 9449/11208 [56:35<1:08:01,  2.32s/it]\u001b[A\n"," 84% 9450/11208 [56:37<1:08:00,  2.32s/it]\u001b[A\n"," 84% 9451/11208 [56:39<1:08:01,  2.32s/it]\u001b[A\n"," 84% 9452/11208 [56:42<1:08:02,  2.33s/it]\u001b[A\n"," 84% 9453/11208 [56:44<1:06:11,  2.26s/it]\u001b[A\n"," 84% 9454/11208 [56:46<1:06:30,  2.28s/it]\u001b[A\n"," 84% 9455/11208 [56:48<1:06:51,  2.29s/it]\u001b[A\n"," 84% 9456/11208 [56:51<1:07:10,  2.30s/it]\u001b[A\n"," 84% 9457/11208 [56:53<1:06:49,  2.29s/it]\u001b[A\n"," 84% 9458/11208 [56:55<1:07:08,  2.30s/it]\u001b[A\n"," 84% 9459/11208 [56:58<1:07:15,  2.31s/it]\u001b[A\n"," 84% 9460/11208 [57:00<1:07:24,  2.31s/it]\u001b[A\n"," 84% 9461/11208 [57:02<1:07:30,  2.32s/it]\u001b[A\n"," 84% 9462/11208 [57:05<1:07:26,  2.32s/it]\u001b[A\n"," 84% 9463/11208 [57:07<1:07:23,  2.32s/it]\u001b[A\n"," 84% 9464/11208 [57:09<1:07:23,  2.32s/it]\u001b[A\n"," 84% 9465/11208 [57:12<1:07:24,  2.32s/it]\u001b[A\n"," 84% 9466/11208 [57:14<1:07:11,  2.31s/it]\u001b[A\n"," 84% 9467/11208 [57:16<1:05:51,  2.27s/it]\u001b[A\n"," 84% 9468/11208 [57:18<1:06:22,  2.29s/it]\u001b[A\n"," 84% 9469/11208 [57:21<1:06:40,  2.30s/it]\u001b[A\n"," 84% 9470/11208 [57:23<1:06:46,  2.31s/it]\u001b[A\n"," 85% 9471/11208 [57:25<1:06:57,  2.31s/it]\u001b[A\n"," 85% 9472/11208 [57:28<1:06:55,  2.31s/it]\u001b[A\n"," 85% 9473/11208 [57:30<1:06:59,  2.32s/it]\u001b[A\n"," 85% 9474/11208 [57:32<1:06:50,  2.31s/it]\u001b[A\n"," 85% 9475/11208 [57:35<1:06:55,  2.32s/it]\u001b[A\n"," 85% 9476/11208 [57:37<1:06:55,  2.32s/it]\u001b[A\n"," 85% 9477/11208 [57:39<1:06:58,  2.32s/it]\u001b[A\n"," 85% 9478/11208 [57:42<1:07:00,  2.32s/it]\u001b[A\n"," 85% 9479/11208 [57:44<1:06:33,  2.31s/it]\u001b[A\n"," 85% 9480/11208 [57:46<1:06:34,  2.31s/it]\u001b[A\n"," 85% 9481/11208 [57:49<1:06:39,  2.32s/it]\u001b[A\n"," 85% 9482/11208 [57:51<1:06:38,  2.32s/it]\u001b[A\n"," 85% 9483/11208 [57:53<1:06:45,  2.32s/it]\u001b[A\n"," 85% 9484/11208 [57:56<1:06:44,  2.32s/it]\u001b[A\n"," 85% 9485/11208 [57:58<1:06:44,  2.32s/it]\u001b[A\n"," 85% 9486/11208 [58:00<1:05:15,  2.27s/it]\u001b[A\n"," 85% 9487/11208 [58:02<1:05:45,  2.29s/it]\u001b[A\n"," 85% 9488/11208 [58:05<1:06:00,  2.30s/it]\u001b[A\n"," 85% 9489/11208 [58:07<1:06:11,  2.31s/it]\u001b[A\n"," 85% 9490/11208 [58:09<1:06:05,  2.31s/it]\u001b[A\n"," 85% 9491/11208 [58:12<1:06:01,  2.31s/it]\u001b[A\n"," 85% 9492/11208 [58:14<1:06:13,  2.32s/it]\u001b[A\n"," 85% 9493/11208 [58:16<1:04:41,  2.26s/it]\u001b[A\n"," 85% 9494/11208 [58:18<1:05:17,  2.29s/it]\u001b[A\n"," 85% 9495/11208 [58:21<1:05:39,  2.30s/it]\u001b[A\n"," 85% 9496/11208 [58:23<1:05:51,  2.31s/it]\u001b[A\n"," 85% 9497/11208 [58:25<1:06:01,  2.32s/it]\u001b[A\n"," 85% 9498/11208 [58:28<1:06:10,  2.32s/it]\u001b[A\n"," 85% 9499/11208 [58:30<1:06:14,  2.33s/it]\u001b[A\n"," 85% 9500/11208 [58:32<1:06:12,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6394, 'learning_rate': 7.619557458957888e-06, 'epoch': 2.54}\n","\n"," 85% 9500/11208 [58:33<1:06:12,  2.33s/it]\u001b[A\n"," 85% 9501/11208 [58:35<1:06:20,  2.33s/it]\u001b[A\n"," 85% 9502/11208 [58:37<1:06:06,  2.33s/it]\u001b[A\n"," 85% 9503/11208 [58:39<1:06:12,  2.33s/it]\u001b[A\n"," 85% 9504/11208 [58:42<1:05:51,  2.32s/it]\u001b[A\n"," 85% 9505/11208 [58:44<1:05:56,  2.32s/it]\u001b[A\n"," 85% 9506/11208 [58:46<1:05:13,  2.30s/it]\u001b[A\n"," 85% 9507/11208 [58:49<1:04:52,  2.29s/it]\u001b[A\n"," 85% 9508/11208 [58:51<1:05:11,  2.30s/it]\u001b[A\n"," 85% 9509/11208 [58:53<1:05:20,  2.31s/it]\u001b[A\n"," 85% 9510/11208 [58:56<1:05:21,  2.31s/it]\u001b[A\n"," 85% 9511/11208 [58:58<1:05:20,  2.31s/it]\u001b[A\n"," 85% 9512/11208 [59:00<1:04:22,  2.28s/it]\u001b[A\n"," 85% 9513/11208 [59:02<1:04:51,  2.30s/it]\u001b[A\n"," 85% 9514/11208 [59:05<1:05:07,  2.31s/it]\u001b[A\n"," 85% 9515/11208 [59:07<1:05:23,  2.32s/it]\u001b[A\n"," 85% 9516/11208 [59:09<1:05:24,  2.32s/it]\u001b[A\n"," 85% 9517/11208 [59:12<1:05:30,  2.32s/it]\u001b[A\n"," 85% 9518/11208 [59:14<1:05:35,  2.33s/it]\u001b[A\n"," 85% 9519/11208 [59:16<1:05:39,  2.33s/it]\u001b[A\n"," 85% 9520/11208 [59:19<1:05:38,  2.33s/it]\u001b[A\n"," 85% 9521/11208 [59:21<1:05:32,  2.33s/it]\u001b[A\n"," 85% 9522/11208 [59:23<1:04:34,  2.30s/it]\u001b[A\n"," 85% 9523/11208 [59:26<1:05:00,  2.31s/it]\u001b[A\n"," 85% 9524/11208 [59:28<1:05:16,  2.33s/it]\u001b[A\n"," 85% 9525/11208 [59:30<1:05:20,  2.33s/it]\u001b[A\n"," 85% 9526/11208 [59:33<1:05:19,  2.33s/it]\u001b[A\n"," 85% 9527/11208 [59:35<1:05:26,  2.34s/it]\u001b[A\n"," 85% 9528/11208 [59:37<1:05:15,  2.33s/it]\u001b[A\n"," 85% 9529/11208 [59:40<1:05:06,  2.33s/it]\u001b[A\n"," 85% 9530/11208 [59:42<1:05:01,  2.33s/it]\u001b[A\n"," 85% 9531/11208 [59:44<1:04:38,  2.31s/it]\u001b[A\n"," 85% 9532/11208 [59:47<1:04:45,  2.32s/it]\u001b[A\n"," 85% 9533/11208 [59:49<1:04:49,  2.32s/it]\u001b[A\n"," 85% 9534/11208 [59:51<1:04:49,  2.32s/it]\u001b[A\n"," 85% 9535/11208 [59:54<1:04:45,  2.32s/it]\u001b[A\n"," 85% 9536/11208 [59:56<1:04:41,  2.32s/it]\u001b[A\n"," 85% 9537/11208 [59:58<1:04:37,  2.32s/it]\u001b[A\n"," 85% 9538/11208 [1:00:01<1:04:46,  2.33s/it]\u001b[A\n"," 85% 9539/11208 [1:00:03<1:04:39,  2.32s/it]\u001b[A\n"," 85% 9540/11208 [1:00:05<1:04:38,  2.32s/it]\u001b[A\n"," 85% 9541/11208 [1:00:08<1:04:41,  2.33s/it]\u001b[A\n"," 85% 9542/11208 [1:00:10<1:04:34,  2.33s/it]\u001b[A\n"," 85% 9543/11208 [1:00:12<1:04:33,  2.33s/it]\u001b[A\n"," 85% 9544/11208 [1:00:14<1:04:33,  2.33s/it]\u001b[A\n"," 85% 9545/11208 [1:00:17<1:04:30,  2.33s/it]\u001b[A\n"," 85% 9546/11208 [1:00:19<1:04:33,  2.33s/it]\u001b[A\n"," 85% 9547/11208 [1:00:21<1:02:38,  2.26s/it]\u001b[A\n"," 85% 9548/11208 [1:00:23<1:02:03,  2.24s/it]\u001b[A\n"," 85% 9549/11208 [1:00:26<1:02:45,  2.27s/it]\u001b[A\n"," 85% 9550/11208 [1:00:28<1:03:16,  2.29s/it]\u001b[A\n"," 85% 9551/11208 [1:00:30<1:03:41,  2.31s/it]\u001b[A\n"," 85% 9552/11208 [1:00:33<1:03:49,  2.31s/it]\u001b[A\n"," 85% 9553/11208 [1:00:35<1:04:03,  2.32s/it]\u001b[A\n"," 85% 9554/11208 [1:00:37<1:04:10,  2.33s/it]\u001b[A\n"," 85% 9555/11208 [1:00:40<1:04:04,  2.33s/it]\u001b[A\n"," 85% 9556/11208 [1:00:42<1:03:34,  2.31s/it]\u001b[A\n"," 85% 9557/11208 [1:00:44<1:03:37,  2.31s/it]\u001b[A\n"," 85% 9558/11208 [1:00:47<1:03:47,  2.32s/it]\u001b[A\n"," 85% 9559/11208 [1:00:49<1:03:55,  2.33s/it]\u001b[A\n"," 85% 9560/11208 [1:00:51<1:04:03,  2.33s/it]\u001b[A\n"," 85% 9561/11208 [1:00:54<1:03:22,  2.31s/it]\u001b[A\n"," 85% 9562/11208 [1:00:56<1:03:36,  2.32s/it]\u001b[A\n"," 85% 9563/11208 [1:00:58<1:03:02,  2.30s/it]\u001b[A\n"," 85% 9564/11208 [1:01:01<1:03:08,  2.30s/it]\u001b[A\n"," 85% 9565/11208 [1:01:03<1:02:31,  2.28s/it]\u001b[A\n"," 85% 9566/11208 [1:01:05<1:02:52,  2.30s/it]\u001b[A\n"," 85% 9567/11208 [1:01:07<1:03:11,  2.31s/it]\u001b[A\n"," 85% 9568/11208 [1:01:10<1:03:22,  2.32s/it]\u001b[A\n"," 85% 9569/11208 [1:01:12<1:03:25,  2.32s/it]\u001b[A\n"," 85% 9570/11208 [1:01:14<1:03:30,  2.33s/it]\u001b[A\n"," 85% 9571/11208 [1:01:17<1:02:23,  2.29s/it]\u001b[A\n"," 85% 9572/11208 [1:01:19<1:02:38,  2.30s/it]\u001b[A\n"," 85% 9573/11208 [1:01:21<1:02:04,  2.28s/it]\u001b[A\n"," 85% 9574/11208 [1:01:24<1:02:33,  2.30s/it]\u001b[A\n"," 85% 9575/11208 [1:01:26<1:02:55,  2.31s/it]\u001b[A\n"," 85% 9576/11208 [1:01:28<1:03:02,  2.32s/it]\u001b[A\n"," 85% 9577/11208 [1:01:30<1:01:40,  2.27s/it]\u001b[A\n"," 85% 9578/11208 [1:01:33<1:02:05,  2.29s/it]\u001b[A\n"," 85% 9579/11208 [1:01:35<1:02:26,  2.30s/it]\u001b[A\n"," 85% 9580/11208 [1:01:37<1:02:46,  2.31s/it]\u001b[A\n"," 85% 9581/11208 [1:01:40<1:02:46,  2.32s/it]\u001b[A\n"," 85% 9582/11208 [1:01:42<1:02:51,  2.32s/it]\u001b[A\n"," 86% 9583/11208 [1:01:44<1:02:54,  2.32s/it]\u001b[A\n"," 86% 9584/11208 [1:01:47<1:02:51,  2.32s/it]\u001b[A\n"," 86% 9585/11208 [1:01:49<1:03:01,  2.33s/it]\u001b[A\n"," 86% 9586/11208 [1:01:51<1:03:07,  2.33s/it]\u001b[A\n"," 86% 9587/11208 [1:01:54<1:02:14,  2.30s/it]\u001b[A\n"," 86% 9588/11208 [1:01:56<1:02:33,  2.32s/it]\u001b[A\n"," 86% 9589/11208 [1:01:58<1:02:41,  2.32s/it]\u001b[A\n"," 86% 9590/11208 [1:02:01<1:02:38,  2.32s/it]\u001b[A\n"," 86% 9591/11208 [1:02:03<1:02:37,  2.32s/it]\u001b[A\n"," 86% 9592/11208 [1:02:05<1:02:33,  2.32s/it]\u001b[A\n"," 86% 9593/11208 [1:02:08<1:02:31,  2.32s/it]\u001b[A\n"," 86% 9594/11208 [1:02:10<1:02:36,  2.33s/it]\u001b[A\n"," 86% 9595/11208 [1:02:12<1:02:49,  2.34s/it]\u001b[A\n"," 86% 9596/11208 [1:02:15<1:02:56,  2.34s/it]\u001b[A\n"," 86% 9597/11208 [1:02:17<1:02:44,  2.34s/it]\u001b[A\n"," 86% 9598/11208 [1:02:19<1:02:35,  2.33s/it]\u001b[A\n"," 86% 9599/11208 [1:02:22<1:02:35,  2.33s/it]\u001b[A\n"," 86% 9600/11208 [1:02:24<1:02:25,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6661, 'learning_rate': 7.173447537473234e-06, 'epoch': 2.57}\n","\n"," 86% 9600/11208 [1:02:24<1:02:25,  2.33s/it]\u001b[A\n"," 86% 9601/11208 [1:02:26<1:02:16,  2.33s/it]\u001b[A\n"," 86% 9602/11208 [1:02:28<1:01:21,  2.29s/it]\u001b[A\n"," 86% 9603/11208 [1:02:31<1:00:46,  2.27s/it]\u001b[A\n"," 86% 9604/11208 [1:02:33<1:01:14,  2.29s/it]\u001b[A\n"," 86% 9605/11208 [1:02:35<1:01:28,  2.30s/it]\u001b[A\n"," 86% 9606/11208 [1:02:38<1:00:21,  2.26s/it]\u001b[A\n"," 86% 9607/11208 [1:02:40<1:00:47,  2.28s/it]\u001b[A\n"," 86% 9608/11208 [1:02:42<1:01:03,  2.29s/it]\u001b[A\n"," 86% 9609/11208 [1:02:44<1:01:08,  2.29s/it]\u001b[A\n"," 86% 9610/11208 [1:02:47<1:00:07,  2.26s/it]\u001b[A\n"," 86% 9611/11208 [1:02:49<1:00:40,  2.28s/it]\u001b[A\n"," 86% 9612/11208 [1:02:51<1:01:00,  2.29s/it]\u001b[A\n"," 86% 9613/11208 [1:02:54<1:00:25,  2.27s/it]\u001b[A\n"," 86% 9614/11208 [1:02:56<1:00:32,  2.28s/it]\u001b[A\n"," 86% 9615/11208 [1:02:58<1:00:47,  2.29s/it]\u001b[A\n"," 86% 9616/11208 [1:03:00<1:01:02,  2.30s/it]\u001b[A\n"," 86% 9617/11208 [1:03:03<1:01:02,  2.30s/it]\u001b[A\n"," 86% 9618/11208 [1:03:05<1:01:14,  2.31s/it]\u001b[A\n"," 86% 9619/11208 [1:03:07<1:01:22,  2.32s/it]\u001b[A\n"," 86% 9620/11208 [1:03:10<1:01:18,  2.32s/it]\u001b[A\n"," 86% 9621/11208 [1:03:12<58:54,  2.23s/it]  \u001b[A\n"," 86% 9622/11208 [1:03:14<59:42,  2.26s/it]\u001b[A\n"," 86% 9623/11208 [1:03:16<59:52,  2.27s/it]\u001b[A\n"," 86% 9624/11208 [1:03:19<1:00:17,  2.28s/it]\u001b[A\n"," 86% 9625/11208 [1:03:21<1:00:34,  2.30s/it]\u001b[A\n"," 86% 9626/11208 [1:03:23<1:00:50,  2.31s/it]\u001b[A\n"," 86% 9627/11208 [1:03:26<1:01:02,  2.32s/it]\u001b[A\n"," 86% 9628/11208 [1:03:28<1:01:00,  2.32s/it]\u001b[A\n"," 86% 9629/11208 [1:03:30<1:01:05,  2.32s/it]\u001b[A\n"," 86% 9630/11208 [1:03:33<1:01:00,  2.32s/it]\u001b[A\n"," 86% 9631/11208 [1:03:35<1:01:01,  2.32s/it]\u001b[A\n"," 86% 9632/11208 [1:03:37<1:01:05,  2.33s/it]\u001b[A\n"," 86% 9633/11208 [1:03:39<59:24,  2.26s/it]  \u001b[A\n"," 86% 9634/11208 [1:03:42<59:56,  2.28s/it]\u001b[A\n"," 86% 9635/11208 [1:03:44<1:00:13,  2.30s/it]\u001b[A\n"," 86% 9636/11208 [1:03:46<1:00:17,  2.30s/it]\u001b[A\n"," 86% 9637/11208 [1:03:49<1:00:30,  2.31s/it]\u001b[A\n"," 86% 9638/11208 [1:03:51<1:00:34,  2.31s/it]\u001b[A\n"," 86% 9639/11208 [1:03:53<1:00:40,  2.32s/it]\u001b[A\n"," 86% 9640/11208 [1:03:56<1:00:11,  2.30s/it]\u001b[A\n"," 86% 9641/11208 [1:03:58<1:00:22,  2.31s/it]\u001b[A\n"," 86% 9642/11208 [1:04:00<1:00:29,  2.32s/it]\u001b[A\n"," 86% 9643/11208 [1:04:03<1:00:21,  2.31s/it]\u001b[A\n"," 86% 9644/11208 [1:04:05<1:00:24,  2.32s/it]\u001b[A\n"," 86% 9645/11208 [1:04:07<1:00:17,  2.31s/it]\u001b[A\n"," 86% 9646/11208 [1:04:10<1:00:23,  2.32s/it]\u001b[A\n"," 86% 9647/11208 [1:04:12<1:00:27,  2.32s/it]\u001b[A\n"," 86% 9648/11208 [1:04:14<1:00:24,  2.32s/it]\u001b[A\n"," 86% 9649/11208 [1:04:17<1:00:05,  2.31s/it]\u001b[A\n"," 86% 9650/11208 [1:04:19<1:00:05,  2.31s/it]\u001b[A\n"," 86% 9651/11208 [1:04:21<1:00:01,  2.31s/it]\u001b[A\n"," 86% 9652/11208 [1:04:24<1:00:04,  2.32s/it]\u001b[A\n"," 86% 9653/11208 [1:04:26<1:00:06,  2.32s/it]\u001b[A\n"," 86% 9654/11208 [1:04:28<59:59,  2.32s/it]  \u001b[A\n"," 86% 9655/11208 [1:04:30<57:42,  2.23s/it]\u001b[A\n"," 86% 9656/11208 [1:04:32<57:21,  2.22s/it]\u001b[A\n"," 86% 9657/11208 [1:04:35<58:04,  2.25s/it]\u001b[A\n"," 86% 9658/11208 [1:04:37<58:40,  2.27s/it]\u001b[A\n"," 86% 9659/11208 [1:04:39<59:01,  2.29s/it]\u001b[A\n"," 86% 9660/11208 [1:04:42<59:20,  2.30s/it]\u001b[A\n"," 86% 9661/11208 [1:04:44<59:25,  2.30s/it]\u001b[A\n"," 86% 9662/11208 [1:04:46<59:31,  2.31s/it]\u001b[A\n"," 86% 9663/11208 [1:04:49<59:36,  2.32s/it]\u001b[A\n"," 86% 9664/11208 [1:04:51<58:02,  2.26s/it]\u001b[A\n"," 86% 9665/11208 [1:04:53<58:35,  2.28s/it]\u001b[A\n"," 86% 9666/11208 [1:04:55<58:51,  2.29s/it]\u001b[A\n"," 86% 9667/11208 [1:04:58<59:11,  2.30s/it]\u001b[A\n"," 86% 9668/11208 [1:05:00<59:30,  2.32s/it]\u001b[A\n"," 86% 9669/11208 [1:05:02<59:48,  2.33s/it]\u001b[A\n"," 86% 9670/11208 [1:05:05<59:49,  2.33s/it]\u001b[A\n"," 86% 9671/11208 [1:05:07<59:39,  2.33s/it]\u001b[A\n"," 86% 9672/11208 [1:05:09<59:36,  2.33s/it]\u001b[A\n"," 86% 9673/11208 [1:05:12<58:38,  2.29s/it]\u001b[A\n"," 86% 9674/11208 [1:05:14<58:46,  2.30s/it]\u001b[A\n"," 86% 9675/11208 [1:05:16<58:56,  2.31s/it]\u001b[A\n"," 86% 9676/11208 [1:05:19<58:52,  2.31s/it]\u001b[A\n"," 86% 9677/11208 [1:05:21<57:48,  2.27s/it]\u001b[A\n"," 86% 9678/11208 [1:05:23<57:31,  2.26s/it]\u001b[A\n"," 86% 9679/11208 [1:05:25<57:54,  2.27s/it]\u001b[A\n"," 86% 9680/11208 [1:05:28<58:17,  2.29s/it]\u001b[A\n"," 86% 9681/11208 [1:05:30<58:27,  2.30s/it]\u001b[A\n"," 86% 9682/11208 [1:05:32<58:41,  2.31s/it]\u001b[A\n"," 86% 9683/11208 [1:05:35<58:47,  2.31s/it]\u001b[A\n"," 86% 9684/11208 [1:05:37<58:49,  2.32s/it]\u001b[A\n"," 86% 9685/11208 [1:05:39<58:55,  2.32s/it]\u001b[A\n"," 86% 9686/11208 [1:05:42<58:55,  2.32s/it]\u001b[A\n"," 86% 9687/11208 [1:05:44<58:55,  2.32s/it]\u001b[A\n"," 86% 9688/11208 [1:05:46<58:52,  2.32s/it]\u001b[A\n"," 86% 9689/11208 [1:05:49<58:50,  2.32s/it]\u001b[A\n"," 86% 9690/11208 [1:05:51<58:46,  2.32s/it]\u001b[A\n"," 86% 9691/11208 [1:05:53<58:49,  2.33s/it]\u001b[A\n"," 86% 9692/11208 [1:05:56<58:43,  2.32s/it]\u001b[A\n"," 86% 9693/11208 [1:05:58<58:36,  2.32s/it]\u001b[A\n"," 86% 9694/11208 [1:06:00<58:33,  2.32s/it]\u001b[A\n"," 87% 9695/11208 [1:06:02<58:30,  2.32s/it]\u001b[A\n"," 87% 9696/11208 [1:06:05<58:36,  2.33s/it]\u001b[A\n"," 87% 9697/11208 [1:06:07<58:32,  2.32s/it]\u001b[A\n"," 87% 9698/11208 [1:06:09<58:29,  2.32s/it]\u001b[A\n"," 87% 9699/11208 [1:06:12<58:20,  2.32s/it]\u001b[A\n"," 87% 9700/11208 [1:06:14<58:21,  2.32s/it]\u001b[A\n","\u001b[A{'loss': 1.639, 'learning_rate': 6.7273376159885795e-06, 'epoch': 2.6}\n","\n"," 87% 9700/11208 [1:06:14<58:21,  2.32s/it]\u001b[A\n"," 87% 9701/11208 [1:06:16<58:28,  2.33s/it]\u001b[A\n"," 87% 9702/11208 [1:06:19<58:31,  2.33s/it]\u001b[A\n"," 87% 9703/11208 [1:06:21<58:27,  2.33s/it]\u001b[A\n"," 87% 9704/11208 [1:06:23<58:25,  2.33s/it]\u001b[A\n"," 87% 9705/11208 [1:06:26<58:01,  2.32s/it]\u001b[A\n"," 87% 9706/11208 [1:06:28<58:02,  2.32s/it]\u001b[A\n"," 87% 9707/11208 [1:06:30<58:11,  2.33s/it]\u001b[A\n"," 87% 9708/11208 [1:06:33<58:14,  2.33s/it]\u001b[A\n"," 87% 9709/11208 [1:06:35<57:41,  2.31s/it]\u001b[A\n"," 87% 9710/11208 [1:06:37<57:50,  2.32s/it]\u001b[A\n"," 87% 9711/11208 [1:06:40<57:53,  2.32s/it]\u001b[A\n"," 87% 9712/11208 [1:06:42<58:01,  2.33s/it]\u001b[A\n"," 87% 9713/11208 [1:06:44<57:57,  2.33s/it]\u001b[A\n"," 87% 9714/11208 [1:06:47<57:57,  2.33s/it]\u001b[A\n"," 87% 9715/11208 [1:06:49<58:00,  2.33s/it]\u001b[A\n"," 87% 9716/11208 [1:06:51<57:58,  2.33s/it]\u001b[A\n"," 87% 9717/11208 [1:06:54<57:56,  2.33s/it]\u001b[A\n"," 87% 9718/11208 [1:06:56<57:51,  2.33s/it]\u001b[A\n"," 87% 9719/11208 [1:06:58<57:55,  2.33s/it]\u001b[A\n"," 87% 9720/11208 [1:07:01<57:53,  2.33s/it]\u001b[A\n"," 87% 9721/11208 [1:07:03<57:52,  2.34s/it]\u001b[A\n"," 87% 9722/11208 [1:07:05<57:47,  2.33s/it]\u001b[A\n"," 87% 9723/11208 [1:07:08<57:22,  2.32s/it]\u001b[A\n"," 87% 9724/11208 [1:07:10<57:21,  2.32s/it]\u001b[A\n"," 87% 9725/11208 [1:07:12<57:27,  2.32s/it]\u001b[A\n"," 87% 9726/11208 [1:07:15<57:20,  2.32s/it]\u001b[A\n"," 87% 9727/11208 [1:07:17<56:51,  2.30s/it]\u001b[A\n"," 87% 9728/11208 [1:07:19<57:01,  2.31s/it]\u001b[A\n"," 87% 9729/11208 [1:07:22<57:12,  2.32s/it]\u001b[A\n"," 87% 9730/11208 [1:07:24<57:14,  2.32s/it]\u001b[A\n"," 87% 9731/11208 [1:07:26<57:18,  2.33s/it]\u001b[A\n"," 87% 9732/11208 [1:07:29<57:12,  2.33s/it]\u001b[A\n"," 87% 9733/11208 [1:07:31<57:15,  2.33s/it]\u001b[A\n"," 87% 9734/11208 [1:07:33<57:11,  2.33s/it]\u001b[A\n"," 87% 9735/11208 [1:07:36<57:11,  2.33s/it]\u001b[A\n"," 87% 9736/11208 [1:07:38<57:11,  2.33s/it]\u001b[A\n"," 87% 9737/11208 [1:07:40<56:38,  2.31s/it]\u001b[A\n"," 87% 9738/11208 [1:07:42<56:42,  2.31s/it]\u001b[A\n"," 87% 9739/11208 [1:07:45<56:46,  2.32s/it]\u001b[A\n"," 87% 9740/11208 [1:07:47<57:01,  2.33s/it]\u001b[A\n"," 87% 9741/11208 [1:07:49<57:09,  2.34s/it]\u001b[A\n"," 87% 9742/11208 [1:07:52<57:01,  2.33s/it]\u001b[A\n"," 87% 9743/11208 [1:07:54<56:57,  2.33s/it]\u001b[A\n"," 87% 9744/11208 [1:07:56<56:06,  2.30s/it]\u001b[A\n"," 87% 9745/11208 [1:07:59<56:09,  2.30s/it]\u001b[A\n"," 87% 9746/11208 [1:08:01<56:25,  2.32s/it]\u001b[A\n"," 87% 9747/11208 [1:08:03<56:28,  2.32s/it]\u001b[A\n"," 87% 9748/11208 [1:08:06<56:32,  2.32s/it]\u001b[A\n"," 87% 9749/11208 [1:08:08<56:34,  2.33s/it]\u001b[A\n"," 87% 9750/11208 [1:08:10<56:36,  2.33s/it]\u001b[A\n"," 87% 9751/11208 [1:08:13<56:33,  2.33s/it]\u001b[A\n"," 87% 9752/11208 [1:08:15<56:22,  2.32s/it]\u001b[A\n"," 87% 9753/11208 [1:08:17<56:26,  2.33s/it]\u001b[A\n"," 87% 9754/11208 [1:08:20<56:29,  2.33s/it]\u001b[A\n"," 87% 9755/11208 [1:08:22<56:31,  2.33s/it]\u001b[A\n"," 87% 9756/11208 [1:08:24<56:29,  2.33s/it]\u001b[A\n"," 87% 9757/11208 [1:08:27<56:27,  2.33s/it]\u001b[A\n"," 87% 9758/11208 [1:08:29<56:27,  2.34s/it]\u001b[A\n"," 87% 9759/11208 [1:08:31<56:27,  2.34s/it]\u001b[A\n"," 87% 9760/11208 [1:08:34<56:24,  2.34s/it]\u001b[A\n"," 87% 9761/11208 [1:08:36<56:24,  2.34s/it]\u001b[A\n"," 87% 9762/11208 [1:08:38<56:20,  2.34s/it]\u001b[A\n"," 87% 9763/11208 [1:08:41<56:19,  2.34s/it]\u001b[A\n"," 87% 9764/11208 [1:08:43<55:41,  2.31s/it]\u001b[A\n"," 87% 9765/11208 [1:08:45<55:52,  2.32s/it]\u001b[A\n"," 87% 9766/11208 [1:08:48<55:53,  2.33s/it]\u001b[A\n"," 87% 9767/11208 [1:08:50<55:52,  2.33s/it]\u001b[A\n"," 87% 9768/11208 [1:08:52<55:39,  2.32s/it]\u001b[A\n"," 87% 9769/11208 [1:08:55<55:39,  2.32s/it]\u001b[A\n"," 87% 9770/11208 [1:08:57<55:41,  2.32s/it]\u001b[A\n"," 87% 9771/11208 [1:08:59<55:42,  2.33s/it]\u001b[A\n"," 87% 9772/11208 [1:09:02<55:44,  2.33s/it]\u001b[A\n"," 87% 9773/11208 [1:09:04<55:44,  2.33s/it]\u001b[A\n"," 87% 9774/11208 [1:09:06<55:40,  2.33s/it]\u001b[A\n"," 87% 9775/11208 [1:09:09<55:37,  2.33s/it]\u001b[A\n"," 87% 9776/11208 [1:09:11<55:36,  2.33s/it]\u001b[A\n"," 87% 9777/11208 [1:09:13<55:27,  2.33s/it]\u001b[A\n"," 87% 9778/11208 [1:09:16<55:26,  2.33s/it]\u001b[A\n"," 87% 9779/11208 [1:09:18<55:25,  2.33s/it]\u001b[A\n"," 87% 9780/11208 [1:09:20<55:25,  2.33s/it]\u001b[A\n"," 87% 9781/11208 [1:09:23<55:14,  2.32s/it]\u001b[A\n"," 87% 9782/11208 [1:09:25<55:14,  2.32s/it]\u001b[A\n"," 87% 9783/11208 [1:09:27<55:07,  2.32s/it]\u001b[A\n"," 87% 9784/11208 [1:09:29<55:02,  2.32s/it]\u001b[A\n"," 87% 9785/11208 [1:09:32<55:01,  2.32s/it]\u001b[A\n"," 87% 9786/11208 [1:09:34<55:05,  2.32s/it]\u001b[A\n"," 87% 9787/11208 [1:09:36<55:04,  2.33s/it]\u001b[A\n"," 87% 9788/11208 [1:09:39<55:07,  2.33s/it]\u001b[A\n"," 87% 9789/11208 [1:09:41<54:59,  2.33s/it]\u001b[A\n"," 87% 9790/11208 [1:09:43<54:58,  2.33s/it]\u001b[A\n"," 87% 9791/11208 [1:09:46<54:15,  2.30s/it]\u001b[A\n"," 87% 9792/11208 [1:09:48<53:45,  2.28s/it]\u001b[A\n"," 87% 9793/11208 [1:09:50<53:55,  2.29s/it]\u001b[A\n"," 87% 9794/11208 [1:09:53<54:15,  2.30s/it]\u001b[A\n"," 87% 9795/11208 [1:09:55<54:20,  2.31s/it]\u001b[A\n"," 87% 9796/11208 [1:09:57<53:25,  2.27s/it]\u001b[A\n"," 87% 9797/11208 [1:09:59<52:46,  2.24s/it]\u001b[A\n"," 87% 9798/11208 [1:10:02<53:19,  2.27s/it]\u001b[A\n"," 87% 9799/11208 [1:10:04<53:39,  2.28s/it]\u001b[A\n"," 87% 9800/11208 [1:10:06<53:54,  2.30s/it]\u001b[A\n","\u001b[A{'loss': 1.6781, 'learning_rate': 6.281227694503926e-06, 'epoch': 2.62}\n","\n"," 87% 9800/11208 [1:10:06<53:54,  2.30s/it]\u001b[A\n"," 87% 9801/11208 [1:10:09<54:09,  2.31s/it]\u001b[A\n"," 87% 9802/11208 [1:10:11<54:15,  2.32s/it]\u001b[A\n"," 87% 9803/11208 [1:10:13<54:13,  2.32s/it]\u001b[A\n"," 87% 9804/11208 [1:10:16<54:16,  2.32s/it]\u001b[A\n"," 87% 9805/11208 [1:10:18<54:18,  2.32s/it]\u001b[A\n"," 87% 9806/11208 [1:10:20<54:15,  2.32s/it]\u001b[A\n"," 88% 9807/11208 [1:10:23<54:17,  2.33s/it]\u001b[A\n"," 88% 9808/11208 [1:10:25<54:17,  2.33s/it]\u001b[A\n"," 88% 9809/11208 [1:10:27<54:11,  2.32s/it]\u001b[A\n"," 88% 9810/11208 [1:10:29<54:05,  2.32s/it]\u001b[A\n"," 88% 9811/11208 [1:10:32<54:09,  2.33s/it]\u001b[A\n"," 88% 9812/11208 [1:10:34<53:06,  2.28s/it]\u001b[A\n"," 88% 9813/11208 [1:10:36<53:25,  2.30s/it]\u001b[A\n"," 88% 9814/11208 [1:10:39<53:33,  2.31s/it]\u001b[A\n"," 88% 9815/11208 [1:10:41<53:39,  2.31s/it]\u001b[A\n"," 88% 9816/11208 [1:10:43<53:44,  2.32s/it]\u001b[A\n"," 88% 9817/11208 [1:10:46<53:39,  2.31s/it]\u001b[A\n"," 88% 9818/11208 [1:10:48<53:44,  2.32s/it]\u001b[A\n"," 88% 9819/11208 [1:10:50<53:48,  2.32s/it]\u001b[A\n"," 88% 9820/11208 [1:10:53<53:49,  2.33s/it]\u001b[A\n"," 88% 9821/11208 [1:10:55<53:52,  2.33s/it]\u001b[A\n"," 88% 9822/11208 [1:10:57<53:48,  2.33s/it]\u001b[A\n"," 88% 9823/11208 [1:11:00<53:44,  2.33s/it]\u001b[A\n"," 88% 9824/11208 [1:11:02<53:39,  2.33s/it]\u001b[A\n"," 88% 9825/11208 [1:11:04<53:31,  2.32s/it]\u001b[A\n"," 88% 9826/11208 [1:11:07<53:27,  2.32s/it]\u001b[A\n"," 88% 9827/11208 [1:11:09<53:31,  2.33s/it]\u001b[A\n"," 88% 9828/11208 [1:11:11<53:33,  2.33s/it]\u001b[A\n"," 88% 9829/11208 [1:11:14<53:13,  2.32s/it]\u001b[A\n"," 88% 9830/11208 [1:11:16<53:13,  2.32s/it]\u001b[A\n"," 88% 9831/11208 [1:11:18<53:08,  2.32s/it]\u001b[A\n"," 88% 9832/11208 [1:11:20<53:16,  2.32s/it]\u001b[A\n"," 88% 9833/11208 [1:11:23<53:19,  2.33s/it]\u001b[A\n"," 88% 9834/11208 [1:11:25<53:23,  2.33s/it]\u001b[A\n"," 88% 9835/11208 [1:11:27<53:21,  2.33s/it]\u001b[A\n"," 88% 9836/11208 [1:11:30<53:22,  2.33s/it]\u001b[A\n"," 88% 9837/11208 [1:11:32<53:13,  2.33s/it]\u001b[A\n"," 88% 9838/11208 [1:11:34<53:07,  2.33s/it]\u001b[A\n"," 88% 9839/11208 [1:11:37<53:09,  2.33s/it]\u001b[A\n"," 88% 9840/11208 [1:11:39<53:10,  2.33s/it]\u001b[A\n"," 88% 9841/11208 [1:11:41<53:02,  2.33s/it]\u001b[A\n"," 88% 9842/11208 [1:11:44<52:55,  2.32s/it]\u001b[A\n"," 88% 9843/11208 [1:11:46<52:56,  2.33s/it]\u001b[A\n"," 88% 9844/11208 [1:11:48<52:52,  2.33s/it]\u001b[A\n"," 88% 9845/11208 [1:11:51<52:52,  2.33s/it]\u001b[A\n"," 88% 9846/11208 [1:11:53<52:53,  2.33s/it]\u001b[A\n"," 88% 9847/11208 [1:11:55<52:48,  2.33s/it]\u001b[A\n"," 88% 9848/11208 [1:11:58<52:53,  2.33s/it]\u001b[A\n"," 88% 9849/11208 [1:12:00<52:48,  2.33s/it]\u001b[A\n"," 88% 9850/11208 [1:12:02<52:46,  2.33s/it]\u001b[A\n"," 88% 9851/11208 [1:12:05<52:45,  2.33s/it]\u001b[A\n"," 88% 9852/11208 [1:12:07<52:46,  2.34s/it]\u001b[A\n"," 88% 9853/11208 [1:12:09<52:46,  2.34s/it]\u001b[A\n"," 88% 9854/11208 [1:12:12<52:40,  2.33s/it]\u001b[A\n"," 88% 9855/11208 [1:12:14<52:35,  2.33s/it]\u001b[A\n"," 88% 9856/11208 [1:12:16<52:34,  2.33s/it]\u001b[A\n"," 88% 9857/11208 [1:12:19<52:35,  2.34s/it]\u001b[A\n"," 88% 9858/11208 [1:12:21<52:33,  2.34s/it]\u001b[A\n"," 88% 9859/11208 [1:12:23<52:35,  2.34s/it]\u001b[A\n"," 88% 9860/11208 [1:12:26<52:35,  2.34s/it]\u001b[A\n"," 88% 9861/11208 [1:12:28<52:27,  2.34s/it]\u001b[A\n"," 88% 9862/11208 [1:12:30<51:08,  2.28s/it]\u001b[A\n"," 88% 9863/11208 [1:12:33<51:20,  2.29s/it]\u001b[A\n"," 88% 9864/11208 [1:12:35<51:28,  2.30s/it]\u001b[A\n"," 88% 9865/11208 [1:12:37<51:34,  2.30s/it]\u001b[A\n"," 88% 9866/11208 [1:12:40<51:41,  2.31s/it]\u001b[A\n"," 88% 9867/11208 [1:12:42<51:38,  2.31s/it]\u001b[A\n"," 88% 9868/11208 [1:12:44<50:42,  2.27s/it]\u001b[A\n"," 88% 9869/11208 [1:12:46<51:03,  2.29s/it]\u001b[A\n"," 88% 9870/11208 [1:12:49<51:12,  2.30s/it]\u001b[A\n"," 88% 9871/11208 [1:12:51<51:22,  2.31s/it]\u001b[A\n"," 88% 9872/11208 [1:12:53<51:27,  2.31s/it]\u001b[A\n"," 88% 9873/11208 [1:12:56<51:30,  2.31s/it]\u001b[A\n"," 88% 9874/11208 [1:12:58<51:36,  2.32s/it]\u001b[A\n"," 88% 9875/11208 [1:13:00<51:40,  2.33s/it]\u001b[A\n"," 88% 9876/11208 [1:13:03<51:10,  2.31s/it]\u001b[A\n"," 88% 9877/11208 [1:13:05<51:19,  2.31s/it]\u001b[A\n"," 88% 9878/11208 [1:13:07<51:17,  2.31s/it]\u001b[A\n"," 88% 9879/11208 [1:13:10<51:19,  2.32s/it]\u001b[A\n"," 88% 9880/11208 [1:13:12<51:15,  2.32s/it]\u001b[A\n"," 88% 9881/11208 [1:13:14<50:42,  2.29s/it]\u001b[A\n"," 88% 9882/11208 [1:13:16<50:53,  2.30s/it]\u001b[A\n"," 88% 9883/11208 [1:13:19<51:00,  2.31s/it]\u001b[A\n"," 88% 9884/11208 [1:13:21<51:12,  2.32s/it]\u001b[A\n"," 88% 9885/11208 [1:13:23<51:16,  2.33s/it]\u001b[A\n"," 88% 9886/11208 [1:13:26<51:13,  2.33s/it]\u001b[A\n"," 88% 9887/11208 [1:13:28<51:11,  2.32s/it]\u001b[A\n"," 88% 9888/11208 [1:13:30<49:22,  2.24s/it]\u001b[A\n"," 88% 9889/11208 [1:13:32<49:52,  2.27s/it]\u001b[A\n"," 88% 9890/11208 [1:13:35<50:17,  2.29s/it]\u001b[A\n"," 88% 9891/11208 [1:13:37<50:28,  2.30s/it]\u001b[A\n"," 88% 9892/11208 [1:13:39<50:37,  2.31s/it]\u001b[A\n"," 88% 9893/11208 [1:13:42<50:41,  2.31s/it]\u001b[A\n"," 88% 9894/11208 [1:13:44<50:47,  2.32s/it]\u001b[A\n"," 88% 9895/11208 [1:13:46<50:46,  2.32s/it]\u001b[A\n"," 88% 9896/11208 [1:13:49<49:12,  2.25s/it]\u001b[A\n"," 88% 9897/11208 [1:13:51<49:43,  2.28s/it]\u001b[A\n"," 88% 9898/11208 [1:13:53<50:00,  2.29s/it]\u001b[A\n"," 88% 9899/11208 [1:13:56<50:19,  2.31s/it]\u001b[A\n"," 88% 9900/11208 [1:13:58<49:51,  2.29s/it]\u001b[A\n","\u001b[A{'loss': 1.6933, 'learning_rate': 5.835117773019273e-06, 'epoch': 2.65}\n","\n"," 88% 9900/11208 [1:13:58<49:51,  2.29s/it]\u001b[A\n"," 88% 9901/11208 [1:14:00<50:06,  2.30s/it]\u001b[A\n"," 88% 9902/11208 [1:14:02<50:03,  2.30s/it]\u001b[A\n"," 88% 9903/11208 [1:14:05<50:06,  2.30s/it]\u001b[A\n"," 88% 9904/11208 [1:14:07<50:13,  2.31s/it]\u001b[A\n"," 88% 9905/11208 [1:14:09<50:16,  2.32s/it]\u001b[A\n"," 88% 9906/11208 [1:14:12<50:20,  2.32s/it]\u001b[A\n"," 88% 9907/11208 [1:14:14<49:55,  2.30s/it]\u001b[A\n"," 88% 9908/11208 [1:14:16<50:06,  2.31s/it]\u001b[A\n"," 88% 9909/11208 [1:14:19<50:04,  2.31s/it]\u001b[A\n"," 88% 9910/11208 [1:14:21<50:05,  2.32s/it]\u001b[A\n"," 88% 9911/11208 [1:14:23<49:57,  2.31s/it]\u001b[A\n"," 88% 9912/11208 [1:14:26<49:58,  2.31s/it]\u001b[A\n"," 88% 9913/11208 [1:14:28<49:42,  2.30s/it]\u001b[A\n"," 88% 9914/11208 [1:14:30<49:49,  2.31s/it]\u001b[A\n"," 88% 9915/11208 [1:14:32<49:46,  2.31s/it]\u001b[A\n"," 88% 9916/11208 [1:14:35<49:53,  2.32s/it]\u001b[A\n"," 88% 9917/11208 [1:14:37<49:52,  2.32s/it]\u001b[A\n"," 88% 9918/11208 [1:14:39<49:56,  2.32s/it]\u001b[A\n"," 88% 9919/11208 [1:14:42<49:51,  2.32s/it]\u001b[A\n"," 89% 9920/11208 [1:14:44<49:54,  2.32s/it]\u001b[A\n"," 89% 9921/11208 [1:14:46<49:57,  2.33s/it]\u001b[A\n"," 89% 9922/11208 [1:14:49<49:48,  2.32s/it]\u001b[A\n"," 89% 9923/11208 [1:14:51<49:50,  2.33s/it]\u001b[A\n"," 89% 9924/11208 [1:14:53<49:48,  2.33s/it]\u001b[A\n"," 89% 9925/11208 [1:14:56<49:42,  2.32s/it]\u001b[A\n"," 89% 9926/11208 [1:14:58<49:45,  2.33s/it]\u001b[A\n"," 89% 9927/11208 [1:15:00<49:48,  2.33s/it]\u001b[A\n"," 89% 9928/11208 [1:15:03<49:44,  2.33s/it]\u001b[A\n"," 89% 9929/11208 [1:15:05<49:41,  2.33s/it]\u001b[A\n"," 89% 9930/11208 [1:15:07<49:37,  2.33s/it]\u001b[A\n"," 89% 9931/11208 [1:15:10<49:04,  2.31s/it]\u001b[A\n"," 89% 9932/11208 [1:15:12<49:15,  2.32s/it]\u001b[A\n"," 89% 9933/11208 [1:15:14<49:21,  2.32s/it]\u001b[A\n"," 89% 9934/11208 [1:15:17<49:22,  2.33s/it]\u001b[A\n"," 89% 9935/11208 [1:15:19<49:24,  2.33s/it]\u001b[A\n"," 89% 9936/11208 [1:15:21<49:21,  2.33s/it]\u001b[A\n"," 89% 9937/11208 [1:15:24<49:16,  2.33s/it]\u001b[A\n"," 89% 9938/11208 [1:15:26<48:54,  2.31s/it]\u001b[A\n"," 89% 9939/11208 [1:15:28<49:02,  2.32s/it]\u001b[A\n"," 89% 9940/11208 [1:15:31<49:03,  2.32s/it]\u001b[A\n"," 89% 9941/11208 [1:15:33<49:00,  2.32s/it]\u001b[A\n"," 89% 9942/11208 [1:15:35<49:01,  2.32s/it]\u001b[A\n"," 89% 9943/11208 [1:15:38<49:04,  2.33s/it]\u001b[A\n"," 89% 9944/11208 [1:15:40<49:01,  2.33s/it]\u001b[A\n"," 89% 9945/11208 [1:15:42<48:08,  2.29s/it]\u001b[A\n"," 89% 9946/11208 [1:15:44<48:25,  2.30s/it]\u001b[A\n"," 89% 9947/11208 [1:15:47<48:27,  2.31s/it]\u001b[A\n"," 89% 9948/11208 [1:15:49<48:33,  2.31s/it]\u001b[A\n"," 89% 9949/11208 [1:15:51<48:33,  2.31s/it]\u001b[A\n"," 89% 9950/11208 [1:15:54<48:39,  2.32s/it]\u001b[A\n"," 89% 9951/11208 [1:15:56<48:42,  2.32s/it]\u001b[A\n"," 89% 9952/11208 [1:15:58<48:36,  2.32s/it]\u001b[A\n"," 89% 9953/11208 [1:16:01<47:59,  2.29s/it]\u001b[A\n"," 89% 9954/11208 [1:16:03<48:11,  2.31s/it]\u001b[A\n"," 89% 9955/11208 [1:16:05<48:24,  2.32s/it]\u001b[A\n"," 89% 9956/11208 [1:16:08<48:30,  2.32s/it]\u001b[A\n"," 89% 9957/11208 [1:16:10<48:29,  2.33s/it]\u001b[A\n"," 89% 9958/11208 [1:16:12<48:28,  2.33s/it]\u001b[A\n"," 89% 9959/11208 [1:16:15<48:29,  2.33s/it]\u001b[A\n"," 89% 9960/11208 [1:16:17<48:24,  2.33s/it]\u001b[A\n"," 89% 9961/11208 [1:16:19<48:19,  2.33s/it]\u001b[A\n"," 89% 9962/11208 [1:16:22<48:21,  2.33s/it]\u001b[A\n"," 89% 9963/11208 [1:16:24<48:12,  2.32s/it]\u001b[A\n"," 89% 9964/11208 [1:16:26<48:01,  2.32s/it]\u001b[A\n"," 89% 9965/11208 [1:16:29<48:05,  2.32s/it]\u001b[A\n"," 89% 9966/11208 [1:16:31<48:06,  2.32s/it]\u001b[A\n"," 89% 9967/11208 [1:16:33<48:03,  2.32s/it]\u001b[A\n"," 89% 9968/11208 [1:16:36<48:00,  2.32s/it]\u001b[A\n"," 89% 9969/11208 [1:16:38<48:00,  2.32s/it]\u001b[A\n"," 89% 9970/11208 [1:16:40<48:00,  2.33s/it]\u001b[A\n"," 89% 9971/11208 [1:16:43<47:58,  2.33s/it]\u001b[A\n"," 89% 9972/11208 [1:16:45<47:13,  2.29s/it]\u001b[A\n"," 89% 9973/11208 [1:16:47<47:22,  2.30s/it]\u001b[A\n"," 89% 9974/11208 [1:16:49<47:31,  2.31s/it]\u001b[A\n"," 89% 9975/11208 [1:16:52<46:56,  2.28s/it]\u001b[A\n"," 89% 9976/11208 [1:16:54<47:09,  2.30s/it]\u001b[A\n"," 89% 9977/11208 [1:16:56<47:14,  2.30s/it]\u001b[A\n"," 89% 9978/11208 [1:16:59<47:22,  2.31s/it]\u001b[A\n"," 89% 9979/11208 [1:17:01<47:08,  2.30s/it]\u001b[A\n"," 89% 9980/11208 [1:17:03<46:44,  2.28s/it]\u001b[A\n"," 89% 9981/11208 [1:17:05<46:09,  2.26s/it]\u001b[A\n"," 89% 9982/11208 [1:17:08<46:35,  2.28s/it]\u001b[A\n"," 89% 9983/11208 [1:17:10<46:47,  2.29s/it]\u001b[A\n"," 89% 9984/11208 [1:17:12<46:59,  2.30s/it]\u001b[A\n"," 89% 9985/11208 [1:17:15<46:58,  2.30s/it]\u001b[A\n"," 89% 9986/11208 [1:17:17<47:06,  2.31s/it]\u001b[A\n"," 89% 9987/11208 [1:17:19<47:02,  2.31s/it]\u001b[A\n"," 89% 9988/11208 [1:17:22<47:07,  2.32s/it]\u001b[A\n"," 89% 9989/11208 [1:17:24<47:03,  2.32s/it]\u001b[A\n"," 89% 9990/11208 [1:17:26<47:04,  2.32s/it]\u001b[A\n"," 89% 9991/11208 [1:17:29<47:07,  2.32s/it]\u001b[A\n"," 89% 9992/11208 [1:17:31<47:01,  2.32s/it]\u001b[A\n"," 89% 9993/11208 [1:17:33<47:01,  2.32s/it]\u001b[A\n"," 89% 9994/11208 [1:17:36<47:01,  2.32s/it]\u001b[A\n"," 89% 9995/11208 [1:17:38<47:03,  2.33s/it]\u001b[A\n"," 89% 9996/11208 [1:17:40<46:33,  2.30s/it]\u001b[A\n"," 89% 9997/11208 [1:17:42<46:40,  2.31s/it]\u001b[A\n"," 89% 9998/11208 [1:17:45<46:46,  2.32s/it]\u001b[A\n"," 89% 9999/11208 [1:17:47<46:51,  2.33s/it]\u001b[A\n"," 89% 10000/11208 [1:17:49<46:51,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6693, 'learning_rate': 5.389007851534619e-06, 'epoch': 2.68}\n","\n"," 89% 10000/11208 [1:17:50<46:51,  2.33s/it]\u001b[A[INFO|trainer.py:2700] 2022-12-14 21:55:13,019 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-10000\n","[INFO|configuration_utils.py:447] 2022-12-14 21:55:13,028 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-10000/config.json\n","[INFO|modeling_utils.py:1680] 2022-12-14 21:55:23,362 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-10000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-14 21:55:23,391 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-10000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-14 21:55:23,400 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-10000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-14 21:55:23,780 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-10000/spiece.model\n","\n"," 89% 10001/11208 [1:18:23<3:54:54, 11.68s/it]\u001b[A\n"," 89% 10002/11208 [1:18:25<2:58:19,  8.87s/it]\u001b[A\n"," 89% 10003/11208 [1:18:28<2:18:52,  6.91s/it]\u001b[A\n"," 89% 10004/11208 [1:18:30<1:51:18,  5.55s/it]\u001b[A\n"," 89% 10005/11208 [1:18:32<1:32:11,  4.60s/it]\u001b[A\n"," 89% 10006/11208 [1:18:35<1:18:57,  3.94s/it]\u001b[A\n"," 89% 10007/11208 [1:18:37<1:09:40,  3.48s/it]\u001b[A\n"," 89% 10008/11208 [1:18:40<1:03:09,  3.16s/it]\u001b[A\n"," 89% 10009/11208 [1:18:42<58:52,  2.95s/it]  \u001b[A\n"," 89% 10010/11208 [1:18:44<55:59,  2.80s/it]\u001b[A\n"," 89% 10011/11208 [1:18:47<54:45,  2.74s/it]\u001b[A\n"," 89% 10012/11208 [1:18:50<53:33,  2.69s/it]\u001b[A\n"," 89% 10013/11208 [1:18:52<52:39,  2.64s/it]\u001b[A\n"," 89% 10014/11208 [1:18:55<51:47,  2.60s/it]\u001b[A\n"," 89% 10015/11208 [1:18:57<50:39,  2.55s/it]\u001b[A\n"," 89% 10016/11208 [1:19:00<49:52,  2.51s/it]\u001b[A\n"," 89% 10017/11208 [1:19:02<49:06,  2.47s/it]\u001b[A\n"," 89% 10018/11208 [1:19:04<48:39,  2.45s/it]\u001b[A\n"," 89% 10019/11208 [1:19:07<48:10,  2.43s/it]\u001b[A\n"," 89% 10020/11208 [1:19:09<47:41,  2.41s/it]\u001b[A\n"," 89% 10021/11208 [1:19:11<47:04,  2.38s/it]\u001b[A\n"," 89% 10022/11208 [1:19:14<46:54,  2.37s/it]\u001b[A\n"," 89% 10023/11208 [1:19:16<46:32,  2.36s/it]\u001b[A\n"," 89% 10024/11208 [1:19:18<46:22,  2.35s/it]\u001b[A\n"," 89% 10025/11208 [1:19:21<46:08,  2.34s/it]\u001b[A\n"," 89% 10026/11208 [1:19:23<44:50,  2.28s/it]\u001b[A\n"," 89% 10027/11208 [1:19:25<45:00,  2.29s/it]\u001b[A\n"," 89% 10028/11208 [1:19:27<44:59,  2.29s/it]\u001b[A\n"," 89% 10029/11208 [1:19:30<45:10,  2.30s/it]\u001b[A\n"," 89% 10030/11208 [1:19:32<45:07,  2.30s/it]\u001b[A\n"," 89% 10031/11208 [1:19:34<45:05,  2.30s/it]\u001b[A\n"," 90% 10032/11208 [1:19:37<44:56,  2.29s/it]\u001b[A\n"," 90% 10033/11208 [1:19:39<44:58,  2.30s/it]\u001b[A\n"," 90% 10034/11208 [1:19:41<45:04,  2.30s/it]\u001b[A\n"," 90% 10035/11208 [1:19:44<45:03,  2.30s/it]\u001b[A\n"," 90% 10036/11208 [1:19:46<45:14,  2.32s/it]\u001b[A\n"," 90% 10037/11208 [1:19:48<45:14,  2.32s/it]\u001b[A\n"," 90% 10038/11208 [1:19:51<45:15,  2.32s/it]\u001b[A\n"," 90% 10039/11208 [1:19:53<45:14,  2.32s/it]\u001b[A\n"," 90% 10040/11208 [1:19:55<45:15,  2.32s/it]\u001b[A\n"," 90% 10041/11208 [1:19:58<45:15,  2.33s/it]\u001b[A\n"," 90% 10042/11208 [1:20:00<45:24,  2.34s/it]\u001b[A\n"," 90% 10043/11208 [1:20:02<45:21,  2.34s/it]\u001b[A\n"," 90% 10044/11208 [1:20:05<45:25,  2.34s/it]\u001b[A\n"," 90% 10045/11208 [1:20:07<45:10,  2.33s/it]\u001b[A\n"," 90% 10046/11208 [1:20:09<45:15,  2.34s/it]\u001b[A\n"," 90% 10047/11208 [1:20:12<44:58,  2.32s/it]\u001b[A\n"," 90% 10048/11208 [1:20:14<45:10,  2.34s/it]\u001b[A\n"," 90% 10049/11208 [1:20:16<45:17,  2.34s/it]\u001b[A\n"," 90% 10050/11208 [1:20:19<45:24,  2.35s/it]\u001b[A\n"," 90% 10051/11208 [1:20:21<45:24,  2.35s/it]\u001b[A\n"," 90% 10052/11208 [1:20:23<44:47,  2.32s/it]\u001b[A\n"," 90% 10053/11208 [1:20:26<44:35,  2.32s/it]\u001b[A\n"," 90% 10054/11208 [1:20:28<44:51,  2.33s/it]\u001b[A\n"," 90% 10055/11208 [1:20:30<44:59,  2.34s/it]\u001b[A\n"," 90% 10056/11208 [1:20:33<45:00,  2.34s/it]\u001b[A\n"," 90% 10057/11208 [1:20:35<45:05,  2.35s/it]\u001b[A\n"," 90% 10058/11208 [1:20:37<44:56,  2.35s/it]\u001b[A\n"," 90% 10059/11208 [1:20:40<44:49,  2.34s/it]\u001b[A\n"," 90% 10060/11208 [1:20:42<43:49,  2.29s/it]\u001b[A\n"," 90% 10061/11208 [1:20:44<43:41,  2.29s/it]\u001b[A\n"," 90% 10062/11208 [1:20:46<43:57,  2.30s/it]\u001b[A\n"," 90% 10063/11208 [1:20:49<44:05,  2.31s/it]\u001b[A\n"," 90% 10064/11208 [1:20:51<43:27,  2.28s/it]\u001b[A\n"," 90% 10065/11208 [1:20:53<43:45,  2.30s/it]\u001b[A\n"," 90% 10066/11208 [1:20:56<43:57,  2.31s/it]\u001b[A\n"," 90% 10067/11208 [1:20:58<43:58,  2.31s/it]\u001b[A\n"," 90% 10068/11208 [1:21:00<43:58,  2.31s/it]\u001b[A\n"," 90% 10069/11208 [1:21:03<44:02,  2.32s/it]\u001b[A\n"," 90% 10070/11208 [1:21:05<44:01,  2.32s/it]\u001b[A\n"," 90% 10071/11208 [1:21:07<44:00,  2.32s/it]\u001b[A\n"," 90% 10072/11208 [1:21:10<43:59,  2.32s/it]\u001b[A\n"," 90% 10073/11208 [1:21:12<43:59,  2.33s/it]\u001b[A\n"," 90% 10074/11208 [1:21:14<43:53,  2.32s/it]\u001b[A\n"," 90% 10075/11208 [1:21:17<43:55,  2.33s/it]\u001b[A\n"," 90% 10076/11208 [1:21:19<43:51,  2.32s/it]\u001b[A\n"," 90% 10077/11208 [1:21:21<43:48,  2.32s/it]\u001b[A\n"," 90% 10078/11208 [1:21:24<43:40,  2.32s/it]\u001b[A\n"," 90% 10079/11208 [1:21:26<43:31,  2.31s/it]\u001b[A\n"," 90% 10080/11208 [1:21:28<43:32,  2.32s/it]\u001b[A\n"," 90% 10081/11208 [1:21:31<43:41,  2.33s/it]\u001b[A\n"," 90% 10082/11208 [1:21:33<43:43,  2.33s/it]\u001b[A\n"," 90% 10083/11208 [1:21:35<43:37,  2.33s/it]\u001b[A\n"," 90% 10084/11208 [1:21:38<43:37,  2.33s/it]\u001b[A\n"," 90% 10085/11208 [1:21:40<43:36,  2.33s/it]\u001b[A\n"," 90% 10086/11208 [1:21:42<43:37,  2.33s/it]\u001b[A\n"," 90% 10087/11208 [1:21:44<43:32,  2.33s/it]\u001b[A\n"," 90% 10088/11208 [1:21:47<43:20,  2.32s/it]\u001b[A\n"," 90% 10089/11208 [1:21:49<43:21,  2.32s/it]\u001b[A\n"," 90% 10090/11208 [1:21:51<43:18,  2.32s/it]\u001b[A\n"," 90% 10091/11208 [1:21:54<43:20,  2.33s/it]\u001b[A\n"," 90% 10092/11208 [1:21:56<43:21,  2.33s/it]\u001b[A\n"," 90% 10093/11208 [1:21:58<43:16,  2.33s/it]\u001b[A\n"," 90% 10094/11208 [1:22:01<43:16,  2.33s/it]\u001b[A\n"," 90% 10095/11208 [1:22:03<43:13,  2.33s/it]\u001b[A\n"," 90% 10096/11208 [1:22:05<43:10,  2.33s/it]\u001b[A\n"," 90% 10097/11208 [1:22:08<43:08,  2.33s/it]\u001b[A\n"," 90% 10098/11208 [1:22:10<42:28,  2.30s/it]\u001b[A\n"," 90% 10099/11208 [1:22:12<42:40,  2.31s/it]\u001b[A\n"," 90% 10100/11208 [1:22:15<42:09,  2.28s/it]\u001b[A\n","\u001b[A{'loss': 1.7178, 'learning_rate': 4.942897930049964e-06, 'epoch': 2.7}\n","\n"," 90% 10100/11208 [1:22:15<42:09,  2.28s/it]\u001b[A\n"," 90% 10101/11208 [1:22:17<42:25,  2.30s/it]\u001b[A\n"," 90% 10102/11208 [1:22:19<42:35,  2.31s/it]\u001b[A\n"," 90% 10103/11208 [1:22:22<42:41,  2.32s/it]\u001b[A\n"," 90% 10104/11208 [1:22:24<42:22,  2.30s/it]\u001b[A\n"," 90% 10105/11208 [1:22:26<41:45,  2.27s/it]\u001b[A\n"," 90% 10106/11208 [1:22:28<41:58,  2.29s/it]\u001b[A\n"," 90% 10107/11208 [1:22:31<42:10,  2.30s/it]\u001b[A\n"," 90% 10108/11208 [1:22:33<41:38,  2.27s/it]\u001b[A\n"," 90% 10109/11208 [1:22:35<41:53,  2.29s/it]\u001b[A\n"," 90% 10110/11208 [1:22:38<42:01,  2.30s/it]\u001b[A\n"," 90% 10111/11208 [1:22:40<42:05,  2.30s/it]\u001b[A\n"," 90% 10112/11208 [1:22:42<42:10,  2.31s/it]\u001b[A\n"," 90% 10113/11208 [1:22:44<42:15,  2.32s/it]\u001b[A\n"," 90% 10114/11208 [1:22:47<42:16,  2.32s/it]\u001b[A\n"," 90% 10115/11208 [1:22:49<42:14,  2.32s/it]\u001b[A\n"," 90% 10116/11208 [1:22:51<42:17,  2.32s/it]\u001b[A\n"," 90% 10117/11208 [1:22:54<42:05,  2.32s/it]\u001b[A\n"," 90% 10118/11208 [1:22:56<41:09,  2.27s/it]\u001b[A\n"," 90% 10119/11208 [1:22:58<41:27,  2.28s/it]\u001b[A\n"," 90% 10120/11208 [1:23:01<41:36,  2.29s/it]\u001b[A\n"," 90% 10121/11208 [1:23:03<41:52,  2.31s/it]\u001b[A\n"," 90% 10122/11208 [1:23:05<42:02,  2.32s/it]\u001b[A\n"," 90% 10123/11208 [1:23:08<41:42,  2.31s/it]\u001b[A\n"," 90% 10124/11208 [1:23:10<41:49,  2.32s/it]\u001b[A\n"," 90% 10125/11208 [1:23:12<41:52,  2.32s/it]\u001b[A\n"," 90% 10126/11208 [1:23:15<41:59,  2.33s/it]\u001b[A\n"," 90% 10127/11208 [1:23:17<42:04,  2.34s/it]\u001b[A\n"," 90% 10128/11208 [1:23:19<41:46,  2.32s/it]\u001b[A\n"," 90% 10129/11208 [1:23:22<41:52,  2.33s/it]\u001b[A\n"," 90% 10130/11208 [1:23:24<41:55,  2.33s/it]\u001b[A\n"," 90% 10131/11208 [1:23:26<41:54,  2.34s/it]\u001b[A\n"," 90% 10132/11208 [1:23:29<41:51,  2.33s/it]\u001b[A\n"," 90% 10133/11208 [1:23:31<41:51,  2.34s/it]\u001b[A\n"," 90% 10134/11208 [1:23:33<41:49,  2.34s/it]\u001b[A\n"," 90% 10135/11208 [1:23:36<41:46,  2.34s/it]\u001b[A\n"," 90% 10136/11208 [1:23:38<41:39,  2.33s/it]\u001b[A\n"," 90% 10137/11208 [1:23:40<41:35,  2.33s/it]\u001b[A\n"," 90% 10138/11208 [1:23:43<41:31,  2.33s/it]\u001b[A\n"," 90% 10139/11208 [1:23:45<41:28,  2.33s/it]\u001b[A\n"," 90% 10140/11208 [1:23:47<41:27,  2.33s/it]\u001b[A\n"," 90% 10141/11208 [1:23:49<40:59,  2.30s/it]\u001b[A\n"," 90% 10142/11208 [1:23:52<41:04,  2.31s/it]\u001b[A\n"," 90% 10143/11208 [1:23:54<41:05,  2.31s/it]\u001b[A\n"," 91% 10144/11208 [1:23:56<41:05,  2.32s/it]\u001b[A\n"," 91% 10145/11208 [1:23:59<40:59,  2.31s/it]\u001b[A\n"," 91% 10146/11208 [1:24:01<40:58,  2.31s/it]\u001b[A\n"," 91% 10147/11208 [1:24:03<40:57,  2.32s/it]\u001b[A\n"," 91% 10148/11208 [1:24:06<40:56,  2.32s/it]\u001b[A\n"," 91% 10149/11208 [1:24:08<40:52,  2.32s/it]\u001b[A\n"," 91% 10150/11208 [1:24:10<40:49,  2.32s/it]\u001b[A\n"," 91% 10151/11208 [1:24:13<40:48,  2.32s/it]\u001b[A\n"," 91% 10152/11208 [1:24:15<40:56,  2.33s/it]\u001b[A\n"," 91% 10153/11208 [1:24:17<40:59,  2.33s/it]\u001b[A\n"," 91% 10154/11208 [1:24:20<40:52,  2.33s/it]\u001b[A\n"," 91% 10155/11208 [1:24:22<40:47,  2.32s/it]\u001b[A\n"," 91% 10156/11208 [1:24:24<40:49,  2.33s/it]\u001b[A\n"," 91% 10157/11208 [1:24:27<40:50,  2.33s/it]\u001b[A\n"," 91% 10158/11208 [1:24:29<40:51,  2.34s/it]\u001b[A\n"," 91% 10159/11208 [1:24:31<40:53,  2.34s/it]\u001b[A\n"," 91% 10160/11208 [1:24:34<40:50,  2.34s/it]\u001b[A\n"," 91% 10161/11208 [1:24:36<40:50,  2.34s/it]\u001b[A\n"," 91% 10162/11208 [1:24:38<40:44,  2.34s/it]\u001b[A\n"," 91% 10163/11208 [1:24:41<40:25,  2.32s/it]\u001b[A\n"," 91% 10164/11208 [1:24:43<40:19,  2.32s/it]\u001b[A\n"," 91% 10165/11208 [1:24:45<40:24,  2.32s/it]\u001b[A\n"," 91% 10166/11208 [1:24:48<40:24,  2.33s/it]\u001b[A\n"," 91% 10167/11208 [1:24:50<40:23,  2.33s/it]\u001b[A\n"," 91% 10168/11208 [1:24:52<40:23,  2.33s/it]\u001b[A\n"," 91% 10169/11208 [1:24:55<40:23,  2.33s/it]\u001b[A\n"," 91% 10170/11208 [1:24:57<40:19,  2.33s/it]\u001b[A\n"," 91% 10171/11208 [1:24:59<39:54,  2.31s/it]\u001b[A\n"," 91% 10172/11208 [1:25:02<40:01,  2.32s/it]\u001b[A\n"," 91% 10173/11208 [1:25:04<39:32,  2.29s/it]\u001b[A\n"," 91% 10174/11208 [1:25:06<39:37,  2.30s/it]\u001b[A\n"," 91% 10175/11208 [1:25:08<39:44,  2.31s/it]\u001b[A\n"," 91% 10176/11208 [1:25:11<39:44,  2.31s/it]\u001b[A\n"," 91% 10177/11208 [1:25:13<39:40,  2.31s/it]\u001b[A\n"," 91% 10178/11208 [1:25:15<39:37,  2.31s/it]\u001b[A\n"," 91% 10179/11208 [1:25:18<39:38,  2.31s/it]\u001b[A\n"," 91% 10180/11208 [1:25:20<39:07,  2.28s/it]\u001b[A\n"," 91% 10181/11208 [1:25:22<39:15,  2.29s/it]\u001b[A\n"," 91% 10182/11208 [1:25:25<39:20,  2.30s/it]\u001b[A\n"," 91% 10183/11208 [1:25:27<39:23,  2.31s/it]\u001b[A\n"," 91% 10184/11208 [1:25:29<39:26,  2.31s/it]\u001b[A\n"," 91% 10185/11208 [1:25:31<39:25,  2.31s/it]\u001b[A\n"," 91% 10186/11208 [1:25:34<39:21,  2.31s/it]\u001b[A\n"," 91% 10187/11208 [1:25:36<39:14,  2.31s/it]\u001b[A\n"," 91% 10188/11208 [1:25:38<39:20,  2.31s/it]\u001b[A\n"," 91% 10189/11208 [1:25:41<39:20,  2.32s/it]\u001b[A\n"," 91% 10190/11208 [1:25:43<39:18,  2.32s/it]\u001b[A\n"," 91% 10191/11208 [1:25:45<38:59,  2.30s/it]\u001b[A\n"," 91% 10192/11208 [1:25:48<39:04,  2.31s/it]\u001b[A\n"," 91% 10193/11208 [1:25:50<39:05,  2.31s/it]\u001b[A\n"," 91% 10194/11208 [1:25:52<39:06,  2.31s/it]\u001b[A\n"," 91% 10195/11208 [1:25:55<39:07,  2.32s/it]\u001b[A\n"," 91% 10196/11208 [1:25:57<39:09,  2.32s/it]\u001b[A\n"," 91% 10197/11208 [1:25:59<39:11,  2.33s/it]\u001b[A\n"," 91% 10198/11208 [1:26:02<39:13,  2.33s/it]\u001b[A\n"," 91% 10199/11208 [1:26:04<38:55,  2.31s/it]\u001b[A\n"," 91% 10200/11208 [1:26:06<38:52,  2.31s/it]\u001b[A\n","\u001b[A{'loss': 1.7202, 'learning_rate': 4.496788008565311e-06, 'epoch': 2.73}\n","\n"," 91% 10200/11208 [1:26:06<38:52,  2.31s/it]\u001b[A\n"," 91% 10201/11208 [1:26:08<38:33,  2.30s/it]\u001b[A\n"," 91% 10202/11208 [1:26:11<38:26,  2.29s/it]\u001b[A\n"," 91% 10203/11208 [1:26:13<38:36,  2.30s/it]\u001b[A\n"," 91% 10204/11208 [1:26:15<38:47,  2.32s/it]\u001b[A\n"," 91% 10205/11208 [1:26:18<38:19,  2.29s/it]\u001b[A\n"," 91% 10206/11208 [1:26:20<38:26,  2.30s/it]\u001b[A\n"," 91% 10207/11208 [1:26:22<38:32,  2.31s/it]\u001b[A\n"," 91% 10208/11208 [1:26:25<38:37,  2.32s/it]\u001b[A\n"," 91% 10209/11208 [1:26:27<38:39,  2.32s/it]\u001b[A\n"," 91% 10210/11208 [1:26:29<38:31,  2.32s/it]\u001b[A\n"," 91% 10211/11208 [1:26:32<38:33,  2.32s/it]\u001b[A\n"," 91% 10212/11208 [1:26:34<38:31,  2.32s/it]\u001b[A\n"," 91% 10213/11208 [1:26:36<38:30,  2.32s/it]\u001b[A\n"," 91% 10214/11208 [1:26:39<38:29,  2.32s/it]\u001b[A\n"," 91% 10215/11208 [1:26:41<38:29,  2.33s/it]\u001b[A\n"," 91% 10216/11208 [1:26:43<38:24,  2.32s/it]\u001b[A\n"," 91% 10217/11208 [1:26:46<38:24,  2.33s/it]\u001b[A\n"," 91% 10218/11208 [1:26:48<37:38,  2.28s/it]\u001b[A\n"," 91% 10219/11208 [1:26:50<37:48,  2.29s/it]\u001b[A\n"," 91% 10220/11208 [1:26:52<37:55,  2.30s/it]\u001b[A\n"," 91% 10221/11208 [1:26:55<37:34,  2.28s/it]\u001b[A\n"," 91% 10222/11208 [1:26:57<37:39,  2.29s/it]\u001b[A\n"," 91% 10223/11208 [1:26:59<37:21,  2.28s/it]\u001b[A\n"," 91% 10224/11208 [1:27:01<37:37,  2.29s/it]\u001b[A\n"," 91% 10225/11208 [1:27:04<37:44,  2.30s/it]\u001b[A\n"," 91% 10226/11208 [1:27:06<37:39,  2.30s/it]\u001b[A\n"," 91% 10227/11208 [1:27:08<37:40,  2.30s/it]\u001b[A\n"," 91% 10228/11208 [1:27:11<37:43,  2.31s/it]\u001b[A\n"," 91% 10229/11208 [1:27:13<37:43,  2.31s/it]\u001b[A\n"," 91% 10230/11208 [1:27:15<37:46,  2.32s/it]\u001b[A\n"," 91% 10231/11208 [1:27:18<37:45,  2.32s/it]\u001b[A\n"," 91% 10232/11208 [1:27:20<36:34,  2.25s/it]\u001b[A\n"," 91% 10233/11208 [1:27:22<36:56,  2.27s/it]\u001b[A\n"," 91% 10234/11208 [1:27:24<37:06,  2.29s/it]\u001b[A\n"," 91% 10235/11208 [1:27:27<37:18,  2.30s/it]\u001b[A\n"," 91% 10236/11208 [1:27:29<37:25,  2.31s/it]\u001b[A\n"," 91% 10237/11208 [1:27:31<37:17,  2.30s/it]\u001b[A\n"," 91% 10238/11208 [1:27:34<37:23,  2.31s/it]\u001b[A\n"," 91% 10239/11208 [1:27:36<37:12,  2.30s/it]\u001b[A\n"," 91% 10240/11208 [1:27:38<36:44,  2.28s/it]\u001b[A\n"," 91% 10241/11208 [1:27:41<36:55,  2.29s/it]\u001b[A\n"," 91% 10242/11208 [1:27:43<37:03,  2.30s/it]\u001b[A\n"," 91% 10243/11208 [1:27:45<37:11,  2.31s/it]\u001b[A\n"," 91% 10244/11208 [1:27:48<37:05,  2.31s/it]\u001b[A\n"," 91% 10245/11208 [1:27:50<37:10,  2.32s/it]\u001b[A\n"," 91% 10246/11208 [1:27:52<37:16,  2.33s/it]\u001b[A\n"," 91% 10247/11208 [1:27:55<37:11,  2.32s/it]\u001b[A\n"," 91% 10248/11208 [1:27:57<37:12,  2.33s/it]\u001b[A\n"," 91% 10249/11208 [1:27:59<37:14,  2.33s/it]\u001b[A\n"," 91% 10250/11208 [1:28:02<37:15,  2.33s/it]\u001b[A\n"," 91% 10251/11208 [1:28:04<37:14,  2.34s/it]\u001b[A\n"," 91% 10252/11208 [1:28:06<37:13,  2.34s/it]\u001b[A\n"," 91% 10253/11208 [1:28:09<37:13,  2.34s/it]\u001b[A\n"," 91% 10254/11208 [1:28:11<37:09,  2.34s/it]\u001b[A\n"," 91% 10255/11208 [1:28:13<37:06,  2.34s/it]\u001b[A\n"," 92% 10256/11208 [1:28:16<37:01,  2.33s/it]\u001b[A\n"," 92% 10257/11208 [1:28:18<36:58,  2.33s/it]\u001b[A\n"," 92% 10258/11208 [1:28:20<36:58,  2.33s/it]\u001b[A\n"," 92% 10259/11208 [1:28:23<36:54,  2.33s/it]\u001b[A\n"," 92% 10260/11208 [1:28:25<36:52,  2.33s/it]\u001b[A\n"," 92% 10261/11208 [1:28:27<36:51,  2.34s/it]\u001b[A\n"," 92% 10262/11208 [1:28:30<36:40,  2.33s/it]\u001b[A\n"," 92% 10263/11208 [1:28:32<36:39,  2.33s/it]\u001b[A\n"," 92% 10264/11208 [1:28:34<36:41,  2.33s/it]\u001b[A\n"," 92% 10265/11208 [1:28:37<36:36,  2.33s/it]\u001b[A\n"," 92% 10266/11208 [1:28:39<36:26,  2.32s/it]\u001b[A\n"," 92% 10267/11208 [1:28:41<36:28,  2.33s/it]\u001b[A\n"," 92% 10268/11208 [1:28:43<36:27,  2.33s/it]\u001b[A\n"," 92% 10269/11208 [1:28:46<36:27,  2.33s/it]\u001b[A\n"," 92% 10270/11208 [1:28:48<35:54,  2.30s/it]\u001b[A\n"," 92% 10271/11208 [1:28:50<35:58,  2.30s/it]\u001b[A\n"," 92% 10272/11208 [1:28:53<36:03,  2.31s/it]\u001b[A\n"," 92% 10273/11208 [1:28:55<36:07,  2.32s/it]\u001b[A\n"," 92% 10274/11208 [1:28:57<36:09,  2.32s/it]\u001b[A\n"," 92% 10275/11208 [1:29:00<36:02,  2.32s/it]\u001b[A\n"," 92% 10276/11208 [1:29:02<35:59,  2.32s/it]\u001b[A\n"," 92% 10277/11208 [1:29:04<35:34,  2.29s/it]\u001b[A\n"," 92% 10278/11208 [1:29:06<35:13,  2.27s/it]\u001b[A\n"," 92% 10279/11208 [1:29:09<35:29,  2.29s/it]\u001b[A\n"," 92% 10280/11208 [1:29:11<35:39,  2.31s/it]\u001b[A\n"," 92% 10281/11208 [1:29:13<35:45,  2.31s/it]\u001b[A\n"," 92% 10282/11208 [1:29:16<35:43,  2.31s/it]\u001b[A\n"," 92% 10283/11208 [1:29:18<35:40,  2.31s/it]\u001b[A\n"," 92% 10284/11208 [1:29:20<34:15,  2.22s/it]\u001b[A\n"," 92% 10285/11208 [1:29:22<34:41,  2.26s/it]\u001b[A\n"," 92% 10286/11208 [1:29:25<34:37,  2.25s/it]\u001b[A\n"," 92% 10287/11208 [1:29:27<34:48,  2.27s/it]\u001b[A\n"," 92% 10288/11208 [1:29:29<35:03,  2.29s/it]\u001b[A\n"," 92% 10289/11208 [1:29:32<35:10,  2.30s/it]\u001b[A\n"," 92% 10290/11208 [1:29:34<35:18,  2.31s/it]\u001b[A\n"," 92% 10291/11208 [1:29:36<35:19,  2.31s/it]\u001b[A\n"," 92% 10292/11208 [1:29:39<35:22,  2.32s/it]\u001b[A\n"," 92% 10293/11208 [1:29:41<35:24,  2.32s/it]\u001b[A\n"," 92% 10294/11208 [1:29:43<34:51,  2.29s/it]\u001b[A\n"," 92% 10295/11208 [1:29:45<34:58,  2.30s/it]\u001b[A\n"," 92% 10296/11208 [1:29:48<35:02,  2.31s/it]\u001b[A\n"," 92% 10297/11208 [1:29:50<35:03,  2.31s/it]\u001b[A\n"," 92% 10298/11208 [1:29:52<35:03,  2.31s/it]\u001b[A\n"," 92% 10299/11208 [1:29:55<35:03,  2.31s/it]\u001b[A\n"," 92% 10300/11208 [1:29:57<35:03,  2.32s/it]\u001b[A\n","\u001b[A{'loss': 1.7017, 'learning_rate': 4.050678087080657e-06, 'epoch': 2.76}\n","\n"," 92% 10300/11208 [1:29:57<35:03,  2.32s/it]\u001b[A\n"," 92% 10301/11208 [1:29:59<35:03,  2.32s/it]\u001b[A\n"," 92% 10302/11208 [1:30:02<35:03,  2.32s/it]\u001b[A\n"," 92% 10303/11208 [1:30:04<34:59,  2.32s/it]\u001b[A\n"," 92% 10304/11208 [1:30:06<34:55,  2.32s/it]\u001b[A\n"," 92% 10305/11208 [1:30:09<34:54,  2.32s/it]\u001b[A\n"," 92% 10306/11208 [1:30:11<34:48,  2.32s/it]\u001b[A\n"," 92% 10307/11208 [1:30:13<34:46,  2.32s/it]\u001b[A\n"," 92% 10308/11208 [1:30:16<34:43,  2.31s/it]\u001b[A\n"," 92% 10309/11208 [1:30:18<34:45,  2.32s/it]\u001b[A\n"," 92% 10310/11208 [1:30:20<34:45,  2.32s/it]\u001b[A\n"," 92% 10311/11208 [1:30:23<34:42,  2.32s/it]\u001b[A\n"," 92% 10312/11208 [1:30:25<34:40,  2.32s/it]\u001b[A\n"," 92% 10313/11208 [1:30:27<34:39,  2.32s/it]\u001b[A\n"," 92% 10314/11208 [1:30:30<34:36,  2.32s/it]\u001b[A\n"," 92% 10315/11208 [1:30:32<33:52,  2.28s/it]\u001b[A\n"," 92% 10316/11208 [1:30:34<34:00,  2.29s/it]\u001b[A\n"," 92% 10317/11208 [1:30:36<34:07,  2.30s/it]\u001b[A\n"," 92% 10318/11208 [1:30:39<34:14,  2.31s/it]\u001b[A\n"," 92% 10319/11208 [1:30:41<34:18,  2.32s/it]\u001b[A\n"," 92% 10320/11208 [1:30:43<34:16,  2.32s/it]\u001b[A\n"," 92% 10321/11208 [1:30:46<33:54,  2.29s/it]\u001b[A\n"," 92% 10322/11208 [1:30:48<33:59,  2.30s/it]\u001b[A\n"," 92% 10323/11208 [1:30:50<34:03,  2.31s/it]\u001b[A\n"," 92% 10324/11208 [1:30:53<34:08,  2.32s/it]\u001b[A\n"," 92% 10325/11208 [1:30:55<34:08,  2.32s/it]\u001b[A\n"," 92% 10326/11208 [1:30:57<34:06,  2.32s/it]\u001b[A\n"," 92% 10327/11208 [1:31:00<34:03,  2.32s/it]\u001b[A\n"," 92% 10328/11208 [1:31:02<33:44,  2.30s/it]\u001b[A\n"," 92% 10329/11208 [1:31:04<33:50,  2.31s/it]\u001b[A\n"," 92% 10330/11208 [1:31:06<33:51,  2.31s/it]\u001b[A\n"," 92% 10331/11208 [1:31:09<33:53,  2.32s/it]\u001b[A\n"," 92% 10332/11208 [1:31:11<33:54,  2.32s/it]\u001b[A\n"," 92% 10333/11208 [1:31:13<33:54,  2.32s/it]\u001b[A\n"," 92% 10334/11208 [1:31:16<33:49,  2.32s/it]\u001b[A\n"," 92% 10335/11208 [1:31:18<33:43,  2.32s/it]\u001b[A\n"," 92% 10336/11208 [1:31:20<33:26,  2.30s/it]\u001b[A\n"," 92% 10337/11208 [1:31:23<33:32,  2.31s/it]\u001b[A\n"," 92% 10338/11208 [1:31:25<33:34,  2.32s/it]\u001b[A\n"," 92% 10339/11208 [1:31:27<33:38,  2.32s/it]\u001b[A\n"," 92% 10340/11208 [1:31:30<33:35,  2.32s/it]\u001b[A\n"," 92% 10341/11208 [1:31:32<33:37,  2.33s/it]\u001b[A\n"," 92% 10342/11208 [1:31:34<33:34,  2.33s/it]\u001b[A\n"," 92% 10343/11208 [1:31:37<33:31,  2.33s/it]\u001b[A\n"," 92% 10344/11208 [1:31:39<33:30,  2.33s/it]\u001b[A\n"," 92% 10345/11208 [1:31:41<33:24,  2.32s/it]\u001b[A\n"," 92% 10346/11208 [1:31:44<33:23,  2.32s/it]\u001b[A\n"," 92% 10347/11208 [1:31:46<33:20,  2.32s/it]\u001b[A\n"," 92% 10348/11208 [1:31:48<33:17,  2.32s/it]\u001b[A\n"," 92% 10349/11208 [1:31:51<33:18,  2.33s/it]\u001b[A\n"," 92% 10350/11208 [1:31:53<33:09,  2.32s/it]\u001b[A\n"," 92% 10351/11208 [1:31:55<33:11,  2.32s/it]\u001b[A\n"," 92% 10352/11208 [1:31:57<32:27,  2.28s/it]\u001b[A\n"," 92% 10353/11208 [1:32:00<32:38,  2.29s/it]\u001b[A\n"," 92% 10354/11208 [1:32:02<32:49,  2.31s/it]\u001b[A\n"," 92% 10355/11208 [1:32:04<32:52,  2.31s/it]\u001b[A\n"," 92% 10356/11208 [1:32:07<32:54,  2.32s/it]\u001b[A\n"," 92% 10357/11208 [1:32:09<32:55,  2.32s/it]\u001b[A\n"," 92% 10358/11208 [1:32:11<32:52,  2.32s/it]\u001b[A\n"," 92% 10359/11208 [1:32:14<32:55,  2.33s/it]\u001b[A\n"," 92% 10360/11208 [1:32:16<32:55,  2.33s/it]\u001b[A\n"," 92% 10361/11208 [1:32:18<32:49,  2.33s/it]\u001b[A\n"," 92% 10362/11208 [1:32:21<32:55,  2.33s/it]\u001b[A\n"," 92% 10363/11208 [1:32:23<32:58,  2.34s/it]\u001b[A\n"," 92% 10364/11208 [1:32:25<32:57,  2.34s/it]\u001b[A\n"," 92% 10365/11208 [1:32:28<32:47,  2.33s/it]\u001b[A\n"," 92% 10366/11208 [1:32:30<32:43,  2.33s/it]\u001b[A\n"," 92% 10367/11208 [1:32:32<32:42,  2.33s/it]\u001b[A\n"," 93% 10368/11208 [1:32:35<32:39,  2.33s/it]\u001b[A\n"," 93% 10369/11208 [1:32:37<32:34,  2.33s/it]\u001b[A\n"," 93% 10370/11208 [1:32:39<32:06,  2.30s/it]\u001b[A\n"," 93% 10371/11208 [1:32:42<32:08,  2.30s/it]\u001b[A\n"," 93% 10372/11208 [1:32:44<32:14,  2.31s/it]\u001b[A\n"," 93% 10373/11208 [1:32:46<32:17,  2.32s/it]\u001b[A\n"," 93% 10374/11208 [1:32:49<32:17,  2.32s/it]\u001b[A\n"," 93% 10375/11208 [1:32:51<31:30,  2.27s/it]\u001b[A\n"," 93% 10376/11208 [1:32:53<31:30,  2.27s/it]\u001b[A\n"," 93% 10377/11208 [1:32:55<31:41,  2.29s/it]\u001b[A\n"," 93% 10378/11208 [1:32:58<31:49,  2.30s/it]\u001b[A\n"," 93% 10379/11208 [1:33:00<31:51,  2.31s/it]\u001b[A\n"," 93% 10380/11208 [1:33:02<31:53,  2.31s/it]\u001b[A\n"," 93% 10381/11208 [1:33:05<31:52,  2.31s/it]\u001b[A\n"," 93% 10382/11208 [1:33:07<31:42,  2.30s/it]\u001b[A\n"," 93% 10383/11208 [1:33:09<31:47,  2.31s/it]\u001b[A\n"," 93% 10384/11208 [1:33:12<31:33,  2.30s/it]\u001b[A\n"," 93% 10385/11208 [1:33:14<31:38,  2.31s/it]\u001b[A\n"," 93% 10386/11208 [1:33:16<31:41,  2.31s/it]\u001b[A\n"," 93% 10387/11208 [1:33:18<31:40,  2.31s/it]\u001b[A\n"," 93% 10388/11208 [1:33:21<31:38,  2.32s/it]\u001b[A\n"," 93% 10389/11208 [1:33:23<31:38,  2.32s/it]\u001b[A\n"," 93% 10390/11208 [1:33:25<31:38,  2.32s/it]\u001b[A\n"," 93% 10391/11208 [1:33:28<31:39,  2.32s/it]\u001b[A\n"," 93% 10392/11208 [1:33:30<31:37,  2.33s/it]\u001b[A\n"," 93% 10393/11208 [1:33:32<31:34,  2.32s/it]\u001b[A\n"," 93% 10394/11208 [1:33:35<31:33,  2.33s/it]\u001b[A\n"," 93% 10395/11208 [1:33:37<31:29,  2.32s/it]\u001b[A\n"," 93% 10396/11208 [1:33:39<31:27,  2.32s/it]\u001b[A\n"," 93% 10397/11208 [1:33:42<31:24,  2.32s/it]\u001b[A\n"," 93% 10398/11208 [1:33:44<31:25,  2.33s/it]\u001b[A\n"," 93% 10399/11208 [1:33:46<31:22,  2.33s/it]\u001b[A\n"," 93% 10400/11208 [1:33:49<31:21,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.7168, 'learning_rate': 3.6045681655960027e-06, 'epoch': 2.78}\n","\n"," 93% 10400/11208 [1:33:49<31:21,  2.33s/it]\u001b[A\n"," 93% 10401/11208 [1:33:51<31:20,  2.33s/it]\u001b[A\n"," 93% 10402/11208 [1:33:53<31:18,  2.33s/it]\u001b[A\n"," 93% 10403/11208 [1:33:56<31:17,  2.33s/it]\u001b[A\n"," 93% 10404/11208 [1:33:58<31:13,  2.33s/it]\u001b[A\n"," 93% 10405/11208 [1:34:00<31:13,  2.33s/it]\u001b[A\n"," 93% 10406/11208 [1:34:03<31:07,  2.33s/it]\u001b[A\n"," 93% 10407/11208 [1:34:05<31:05,  2.33s/it]\u001b[A\n"," 93% 10408/11208 [1:34:07<31:02,  2.33s/it]\u001b[A\n"," 93% 10409/11208 [1:34:10<30:59,  2.33s/it]\u001b[A\n"," 93% 10410/11208 [1:34:12<30:57,  2.33s/it]\u001b[A\n"," 93% 10411/11208 [1:34:14<30:50,  2.32s/it]\u001b[A\n"," 93% 10412/11208 [1:34:17<30:50,  2.33s/it]\u001b[A\n"," 93% 10413/11208 [1:34:19<30:51,  2.33s/it]\u001b[A\n"," 93% 10414/11208 [1:34:21<30:44,  2.32s/it]\u001b[A\n"," 93% 10415/11208 [1:34:24<30:44,  2.33s/it]\u001b[A\n"," 93% 10416/11208 [1:34:26<30:40,  2.32s/it]\u001b[A\n"," 93% 10417/11208 [1:34:28<30:35,  2.32s/it]\u001b[A\n"," 93% 10418/11208 [1:34:31<30:34,  2.32s/it]\u001b[A\n"," 93% 10419/11208 [1:34:33<30:34,  2.33s/it]\u001b[A\n"," 93% 10420/11208 [1:34:35<30:30,  2.32s/it]\u001b[A\n"," 93% 10421/11208 [1:34:38<30:24,  2.32s/it]\u001b[A\n"," 93% 10422/11208 [1:34:40<30:25,  2.32s/it]\u001b[A\n"," 93% 10423/11208 [1:34:42<30:25,  2.33s/it]\u001b[A\n"," 93% 10424/11208 [1:34:45<30:23,  2.33s/it]\u001b[A\n"," 93% 10425/11208 [1:34:47<30:24,  2.33s/it]\u001b[A\n"," 93% 10426/11208 [1:34:49<30:19,  2.33s/it]\u001b[A\n"," 93% 10427/11208 [1:34:51<30:02,  2.31s/it]\u001b[A\n"," 93% 10428/11208 [1:34:54<30:02,  2.31s/it]\u001b[A\n"," 93% 10429/11208 [1:34:56<30:01,  2.31s/it]\u001b[A\n"," 93% 10430/11208 [1:34:58<30:03,  2.32s/it]\u001b[A\n"," 93% 10431/11208 [1:35:01<30:04,  2.32s/it]\u001b[A\n"," 93% 10432/11208 [1:35:03<30:07,  2.33s/it]\u001b[A\n"," 93% 10433/11208 [1:35:05<30:04,  2.33s/it]\u001b[A\n"," 93% 10434/11208 [1:35:08<30:07,  2.34s/it]\u001b[A\n"," 93% 10435/11208 [1:35:10<30:00,  2.33s/it]\u001b[A\n"," 93% 10436/11208 [1:35:12<29:38,  2.30s/it]\u001b[A\n"," 93% 10437/11208 [1:35:15<29:42,  2.31s/it]\u001b[A\n"," 93% 10438/11208 [1:35:17<29:44,  2.32s/it]\u001b[A\n"," 93% 10439/11208 [1:35:19<29:47,  2.32s/it]\u001b[A\n"," 93% 10440/11208 [1:35:22<29:46,  2.33s/it]\u001b[A\n"," 93% 10441/11208 [1:35:24<29:46,  2.33s/it]\u001b[A\n"," 93% 10442/11208 [1:35:26<29:44,  2.33s/it]\u001b[A\n"," 93% 10443/11208 [1:35:29<29:41,  2.33s/it]\u001b[A\n"," 93% 10444/11208 [1:35:31<29:37,  2.33s/it]\u001b[A\n"," 93% 10445/11208 [1:35:33<29:35,  2.33s/it]\u001b[A\n"," 93% 10446/11208 [1:35:36<29:33,  2.33s/it]\u001b[A\n"," 93% 10447/11208 [1:35:38<29:30,  2.33s/it]\u001b[A\n"," 93% 10448/11208 [1:35:40<29:27,  2.33s/it]\u001b[A\n"," 93% 10449/11208 [1:35:43<29:23,  2.32s/it]\u001b[A\n"," 93% 10450/11208 [1:35:45<29:22,  2.33s/it]\u001b[A\n"," 93% 10451/11208 [1:35:47<29:19,  2.32s/it]\u001b[A\n"," 93% 10452/11208 [1:35:50<29:15,  2.32s/it]\u001b[A\n"," 93% 10453/11208 [1:35:52<29:13,  2.32s/it]\u001b[A\n"," 93% 10454/11208 [1:35:54<29:14,  2.33s/it]\u001b[A\n"," 93% 10455/11208 [1:35:57<29:08,  2.32s/it]\u001b[A\n"," 93% 10456/11208 [1:35:59<29:08,  2.33s/it]\u001b[A\n"," 93% 10457/11208 [1:36:01<29:06,  2.33s/it]\u001b[A\n"," 93% 10458/11208 [1:36:04<29:05,  2.33s/it]\u001b[A\n"," 93% 10459/11208 [1:36:06<29:04,  2.33s/it]\u001b[A\n"," 93% 10460/11208 [1:36:08<29:03,  2.33s/it]\u001b[A\n"," 93% 10461/11208 [1:36:11<29:01,  2.33s/it]\u001b[A\n"," 93% 10462/11208 [1:36:13<28:55,  2.33s/it]\u001b[A\n"," 93% 10463/11208 [1:36:15<28:54,  2.33s/it]\u001b[A\n"," 93% 10464/11208 [1:36:18<28:54,  2.33s/it]\u001b[A\n"," 93% 10465/11208 [1:36:20<28:48,  2.33s/it]\u001b[A\n"," 93% 10466/11208 [1:36:22<28:45,  2.33s/it]\u001b[A\n"," 93% 10467/11208 [1:36:25<28:46,  2.33s/it]\u001b[A\n"," 93% 10468/11208 [1:36:27<28:36,  2.32s/it]\u001b[A\n"," 93% 10469/11208 [1:36:29<27:58,  2.27s/it]\u001b[A\n"," 93% 10470/11208 [1:36:31<28:09,  2.29s/it]\u001b[A\n"," 93% 10471/11208 [1:36:34<28:15,  2.30s/it]\u001b[A\n"," 93% 10472/11208 [1:36:36<28:21,  2.31s/it]\u001b[A\n"," 93% 10473/11208 [1:36:38<27:54,  2.28s/it]\u001b[A\n"," 93% 10474/11208 [1:36:40<27:58,  2.29s/it]\u001b[A\n"," 93% 10475/11208 [1:36:43<27:06,  2.22s/it]\u001b[A\n"," 93% 10476/11208 [1:36:45<27:28,  2.25s/it]\u001b[A\n"," 93% 10477/11208 [1:36:47<27:41,  2.27s/it]\u001b[A\n"," 93% 10478/11208 [1:36:50<27:50,  2.29s/it]\u001b[A\n"," 93% 10479/11208 [1:36:52<27:57,  2.30s/it]\u001b[A\n"," 94% 10480/11208 [1:36:54<27:34,  2.27s/it]\u001b[A\n"," 94% 10481/11208 [1:36:56<27:43,  2.29s/it]\u001b[A\n"," 94% 10482/11208 [1:36:59<27:47,  2.30s/it]\u001b[A\n"," 94% 10483/11208 [1:37:01<27:51,  2.31s/it]\u001b[A\n"," 94% 10484/11208 [1:37:03<27:53,  2.31s/it]\u001b[A\n"," 94% 10485/11208 [1:37:06<27:55,  2.32s/it]\u001b[A\n"," 94% 10486/11208 [1:37:08<27:56,  2.32s/it]\u001b[A\n"," 94% 10487/11208 [1:37:10<27:56,  2.33s/it]\u001b[A\n"," 94% 10488/11208 [1:37:13<27:47,  2.32s/it]\u001b[A\n"," 94% 10489/11208 [1:37:15<27:40,  2.31s/it]\u001b[A\n"," 94% 10490/11208 [1:37:17<27:45,  2.32s/it]\u001b[A\n"," 94% 10491/11208 [1:37:20<27:45,  2.32s/it]\u001b[A\n"," 94% 10492/11208 [1:37:22<27:46,  2.33s/it]\u001b[A\n"," 94% 10493/11208 [1:37:24<27:43,  2.33s/it]\u001b[A\n"," 94% 10494/11208 [1:37:27<27:39,  2.32s/it]\u001b[A\n"," 94% 10495/11208 [1:37:29<27:32,  2.32s/it]\u001b[A\n"," 94% 10496/11208 [1:37:31<27:33,  2.32s/it]\u001b[A\n"," 94% 10497/11208 [1:37:34<27:32,  2.32s/it]\u001b[A\n"," 94% 10498/11208 [1:37:36<27:26,  2.32s/it]\u001b[A\n"," 94% 10499/11208 [1:37:38<27:27,  2.32s/it]\u001b[A\n"," 94% 10500/11208 [1:37:41<27:27,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6833, 'learning_rate': 3.1584582441113493e-06, 'epoch': 2.81}\n","\n"," 94% 10500/11208 [1:37:41<27:27,  2.33s/it]\u001b[A\n"," 94% 10501/11208 [1:37:43<27:26,  2.33s/it]\u001b[A\n"," 94% 10502/11208 [1:37:45<27:18,  2.32s/it]\u001b[A\n"," 94% 10503/11208 [1:37:47<27:20,  2.33s/it]\u001b[A\n"," 94% 10504/11208 [1:37:50<27:20,  2.33s/it]\u001b[A\n"," 94% 10505/11208 [1:37:52<27:22,  2.34s/it]\u001b[A\n"," 94% 10506/11208 [1:37:55<27:16,  2.33s/it]\u001b[A\n"," 94% 10507/11208 [1:37:57<27:11,  2.33s/it]\u001b[A\n"," 94% 10508/11208 [1:37:59<27:11,  2.33s/it]\u001b[A\n"," 94% 10509/11208 [1:38:01<27:08,  2.33s/it]\u001b[A\n"," 94% 10510/11208 [1:38:04<27:08,  2.33s/it]\u001b[A\n"," 94% 10511/11208 [1:38:06<27:05,  2.33s/it]\u001b[A\n"," 94% 10512/11208 [1:38:08<26:48,  2.31s/it]\u001b[A\n"," 94% 10513/11208 [1:38:11<26:51,  2.32s/it]\u001b[A\n"," 94% 10514/11208 [1:38:13<26:53,  2.32s/it]\u001b[A\n"," 94% 10515/11208 [1:38:15<26:53,  2.33s/it]\u001b[A\n"," 94% 10516/11208 [1:38:18<26:48,  2.33s/it]\u001b[A\n"," 94% 10517/11208 [1:38:20<26:47,  2.33s/it]\u001b[A\n"," 94% 10518/11208 [1:38:22<26:46,  2.33s/it]\u001b[A\n"," 94% 10519/11208 [1:38:25<26:44,  2.33s/it]\u001b[A\n"," 94% 10520/11208 [1:38:27<26:40,  2.33s/it]\u001b[A\n"," 94% 10521/11208 [1:38:29<26:42,  2.33s/it]\u001b[A\n"," 94% 10522/11208 [1:38:32<26:39,  2.33s/it]\u001b[A\n"," 94% 10523/11208 [1:38:34<26:36,  2.33s/it]\u001b[A\n"," 94% 10524/11208 [1:38:36<26:32,  2.33s/it]\u001b[A\n"," 94% 10525/11208 [1:38:39<26:28,  2.33s/it]\u001b[A\n"," 94% 10526/11208 [1:38:41<26:29,  2.33s/it]\u001b[A\n"," 94% 10527/11208 [1:38:43<26:26,  2.33s/it]\u001b[A\n"," 94% 10528/11208 [1:38:46<26:26,  2.33s/it]\u001b[A\n"," 94% 10529/11208 [1:38:48<26:21,  2.33s/it]\u001b[A\n"," 94% 10530/11208 [1:38:50<26:18,  2.33s/it]\u001b[A\n"," 94% 10531/11208 [1:38:53<26:15,  2.33s/it]\u001b[A\n"," 94% 10532/11208 [1:38:55<26:15,  2.33s/it]\u001b[A\n"," 94% 10533/11208 [1:38:57<26:12,  2.33s/it]\u001b[A\n"," 94% 10534/11208 [1:39:00<26:09,  2.33s/it]\u001b[A\n"," 94% 10535/11208 [1:39:02<25:38,  2.29s/it]\u001b[A\n"," 94% 10536/11208 [1:39:04<25:47,  2.30s/it]\u001b[A\n"," 94% 10537/11208 [1:39:07<25:49,  2.31s/it]\u001b[A\n"," 94% 10538/11208 [1:39:09<25:52,  2.32s/it]\u001b[A\n"," 94% 10539/11208 [1:39:11<25:54,  2.32s/it]\u001b[A\n"," 94% 10540/11208 [1:39:14<25:53,  2.33s/it]\u001b[A\n"," 94% 10541/11208 [1:39:16<25:52,  2.33s/it]\u001b[A\n"," 94% 10542/11208 [1:39:18<25:33,  2.30s/it]\u001b[A\n"," 94% 10543/11208 [1:39:20<25:37,  2.31s/it]\u001b[A\n"," 94% 10544/11208 [1:39:23<25:39,  2.32s/it]\u001b[A\n"," 94% 10545/11208 [1:39:25<25:35,  2.32s/it]\u001b[A\n"," 94% 10546/11208 [1:39:27<25:35,  2.32s/it]\u001b[A\n"," 94% 10547/11208 [1:39:30<25:37,  2.33s/it]\u001b[A\n"," 94% 10548/11208 [1:39:32<25:35,  2.33s/it]\u001b[A\n"," 94% 10549/11208 [1:39:34<25:36,  2.33s/it]\u001b[A\n"," 94% 10550/11208 [1:39:37<25:35,  2.33s/it]\u001b[A\n"," 94% 10551/11208 [1:39:39<25:33,  2.33s/it]\u001b[A\n"," 94% 10552/11208 [1:39:41<25:31,  2.33s/it]\u001b[A\n"," 94% 10553/11208 [1:39:44<25:29,  2.34s/it]\u001b[A\n"," 94% 10554/11208 [1:39:46<25:28,  2.34s/it]\u001b[A\n"," 94% 10555/11208 [1:39:48<25:22,  2.33s/it]\u001b[A\n"," 94% 10556/11208 [1:39:51<25:20,  2.33s/it]\u001b[A\n"," 94% 10557/11208 [1:39:53<25:17,  2.33s/it]\u001b[A\n"," 94% 10558/11208 [1:39:55<25:07,  2.32s/it]\u001b[A\n"," 94% 10559/11208 [1:39:58<25:06,  2.32s/it]\u001b[A\n"," 94% 10560/11208 [1:40:00<25:05,  2.32s/it]\u001b[A\n"," 94% 10561/11208 [1:40:02<25:04,  2.33s/it]\u001b[A\n"," 94% 10562/11208 [1:40:05<25:04,  2.33s/it]\u001b[A\n"," 94% 10563/11208 [1:40:07<24:59,  2.32s/it]\u001b[A\n"," 94% 10564/11208 [1:40:09<24:54,  2.32s/it]\u001b[A\n"," 94% 10565/11208 [1:40:12<24:55,  2.33s/it]\u001b[A\n"," 94% 10566/11208 [1:40:14<24:55,  2.33s/it]\u001b[A\n"," 94% 10567/11208 [1:40:16<24:51,  2.33s/it]\u001b[A\n"," 94% 10568/11208 [1:40:19<24:43,  2.32s/it]\u001b[A\n"," 94% 10569/11208 [1:40:21<24:42,  2.32s/it]\u001b[A\n"," 94% 10570/11208 [1:40:23<24:39,  2.32s/it]\u001b[A\n"," 94% 10571/11208 [1:40:26<24:40,  2.32s/it]\u001b[A\n"," 94% 10572/11208 [1:40:28<24:42,  2.33s/it]\u001b[A\n"," 94% 10573/11208 [1:40:30<24:31,  2.32s/it]\u001b[A\n"," 94% 10574/11208 [1:40:33<24:37,  2.33s/it]\u001b[A\n"," 94% 10575/11208 [1:40:35<24:35,  2.33s/it]\u001b[A\n"," 94% 10576/11208 [1:40:37<24:33,  2.33s/it]\u001b[A\n"," 94% 10577/11208 [1:40:40<24:32,  2.33s/it]\u001b[A\n"," 94% 10578/11208 [1:40:42<24:30,  2.33s/it]\u001b[A\n"," 94% 10579/11208 [1:40:44<24:27,  2.33s/it]\u001b[A\n"," 94% 10580/11208 [1:40:47<24:23,  2.33s/it]\u001b[A\n"," 94% 10581/11208 [1:40:49<24:21,  2.33s/it]\u001b[A\n"," 94% 10582/11208 [1:40:51<24:09,  2.32s/it]\u001b[A\n"," 94% 10583/11208 [1:40:54<24:05,  2.31s/it]\u001b[A\n"," 94% 10584/11208 [1:40:56<24:02,  2.31s/it]\u001b[A\n"," 94% 10585/11208 [1:40:58<23:59,  2.31s/it]\u001b[A\n"," 94% 10586/11208 [1:41:00<24:00,  2.32s/it]\u001b[A\n"," 94% 10587/11208 [1:41:03<23:56,  2.31s/it]\u001b[A\n"," 94% 10588/11208 [1:41:05<23:56,  2.32s/it]\u001b[A\n"," 94% 10589/11208 [1:41:07<23:26,  2.27s/it]\u001b[A\n"," 94% 10590/11208 [1:41:10<23:33,  2.29s/it]\u001b[A\n"," 94% 10591/11208 [1:41:12<23:36,  2.30s/it]\u001b[A\n"," 95% 10592/11208 [1:41:14<23:38,  2.30s/it]\u001b[A\n"," 95% 10593/11208 [1:41:17<23:41,  2.31s/it]\u001b[A\n"," 95% 10594/11208 [1:41:19<23:42,  2.32s/it]\u001b[A\n"," 95% 10595/11208 [1:41:21<23:41,  2.32s/it]\u001b[A\n"," 95% 10596/11208 [1:41:24<23:40,  2.32s/it]\u001b[A\n"," 95% 10597/11208 [1:41:26<23:39,  2.32s/it]\u001b[A\n"," 95% 10598/11208 [1:41:28<23:39,  2.33s/it]\u001b[A\n"," 95% 10599/11208 [1:41:31<23:34,  2.32s/it]\u001b[A\n"," 95% 10600/11208 [1:41:33<23:34,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6744, 'learning_rate': 2.7123483226266955e-06, 'epoch': 2.84}\n","\n"," 95% 10600/11208 [1:41:33<23:34,  2.33s/it]\u001b[A\n"," 95% 10601/11208 [1:41:35<23:33,  2.33s/it]\u001b[A\n"," 95% 10602/11208 [1:41:38<23:31,  2.33s/it]\u001b[A\n"," 95% 10603/11208 [1:41:40<23:24,  2.32s/it]\u001b[A\n"," 95% 10604/11208 [1:41:42<23:22,  2.32s/it]\u001b[A\n"," 95% 10605/11208 [1:41:44<23:21,  2.32s/it]\u001b[A\n"," 95% 10606/11208 [1:41:47<23:19,  2.32s/it]\u001b[A\n"," 95% 10607/11208 [1:41:49<23:19,  2.33s/it]\u001b[A\n"," 95% 10608/11208 [1:41:51<23:18,  2.33s/it]\u001b[A\n"," 95% 10609/11208 [1:41:54<22:49,  2.29s/it]\u001b[A\n"," 95% 10610/11208 [1:41:56<22:51,  2.29s/it]\u001b[A\n"," 95% 10611/11208 [1:41:58<22:54,  2.30s/it]\u001b[A\n"," 95% 10612/11208 [1:42:01<22:56,  2.31s/it]\u001b[A\n"," 95% 10613/11208 [1:42:03<22:55,  2.31s/it]\u001b[A\n"," 95% 10614/11208 [1:42:05<22:56,  2.32s/it]\u001b[A\n"," 95% 10615/11208 [1:42:08<22:42,  2.30s/it]\u001b[A\n"," 95% 10616/11208 [1:42:10<22:44,  2.30s/it]\u001b[A\n"," 95% 10617/11208 [1:42:12<22:47,  2.31s/it]\u001b[A\n"," 95% 10618/11208 [1:42:14<21:57,  2.23s/it]\u001b[A\n"," 95% 10619/11208 [1:42:16<22:00,  2.24s/it]\u001b[A\n"," 95% 10620/11208 [1:42:19<22:12,  2.27s/it]\u001b[A\n"," 95% 10621/11208 [1:42:21<22:22,  2.29s/it]\u001b[A\n"," 95% 10622/11208 [1:42:23<22:06,  2.26s/it]\u001b[A\n"," 95% 10623/11208 [1:42:26<22:15,  2.28s/it]\u001b[A\n"," 95% 10624/11208 [1:42:28<22:18,  2.29s/it]\u001b[A\n"," 95% 10625/11208 [1:42:30<22:25,  2.31s/it]\u001b[A\n"," 95% 10626/11208 [1:42:33<22:28,  2.32s/it]\u001b[A\n"," 95% 10627/11208 [1:42:35<22:28,  2.32s/it]\u001b[A\n"," 95% 10628/11208 [1:42:37<22:25,  2.32s/it]\u001b[A\n"," 95% 10629/11208 [1:42:40<22:26,  2.33s/it]\u001b[A\n"," 95% 10630/11208 [1:42:42<22:24,  2.33s/it]\u001b[A\n"," 95% 10631/11208 [1:42:44<22:20,  2.32s/it]\u001b[A\n"," 95% 10632/11208 [1:42:47<22:17,  2.32s/it]\u001b[A\n"," 95% 10633/11208 [1:42:49<22:15,  2.32s/it]\u001b[A\n"," 95% 10634/11208 [1:42:51<22:11,  2.32s/it]\u001b[A\n"," 95% 10635/11208 [1:42:54<22:09,  2.32s/it]\u001b[A\n"," 95% 10636/11208 [1:42:56<22:10,  2.33s/it]\u001b[A\n"," 95% 10637/11208 [1:42:58<22:04,  2.32s/it]\u001b[A\n"," 95% 10638/11208 [1:43:01<22:05,  2.33s/it]\u001b[A\n"," 95% 10639/11208 [1:43:03<22:05,  2.33s/it]\u001b[A\n"," 95% 10640/11208 [1:43:05<22:02,  2.33s/it]\u001b[A\n"," 95% 10641/11208 [1:43:08<21:57,  2.32s/it]\u001b[A\n"," 95% 10642/11208 [1:43:10<21:56,  2.33s/it]\u001b[A\n"," 95% 10643/11208 [1:43:12<21:55,  2.33s/it]\u001b[A\n"," 95% 10644/11208 [1:43:15<21:54,  2.33s/it]\u001b[A\n"," 95% 10645/11208 [1:43:17<21:54,  2.34s/it]\u001b[A\n"," 95% 10646/11208 [1:43:19<21:49,  2.33s/it]\u001b[A\n"," 95% 10647/11208 [1:43:22<21:46,  2.33s/it]\u001b[A\n"," 95% 10648/11208 [1:43:24<21:42,  2.33s/it]\u001b[A\n"," 95% 10649/11208 [1:43:26<21:42,  2.33s/it]\u001b[A\n"," 95% 10650/11208 [1:43:28<21:33,  2.32s/it]\u001b[A\n"," 95% 10651/11208 [1:43:31<21:30,  2.32s/it]\u001b[A\n"," 95% 10652/11208 [1:43:33<21:30,  2.32s/it]\u001b[A\n"," 95% 10653/11208 [1:43:35<21:29,  2.32s/it]\u001b[A\n"," 95% 10654/11208 [1:43:38<21:26,  2.32s/it]\u001b[A\n"," 95% 10655/11208 [1:43:40<21:23,  2.32s/it]\u001b[A\n"," 95% 10656/11208 [1:43:42<21:19,  2.32s/it]\u001b[A\n"," 95% 10657/11208 [1:43:45<21:19,  2.32s/it]\u001b[A\n"," 95% 10658/11208 [1:43:47<21:04,  2.30s/it]\u001b[A\n"," 95% 10659/11208 [1:43:49<21:07,  2.31s/it]\u001b[A\n"," 95% 10660/11208 [1:43:52<21:07,  2.31s/it]\u001b[A\n"," 95% 10661/11208 [1:43:54<21:08,  2.32s/it]\u001b[A\n"," 95% 10662/11208 [1:43:56<21:08,  2.32s/it]\u001b[A\n"," 95% 10663/11208 [1:43:59<21:07,  2.33s/it]\u001b[A\n"," 95% 10664/11208 [1:44:01<21:06,  2.33s/it]\u001b[A\n"," 95% 10665/11208 [1:44:03<21:02,  2.32s/it]\u001b[A\n"," 95% 10666/11208 [1:44:06<20:57,  2.32s/it]\u001b[A\n"," 95% 10667/11208 [1:44:08<20:56,  2.32s/it]\u001b[A\n"," 95% 10668/11208 [1:44:10<20:38,  2.29s/it]\u001b[A\n"," 95% 10669/11208 [1:44:12<20:41,  2.30s/it]\u001b[A\n"," 95% 10670/11208 [1:44:15<20:44,  2.31s/it]\u001b[A\n"," 95% 10671/11208 [1:44:17<20:22,  2.28s/it]\u001b[A\n"," 95% 10672/11208 [1:44:19<20:06,  2.25s/it]\u001b[A\n"," 95% 10673/11208 [1:44:22<20:17,  2.28s/it]\u001b[A\n"," 95% 10674/11208 [1:44:24<20:24,  2.29s/it]\u001b[A\n"," 95% 10675/11208 [1:44:26<20:27,  2.30s/it]\u001b[A\n"," 95% 10676/11208 [1:44:28<20:28,  2.31s/it]\u001b[A\n"," 95% 10677/11208 [1:44:31<20:28,  2.31s/it]\u001b[A\n"," 95% 10678/11208 [1:44:33<20:12,  2.29s/it]\u001b[A\n"," 95% 10679/11208 [1:44:35<20:18,  2.30s/it]\u001b[A\n"," 95% 10680/11208 [1:44:38<20:20,  2.31s/it]\u001b[A\n"," 95% 10681/11208 [1:44:40<20:22,  2.32s/it]\u001b[A\n"," 95% 10682/11208 [1:44:42<20:21,  2.32s/it]\u001b[A\n"," 95% 10683/11208 [1:44:45<20:08,  2.30s/it]\u001b[A\n"," 95% 10684/11208 [1:44:47<20:02,  2.30s/it]\u001b[A\n"," 95% 10685/11208 [1:44:49<20:01,  2.30s/it]\u001b[A\n"," 95% 10686/11208 [1:44:52<20:05,  2.31s/it]\u001b[A\n"," 95% 10687/11208 [1:44:54<20:07,  2.32s/it]\u001b[A\n"," 95% 10688/11208 [1:44:56<20:07,  2.32s/it]\u001b[A\n"," 95% 10689/11208 [1:44:59<20:04,  2.32s/it]\u001b[A\n"," 95% 10690/11208 [1:45:01<20:03,  2.32s/it]\u001b[A\n"," 95% 10691/11208 [1:45:03<20:02,  2.33s/it]\u001b[A\n"," 95% 10692/11208 [1:45:06<20:01,  2.33s/it]\u001b[A\n"," 95% 10693/11208 [1:45:08<19:42,  2.30s/it]\u001b[A\n"," 95% 10694/11208 [1:45:10<19:42,  2.30s/it]\u001b[A\n"," 95% 10695/11208 [1:45:12<19:42,  2.30s/it]\u001b[A\n"," 95% 10696/11208 [1:45:15<19:42,  2.31s/it]\u001b[A\n"," 95% 10697/11208 [1:45:17<19:27,  2.29s/it]\u001b[A\n"," 95% 10698/11208 [1:45:19<19:34,  2.30s/it]\u001b[A\n"," 95% 10699/11208 [1:45:22<19:35,  2.31s/it]\u001b[A\n"," 95% 10700/11208 [1:45:24<19:37,  2.32s/it]\u001b[A\n","\u001b[A{'loss': 1.6614, 'learning_rate': 2.2662384011420414e-06, 'epoch': 2.86}\n","\n"," 95% 10700/11208 [1:45:24<19:37,  2.32s/it]\u001b[A\n"," 95% 10701/11208 [1:45:26<19:36,  2.32s/it]\u001b[A\n"," 95% 10702/11208 [1:45:29<19:36,  2.33s/it]\u001b[A\n"," 95% 10703/11208 [1:45:31<19:31,  2.32s/it]\u001b[A\n"," 96% 10704/11208 [1:45:33<19:31,  2.32s/it]\u001b[A\n"," 96% 10705/11208 [1:45:36<19:30,  2.33s/it]\u001b[A\n"," 96% 10706/11208 [1:45:38<19:27,  2.33s/it]\u001b[A\n"," 96% 10707/11208 [1:45:40<19:26,  2.33s/it]\u001b[A\n"," 96% 10708/11208 [1:45:43<19:24,  2.33s/it]\u001b[A\n"," 96% 10709/11208 [1:45:45<19:18,  2.32s/it]\u001b[A\n"," 96% 10710/11208 [1:45:47<19:17,  2.32s/it]\u001b[A\n"," 96% 10711/11208 [1:45:50<19:17,  2.33s/it]\u001b[A\n"," 96% 10712/11208 [1:45:52<19:14,  2.33s/it]\u001b[A\n"," 96% 10713/11208 [1:45:54<19:13,  2.33s/it]\u001b[A\n"," 96% 10714/11208 [1:45:57<19:14,  2.34s/it]\u001b[A\n"," 96% 10715/11208 [1:45:59<19:13,  2.34s/it]\u001b[A\n"," 96% 10716/11208 [1:46:01<19:10,  2.34s/it]\u001b[A\n"," 96% 10717/11208 [1:46:04<19:06,  2.34s/it]\u001b[A\n"," 96% 10718/11208 [1:46:06<19:02,  2.33s/it]\u001b[A\n"," 96% 10719/11208 [1:46:08<18:59,  2.33s/it]\u001b[A\n"," 96% 10720/11208 [1:46:11<18:57,  2.33s/it]\u001b[A\n"," 96% 10721/11208 [1:46:13<18:57,  2.33s/it]\u001b[A\n"," 96% 10722/11208 [1:46:15<18:52,  2.33s/it]\u001b[A\n"," 96% 10723/11208 [1:46:18<18:48,  2.33s/it]\u001b[A\n"," 96% 10724/11208 [1:46:20<18:48,  2.33s/it]\u001b[A\n"," 96% 10725/11208 [1:46:22<18:44,  2.33s/it]\u001b[A\n"," 96% 10726/11208 [1:46:25<18:43,  2.33s/it]\u001b[A\n"," 96% 10727/11208 [1:46:27<18:39,  2.33s/it]\u001b[A\n"," 96% 10728/11208 [1:46:29<18:25,  2.30s/it]\u001b[A\n"," 96% 10729/11208 [1:46:31<18:27,  2.31s/it]\u001b[A\n"," 96% 10730/11208 [1:46:34<18:28,  2.32s/it]\u001b[A\n"," 96% 10731/11208 [1:46:36<18:27,  2.32s/it]\u001b[A\n"," 96% 10732/11208 [1:46:38<18:25,  2.32s/it]\u001b[A\n"," 96% 10733/11208 [1:46:41<18:22,  2.32s/it]\u001b[A\n"," 96% 10734/11208 [1:46:43<18:22,  2.33s/it]\u001b[A\n"," 96% 10735/11208 [1:46:45<18:21,  2.33s/it]\u001b[A\n"," 96% 10736/11208 [1:46:48<18:08,  2.31s/it]\u001b[A\n"," 96% 10737/11208 [1:46:50<18:07,  2.31s/it]\u001b[A\n"," 96% 10738/11208 [1:46:52<18:07,  2.31s/it]\u001b[A\n"," 96% 10739/11208 [1:46:55<18:04,  2.31s/it]\u001b[A\n"," 96% 10740/11208 [1:46:57<18:03,  2.32s/it]\u001b[A\n"," 96% 10741/11208 [1:46:59<18:01,  2.32s/it]\u001b[A\n"," 96% 10742/11208 [1:47:02<17:51,  2.30s/it]\u001b[A\n"," 96% 10743/11208 [1:47:04<17:55,  2.31s/it]\u001b[A\n"," 96% 10744/11208 [1:47:06<17:55,  2.32s/it]\u001b[A\n"," 96% 10745/11208 [1:47:09<17:55,  2.32s/it]\u001b[A\n"," 96% 10746/11208 [1:47:11<17:55,  2.33s/it]\u001b[A\n"," 96% 10747/11208 [1:47:13<17:52,  2.33s/it]\u001b[A\n"," 96% 10748/11208 [1:47:15<17:48,  2.32s/it]\u001b[A\n"," 96% 10749/11208 [1:47:18<17:45,  2.32s/it]\u001b[A\n"," 96% 10750/11208 [1:47:20<17:44,  2.32s/it]\u001b[A\n"," 96% 10751/11208 [1:47:22<17:29,  2.30s/it]\u001b[A\n"," 96% 10752/11208 [1:47:25<17:30,  2.30s/it]\u001b[A\n"," 96% 10753/11208 [1:47:27<17:31,  2.31s/it]\u001b[A\n"," 96% 10754/11208 [1:47:29<16:59,  2.24s/it]\u001b[A\n"," 96% 10755/11208 [1:47:31<17:07,  2.27s/it]\u001b[A\n"," 96% 10756/11208 [1:47:34<17:12,  2.29s/it]\u001b[A\n"," 96% 10757/11208 [1:47:36<17:14,  2.29s/it]\u001b[A\n"," 96% 10758/11208 [1:47:38<17:14,  2.30s/it]\u001b[A\n"," 96% 10759/11208 [1:47:41<17:14,  2.30s/it]\u001b[A\n"," 96% 10760/11208 [1:47:43<17:06,  2.29s/it]\u001b[A\n"," 96% 10761/11208 [1:47:45<17:09,  2.30s/it]\u001b[A\n"," 96% 10762/11208 [1:47:48<17:10,  2.31s/it]\u001b[A\n"," 96% 10763/11208 [1:47:50<17:09,  2.31s/it]\u001b[A\n"," 96% 10764/11208 [1:47:52<17:06,  2.31s/it]\u001b[A\n"," 96% 10765/11208 [1:47:54<16:52,  2.29s/it]\u001b[A\n"," 96% 10766/11208 [1:47:57<16:54,  2.30s/it]\u001b[A\n"," 96% 10767/11208 [1:47:59<16:58,  2.31s/it]\u001b[A\n"," 96% 10768/11208 [1:48:01<16:58,  2.31s/it]\u001b[A\n"," 96% 10769/11208 [1:48:04<16:57,  2.32s/it]\u001b[A\n"," 96% 10770/11208 [1:48:06<16:56,  2.32s/it]\u001b[A\n"," 96% 10771/11208 [1:48:08<16:55,  2.32s/it]\u001b[A\n"," 96% 10772/11208 [1:48:11<16:52,  2.32s/it]\u001b[A\n"," 96% 10773/11208 [1:48:13<16:45,  2.31s/it]\u001b[A\n"," 96% 10774/11208 [1:48:15<16:46,  2.32s/it]\u001b[A\n"," 96% 10775/11208 [1:48:18<16:45,  2.32s/it]\u001b[A\n"," 96% 10776/11208 [1:48:20<16:45,  2.33s/it]\u001b[A\n"," 96% 10777/11208 [1:48:22<16:41,  2.32s/it]\u001b[A\n"," 96% 10778/11208 [1:48:25<16:39,  2.33s/it]\u001b[A\n"," 96% 10779/11208 [1:48:27<16:38,  2.33s/it]\u001b[A\n"," 96% 10780/11208 [1:48:29<16:31,  2.32s/it]\u001b[A\n"," 96% 10781/11208 [1:48:32<16:29,  2.32s/it]\u001b[A\n"," 96% 10782/11208 [1:48:34<16:27,  2.32s/it]\u001b[A\n"," 96% 10783/11208 [1:48:36<16:27,  2.32s/it]\u001b[A\n"," 96% 10784/11208 [1:48:39<16:20,  2.31s/it]\u001b[A\n"," 96% 10785/11208 [1:48:41<16:22,  2.32s/it]\u001b[A\n"," 96% 10786/11208 [1:48:43<16:24,  2.33s/it]\u001b[A\n"," 96% 10787/11208 [1:48:46<16:11,  2.31s/it]\u001b[A\n"," 96% 10788/11208 [1:48:48<16:13,  2.32s/it]\u001b[A\n"," 96% 10789/11208 [1:48:50<16:12,  2.32s/it]\u001b[A\n"," 96% 10790/11208 [1:48:53<16:09,  2.32s/it]\u001b[A\n"," 96% 10791/11208 [1:48:55<16:10,  2.33s/it]\u001b[A\n"," 96% 10792/11208 [1:48:57<16:08,  2.33s/it]\u001b[A\n"," 96% 10793/11208 [1:49:00<16:06,  2.33s/it]\u001b[A\n"," 96% 10794/11208 [1:49:02<16:03,  2.33s/it]\u001b[A\n"," 96% 10795/11208 [1:49:04<16:00,  2.33s/it]\u001b[A\n"," 96% 10796/11208 [1:49:07<15:57,  2.33s/it]\u001b[A\n"," 96% 10797/11208 [1:49:09<15:56,  2.33s/it]\u001b[A\n"," 96% 10798/11208 [1:49:11<15:52,  2.32s/it]\u001b[A\n"," 96% 10799/11208 [1:49:13<15:50,  2.32s/it]\u001b[A\n"," 96% 10800/11208 [1:49:16<15:50,  2.33s/it]\u001b[A\n","\u001b[A{'loss': 1.6787, 'learning_rate': 1.8201284796573876e-06, 'epoch': 2.89}\n","\n"," 96% 10800/11208 [1:49:16<15:50,  2.33s/it]\u001b[A\n"," 96% 10801/11208 [1:49:18<15:49,  2.33s/it]\u001b[A\n"," 96% 10802/11208 [1:49:20<15:32,  2.30s/it]\u001b[A\n"," 96% 10803/11208 [1:49:23<15:35,  2.31s/it]\u001b[A\n"," 96% 10804/11208 [1:49:25<15:35,  2.32s/it]\u001b[A\n"," 96% 10805/11208 [1:49:27<15:33,  2.32s/it]\u001b[A\n"," 96% 10806/11208 [1:49:29<15:07,  2.26s/it]\u001b[A\n"," 96% 10807/11208 [1:49:32<15:14,  2.28s/it]\u001b[A\n"," 96% 10808/11208 [1:49:34<15:16,  2.29s/it]\u001b[A\n"," 96% 10809/11208 [1:49:36<15:18,  2.30s/it]\u001b[A\n"," 96% 10810/11208 [1:49:39<15:19,  2.31s/it]\u001b[A\n"," 96% 10811/11208 [1:49:41<15:19,  2.32s/it]\u001b[A\n"," 96% 10812/11208 [1:49:43<15:17,  2.32s/it]\u001b[A\n"," 96% 10813/11208 [1:49:46<15:16,  2.32s/it]\u001b[A\n"," 96% 10814/11208 [1:49:48<15:15,  2.32s/it]\u001b[A\n"," 96% 10815/11208 [1:49:50<15:13,  2.32s/it]\u001b[A\n"," 97% 10816/11208 [1:49:53<15:12,  2.33s/it]\u001b[A\n"," 97% 10817/11208 [1:49:55<15:09,  2.33s/it]\u001b[A\n"," 97% 10818/11208 [1:49:57<14:59,  2.31s/it]\u001b[A\n"," 97% 10819/11208 [1:50:00<15:00,  2.32s/it]\u001b[A\n"," 97% 10820/11208 [1:50:02<14:59,  2.32s/it]\u001b[A\n"," 97% 10821/11208 [1:50:04<14:58,  2.32s/it]\u001b[A\n"," 97% 10822/11208 [1:50:07<14:57,  2.33s/it]\u001b[A\n"," 97% 10823/11208 [1:50:09<14:56,  2.33s/it]\u001b[A\n"," 97% 10824/11208 [1:50:11<14:53,  2.33s/it]\u001b[A\n"," 97% 10825/11208 [1:50:14<14:50,  2.32s/it]\u001b[A\n"," 97% 10826/11208 [1:50:16<14:47,  2.32s/it]\u001b[A\n"," 97% 10827/11208 [1:50:18<14:46,  2.33s/it]\u001b[A\n"," 97% 10828/11208 [1:50:21<14:44,  2.33s/it]\u001b[A\n"," 97% 10829/11208 [1:50:23<14:43,  2.33s/it]\u001b[A\n"," 97% 10830/11208 [1:50:25<14:40,  2.33s/it]\u001b[A\n"," 97% 10831/11208 [1:50:28<14:37,  2.33s/it]\u001b[A\n"," 97% 10832/11208 [1:50:30<14:33,  2.32s/it]\u001b[A\n"," 97% 10833/11208 [1:50:32<14:32,  2.33s/it]\u001b[A\n"," 97% 10834/11208 [1:50:35<14:30,  2.33s/it]\u001b[A\n"," 97% 10835/11208 [1:50:37<14:27,  2.33s/it]\u001b[A\n"," 97% 10836/11208 [1:50:39<14:25,  2.33s/it]\u001b[A\n"," 97% 10837/11208 [1:50:41<13:56,  2.25s/it]\u001b[A\n"," 97% 10838/11208 [1:50:44<13:54,  2.26s/it]\u001b[A\n"," 97% 10839/11208 [1:50:46<14:00,  2.28s/it]\u001b[A\n"," 97% 10840/11208 [1:50:48<14:05,  2.30s/it]\u001b[A\n"," 97% 10841/11208 [1:50:51<14:04,  2.30s/it]\u001b[A\n"," 97% 10842/11208 [1:50:53<14:05,  2.31s/it]\u001b[A\n"," 97% 10843/11208 [1:50:55<14:05,  2.32s/it]\u001b[A\n"," 97% 10844/11208 [1:50:58<14:02,  2.32s/it]\u001b[A\n"," 97% 10845/11208 [1:51:00<14:02,  2.32s/it]\u001b[A\n"," 97% 10846/11208 [1:51:02<14:00,  2.32s/it]\u001b[A\n"," 97% 10847/11208 [1:51:05<13:57,  2.32s/it]\u001b[A\n"," 97% 10848/11208 [1:51:07<13:55,  2.32s/it]\u001b[A\n"," 97% 10849/11208 [1:51:09<13:54,  2.32s/it]\u001b[A\n"," 97% 10850/11208 [1:51:12<13:53,  2.33s/it]\u001b[A\n"," 97% 10851/11208 [1:51:14<13:51,  2.33s/it]\u001b[A\n"," 97% 10852/11208 [1:51:16<13:49,  2.33s/it]\u001b[A\n"," 97% 10853/11208 [1:51:19<13:48,  2.33s/it]\u001b[A\n"," 97% 10854/11208 [1:51:21<13:46,  2.33s/it]\u001b[A\n"," 97% 10855/11208 [1:51:23<13:45,  2.34s/it]\u001b[A\n"," 97% 10856/11208 [1:51:26<13:44,  2.34s/it]\u001b[A\n"," 97% 10857/11208 [1:51:28<13:40,  2.34s/it]\u001b[A\n"," 97% 10858/11208 [1:51:30<13:37,  2.34s/it]\u001b[A\n"," 97% 10859/11208 [1:51:32<13:26,  2.31s/it]\u001b[A\n"," 97% 10860/11208 [1:51:35<13:24,  2.31s/it]\u001b[A\n"," 97% 10861/11208 [1:51:37<13:24,  2.32s/it]\u001b[A\n"," 97% 10862/11208 [1:51:39<13:22,  2.32s/it]\u001b[A\n"," 97% 10863/11208 [1:51:42<13:21,  2.32s/it]\u001b[A\n"," 97% 10864/11208 [1:51:44<13:20,  2.33s/it]\u001b[A\n"," 97% 10865/11208 [1:51:46<13:12,  2.31s/it]\u001b[A\n"," 97% 10866/11208 [1:51:49<13:11,  2.31s/it]\u001b[A\n"," 97% 10867/11208 [1:51:51<13:11,  2.32s/it]\u001b[A\n"," 97% 10868/11208 [1:51:53<13:10,  2.33s/it]\u001b[A\n"," 97% 10869/11208 [1:51:56<13:07,  2.32s/it]\u001b[A\n"," 97% 10870/11208 [1:51:58<13:06,  2.33s/it]\u001b[A\n"," 97% 10871/11208 [1:52:00<13:05,  2.33s/it]\u001b[A\n"," 97% 10872/11208 [1:52:03<13:01,  2.33s/it]\u001b[A\n"," 97% 10873/11208 [1:52:05<12:58,  2.32s/it]\u001b[A\n"," 97% 10874/11208 [1:52:07<12:57,  2.33s/it]\u001b[A\n"," 97% 10875/11208 [1:52:10<12:55,  2.33s/it]\u001b[A\n"," 97% 10876/11208 [1:52:12<12:51,  2.32s/it]\u001b[A\n"," 97% 10877/11208 [1:52:14<12:49,  2.33s/it]\u001b[A\n"," 97% 10878/11208 [1:52:17<12:47,  2.33s/it]\u001b[A\n"," 97% 10879/11208 [1:52:19<12:43,  2.32s/it]\u001b[A\n"," 97% 10880/11208 [1:52:21<12:37,  2.31s/it]\u001b[A\n"," 97% 10881/11208 [1:52:24<12:37,  2.32s/it]\u001b[A\n"," 97% 10882/11208 [1:52:26<12:34,  2.31s/it]\u001b[A\n"," 97% 10883/11208 [1:52:28<12:33,  2.32s/it]\u001b[A\n"," 97% 10884/11208 [1:52:31<12:33,  2.32s/it]\u001b[A\n"," 97% 10885/11208 [1:52:33<12:31,  2.33s/it]\u001b[A\n"," 97% 10886/11208 [1:52:35<12:20,  2.30s/it]\u001b[A\n"," 97% 10887/11208 [1:52:37<12:17,  2.30s/it]\u001b[A\n"," 97% 10888/11208 [1:52:40<12:15,  2.30s/it]\u001b[A\n"," 97% 10889/11208 [1:52:42<11:59,  2.26s/it]\u001b[A\n"," 97% 10890/11208 [1:52:44<12:05,  2.28s/it]\u001b[A\n"," 97% 10891/11208 [1:52:46<11:45,  2.23s/it]\u001b[A\n"," 97% 10892/11208 [1:52:48<11:35,  2.20s/it]\u001b[A\n"," 97% 10893/11208 [1:52:51<11:45,  2.24s/it]\u001b[A\n"," 97% 10894/11208 [1:52:53<11:52,  2.27s/it]\u001b[A\n"," 97% 10895/11208 [1:52:55<11:47,  2.26s/it]\u001b[A\n"," 97% 10896/11208 [1:52:58<11:52,  2.28s/it]\u001b[A\n"," 97% 10897/11208 [1:53:00<11:54,  2.30s/it]\u001b[A\n"," 97% 10898/11208 [1:53:02<11:54,  2.31s/it]\u001b[A\n"," 97% 10899/11208 [1:53:05<11:55,  2.31s/it]\u001b[A\n"," 97% 10900/11208 [1:53:07<11:55,  2.32s/it]\u001b[A\n","\u001b[A{'loss': 1.6487, 'learning_rate': 1.3740185581727338e-06, 'epoch': 2.92}\n","\n"," 97% 10900/11208 [1:53:07<11:55,  2.32s/it]\u001b[A\n"," 97% 10901/11208 [1:53:09<11:54,  2.33s/it]\u001b[A\n"," 97% 10902/11208 [1:53:12<11:45,  2.30s/it]\u001b[A\n"," 97% 10903/11208 [1:53:14<11:43,  2.31s/it]\u001b[A\n"," 97% 10904/11208 [1:53:16<11:43,  2.32s/it]\u001b[A\n"," 97% 10905/11208 [1:53:19<11:42,  2.32s/it]\u001b[A\n"," 97% 10906/11208 [1:53:21<11:41,  2.32s/it]\u001b[A\n"," 97% 10907/11208 [1:53:23<11:39,  2.32s/it]\u001b[A\n"," 97% 10908/11208 [1:53:26<11:37,  2.33s/it]\u001b[A\n"," 97% 10909/11208 [1:53:28<11:36,  2.33s/it]\u001b[A\n"," 97% 10910/11208 [1:53:30<11:33,  2.33s/it]\u001b[A\n"," 97% 10911/11208 [1:53:33<11:31,  2.33s/it]\u001b[A\n"," 97% 10912/11208 [1:53:35<11:27,  2.32s/it]\u001b[A\n"," 97% 10913/11208 [1:53:37<11:24,  2.32s/it]\u001b[A\n"," 97% 10914/11208 [1:53:40<11:23,  2.33s/it]\u001b[A\n"," 97% 10915/11208 [1:53:42<11:21,  2.33s/it]\u001b[A\n"," 97% 10916/11208 [1:53:44<11:20,  2.33s/it]\u001b[A\n"," 97% 10917/11208 [1:53:47<11:17,  2.33s/it]\u001b[A\n"," 97% 10918/11208 [1:53:49<11:16,  2.33s/it]\u001b[A\n"," 97% 10919/11208 [1:53:51<11:13,  2.33s/it]\u001b[A\n"," 97% 10920/11208 [1:53:53<11:03,  2.30s/it]\u001b[A\n"," 97% 10921/11208 [1:53:56<11:03,  2.31s/it]\u001b[A\n"," 97% 10922/11208 [1:53:58<10:55,  2.29s/it]\u001b[A\n"," 97% 10923/11208 [1:54:00<10:52,  2.29s/it]\u001b[A\n"," 97% 10924/11208 [1:54:03<10:52,  2.30s/it]\u001b[A\n"," 97% 10925/11208 [1:54:05<10:52,  2.31s/it]\u001b[A\n"," 97% 10926/11208 [1:54:07<10:54,  2.32s/it]\u001b[A\n"," 97% 10927/11208 [1:54:10<10:54,  2.33s/it]\u001b[A\n"," 98% 10928/11208 [1:54:12<10:52,  2.33s/it]\u001b[A\n"," 98% 10929/11208 [1:54:14<10:50,  2.33s/it]\u001b[A\n"," 98% 10930/11208 [1:54:17<10:47,  2.33s/it]\u001b[A\n"," 98% 10931/11208 [1:54:19<10:44,  2.33s/it]\u001b[A\n"," 98% 10932/11208 [1:54:21<10:40,  2.32s/it]\u001b[A\n"," 98% 10933/11208 [1:54:24<10:38,  2.32s/it]\u001b[A\n"," 98% 10934/11208 [1:54:26<10:37,  2.33s/it]\u001b[A\n"," 98% 10935/11208 [1:54:28<10:36,  2.33s/it]\u001b[A\n"," 98% 10936/11208 [1:54:31<10:34,  2.33s/it]\u001b[A\n"," 98% 10937/11208 [1:54:33<10:30,  2.33s/it]\u001b[A\n"," 98% 10938/11208 [1:54:35<10:27,  2.33s/it]\u001b[A\n"," 98% 10939/11208 [1:54:38<10:26,  2.33s/it]\u001b[A\n"," 98% 10940/11208 [1:54:40<10:23,  2.33s/it]\u001b[A\n"," 98% 10941/11208 [1:54:42<10:13,  2.30s/it]\u001b[A\n"," 98% 10942/11208 [1:54:44<10:13,  2.31s/it]\u001b[A\n"," 98% 10943/11208 [1:54:47<10:14,  2.32s/it]\u001b[A\n"," 98% 10944/11208 [1:54:49<10:13,  2.32s/it]\u001b[A\n"," 98% 10945/11208 [1:54:51<10:12,  2.33s/it]\u001b[A\n"," 98% 10946/11208 [1:54:54<10:10,  2.33s/it]\u001b[A\n"," 98% 10947/11208 [1:54:56<10:06,  2.33s/it]\u001b[A\n"," 98% 10948/11208 [1:54:58<10:05,  2.33s/it]\u001b[A\n"," 98% 10949/11208 [1:55:01<10:03,  2.33s/it]\u001b[A\n"," 98% 10950/11208 [1:55:03<10:02,  2.34s/it]\u001b[A\n"," 98% 10951/11208 [1:55:05<09:54,  2.32s/it]\u001b[A\n"," 98% 10952/11208 [1:55:08<09:55,  2.32s/it]\u001b[A\n"," 98% 10953/11208 [1:55:10<09:53,  2.33s/it]\u001b[A\n"," 98% 10954/11208 [1:55:12<09:51,  2.33s/it]\u001b[A\n"," 98% 10955/11208 [1:55:15<09:49,  2.33s/it]\u001b[A\n"," 98% 10956/11208 [1:55:17<09:47,  2.33s/it]\u001b[A\n"," 98% 10957/11208 [1:55:19<09:43,  2.33s/it]\u001b[A\n"," 98% 10958/11208 [1:55:22<09:42,  2.33s/it]\u001b[A\n"," 98% 10959/11208 [1:55:24<09:39,  2.33s/it]\u001b[A\n"," 98% 10960/11208 [1:55:26<09:31,  2.30s/it]\u001b[A\n"," 98% 10961/11208 [1:55:29<09:31,  2.31s/it]\u001b[A\n"," 98% 10962/11208 [1:55:31<09:30,  2.32s/it]\u001b[A\n"," 98% 10963/11208 [1:55:33<09:29,  2.33s/it]\u001b[A\n"," 98% 10964/11208 [1:55:36<09:26,  2.32s/it]\u001b[A\n"," 98% 10965/11208 [1:55:38<09:25,  2.33s/it]\u001b[A\n"," 98% 10966/11208 [1:55:40<09:23,  2.33s/it]\u001b[A\n"," 98% 10967/11208 [1:55:43<09:21,  2.33s/it]\u001b[A\n"," 98% 10968/11208 [1:55:45<09:19,  2.33s/it]\u001b[A\n"," 98% 10969/11208 [1:55:47<09:18,  2.34s/it]\u001b[A\n"," 98% 10970/11208 [1:55:50<09:15,  2.33s/it]\u001b[A\n"," 98% 10971/11208 [1:55:52<09:13,  2.34s/it]\u001b[A\n"," 98% 10972/11208 [1:55:54<09:10,  2.33s/it]\u001b[A\n"," 98% 10973/11208 [1:55:57<09:08,  2.33s/it]\u001b[A\n"," 98% 10974/11208 [1:55:59<09:06,  2.34s/it]\u001b[A\n"," 98% 10975/11208 [1:56:01<09:04,  2.34s/it]\u001b[A\n"," 98% 10976/11208 [1:56:04<09:00,  2.33s/it]\u001b[A\n"," 98% 10977/11208 [1:56:06<08:54,  2.31s/it]\u001b[A\n"," 98% 10978/11208 [1:56:08<08:51,  2.31s/it]\u001b[A\n"," 98% 10979/11208 [1:56:11<08:50,  2.32s/it]\u001b[A\n"," 98% 10980/11208 [1:56:13<08:49,  2.32s/it]\u001b[A\n"," 98% 10981/11208 [1:56:15<08:48,  2.33s/it]\u001b[A\n"," 98% 10982/11208 [1:56:18<08:45,  2.32s/it]\u001b[A\n"," 98% 10983/11208 [1:56:20<08:43,  2.33s/it]\u001b[A\n"," 98% 10984/11208 [1:56:22<08:40,  2.32s/it]\u001b[A\n"," 98% 10985/11208 [1:56:25<08:38,  2.33s/it]\u001b[A\n"," 98% 10986/11208 [1:56:27<08:37,  2.33s/it]\u001b[A\n"," 98% 10987/11208 [1:56:29<08:34,  2.33s/it]\u001b[A\n"," 98% 10988/11208 [1:56:31<08:28,  2.31s/it]\u001b[A\n"," 98% 10989/11208 [1:56:34<08:26,  2.31s/it]\u001b[A\n"," 98% 10990/11208 [1:56:36<08:26,  2.32s/it]\u001b[A\n"," 98% 10991/11208 [1:56:38<08:25,  2.33s/it]\u001b[A\n"," 98% 10992/11208 [1:56:41<08:22,  2.33s/it]\u001b[A\n"," 98% 10993/11208 [1:56:43<08:19,  2.32s/it]\u001b[A\n"," 98% 10994/11208 [1:56:45<08:17,  2.32s/it]\u001b[A\n"," 98% 10995/11208 [1:56:48<08:15,  2.33s/it]\u001b[A\n"," 98% 10996/11208 [1:56:50<08:15,  2.33s/it]\u001b[A\n"," 98% 10997/11208 [1:56:52<08:14,  2.34s/it]\u001b[A\n"," 98% 10998/11208 [1:56:55<08:09,  2.33s/it]\u001b[A\n"," 98% 10999/11208 [1:56:57<08:08,  2.34s/it]\u001b[A\n"," 98% 11000/11208 [1:56:59<08:06,  2.34s/it]\u001b[A\n","\u001b[A{'loss': 1.6378, 'learning_rate': 9.279086366880799e-07, 'epoch': 2.94}\n","\n"," 98% 11000/11208 [1:57:00<08:06,  2.34s/it]\u001b[A[INFO|trainer.py:2700] 2022-12-14 22:34:23,072 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-11000\n","[INFO|configuration_utils.py:447] 2022-12-14 22:34:23,083 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-11000/config.json\n","[INFO|modeling_utils.py:1680] 2022-12-14 22:34:32,523 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-11000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-14 22:34:32,529 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-11000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-14 22:34:32,534 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-11000/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-14 22:34:32,949 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/checkpoint-11000/spiece.model\n","\n"," 98% 11001/11208 [1:57:32<39:32, 11.46s/it]\u001b[A\n"," 98% 11002/11208 [1:57:35<29:57,  8.72s/it]\u001b[A\n"," 98% 11003/11208 [1:57:37<23:15,  6.81s/it]\u001b[A\n"," 98% 11004/11208 [1:57:39<18:35,  5.47s/it]\u001b[A\n"," 98% 11005/11208 [1:57:42<15:18,  4.52s/it]\u001b[A\n"," 98% 11006/11208 [1:57:44<13:05,  3.89s/it]\u001b[A\n"," 98% 11007/11208 [1:57:46<11:31,  3.44s/it]\u001b[A\n"," 98% 11008/11208 [1:57:49<10:26,  3.13s/it]\u001b[A\n"," 98% 11009/11208 [1:57:51<09:42,  2.93s/it]\u001b[A\n"," 98% 11010/11208 [1:57:54<09:10,  2.78s/it]\u001b[A\n"," 98% 11011/11208 [1:57:56<08:48,  2.68s/it]\u001b[A\n"," 98% 11012/11208 [1:57:59<08:34,  2.63s/it]\u001b[A\n"," 98% 11013/11208 [1:58:01<08:23,  2.58s/it]\u001b[A\n"," 98% 11014/11208 [1:58:03<08:09,  2.52s/it]\u001b[A\n"," 98% 11015/11208 [1:58:06<08:02,  2.50s/it]\u001b[A\n"," 98% 11016/11208 [1:58:08<07:55,  2.48s/it]\u001b[A\n"," 98% 11017/11208 [1:58:11<07:49,  2.46s/it]\u001b[A\n"," 98% 11018/11208 [1:58:13<07:43,  2.44s/it]\u001b[A\n"," 98% 11019/11208 [1:58:16<07:38,  2.43s/it]\u001b[A\n"," 98% 11020/11208 [1:58:18<07:32,  2.41s/it]\u001b[A\n"," 98% 11021/11208 [1:58:20<07:27,  2.39s/it]\u001b[A\n"," 98% 11022/11208 [1:58:23<07:21,  2.38s/it]\u001b[A\n"," 98% 11023/11208 [1:58:25<07:17,  2.36s/it]\u001b[A\n"," 98% 11024/11208 [1:58:27<07:12,  2.35s/it]\u001b[A\n"," 98% 11025/11208 [1:58:30<07:08,  2.34s/it]\u001b[A\n"," 98% 11026/11208 [1:58:32<07:05,  2.34s/it]\u001b[A\n"," 98% 11027/11208 [1:58:34<07:01,  2.33s/it]\u001b[A\n"," 98% 11028/11208 [1:58:37<06:58,  2.33s/it]\u001b[A\n"," 98% 11029/11208 [1:58:39<06:55,  2.32s/it]\u001b[A\n"," 98% 11030/11208 [1:58:41<06:53,  2.32s/it]\u001b[A\n"," 98% 11031/11208 [1:58:44<06:51,  2.33s/it]\u001b[A\n"," 98% 11032/11208 [1:58:46<06:48,  2.32s/it]\u001b[A\n"," 98% 11033/11208 [1:58:48<06:47,  2.33s/it]\u001b[A\n"," 98% 11034/11208 [1:58:50<06:39,  2.30s/it]\u001b[A\n"," 98% 11035/11208 [1:58:53<06:33,  2.28s/it]\u001b[A\n"," 98% 11036/11208 [1:58:55<06:33,  2.29s/it]\u001b[A\n"," 98% 11037/11208 [1:58:57<06:33,  2.30s/it]\u001b[A\n"," 98% 11038/11208 [1:59:00<06:32,  2.31s/it]\u001b[A\n"," 98% 11039/11208 [1:59:02<06:31,  2.32s/it]\u001b[A\n"," 99% 11040/11208 [1:59:04<06:29,  2.32s/it]\u001b[A\n"," 99% 11041/11208 [1:59:07<06:30,  2.34s/it]\u001b[A\n"," 99% 11042/11208 [1:59:09<06:28,  2.34s/it]\u001b[A\n"," 99% 11043/11208 [1:59:11<06:26,  2.34s/it]\u001b[A\n"," 99% 11044/11208 [1:59:14<06:24,  2.35s/it]\u001b[A\n"," 99% 11045/11208 [1:59:16<06:18,  2.32s/it]\u001b[A\n"," 99% 11046/11208 [1:59:18<06:18,  2.34s/it]\u001b[A\n"," 99% 11047/11208 [1:59:21<06:17,  2.35s/it]\u001b[A\n"," 99% 11048/11208 [1:59:23<06:12,  2.33s/it]\u001b[A\n"," 99% 11049/11208 [1:59:25<06:11,  2.34s/it]\u001b[A\n"," 99% 11050/11208 [1:59:28<06:09,  2.34s/it]\u001b[A\n"," 99% 11051/11208 [1:59:30<06:07,  2.34s/it]\u001b[A\n"," 99% 11052/11208 [1:59:32<06:06,  2.35s/it]\u001b[A\n"," 99% 11053/11208 [1:59:35<06:00,  2.33s/it]\u001b[A\n"," 99% 11054/11208 [1:59:37<06:00,  2.34s/it]\u001b[A\n"," 99% 11055/11208 [1:59:39<05:58,  2.35s/it]\u001b[A\n"," 99% 11056/11208 [1:59:42<05:56,  2.35s/it]\u001b[A\n"," 99% 11057/11208 [1:59:44<05:54,  2.35s/it]\u001b[A\n"," 99% 11058/11208 [1:59:46<05:51,  2.34s/it]\u001b[A\n"," 99% 11059/11208 [1:59:49<05:48,  2.34s/it]\u001b[A\n"," 99% 11060/11208 [1:59:51<05:46,  2.34s/it]\u001b[A\n"," 99% 11061/11208 [1:59:53<05:44,  2.34s/it]\u001b[A\n"," 99% 11062/11208 [1:59:56<05:38,  2.32s/it]\u001b[A\n"," 99% 11063/11208 [1:59:58<05:37,  2.32s/it]\u001b[A\n"," 99% 11064/11208 [2:00:00<05:34,  2.33s/it]\u001b[A\n"," 99% 11065/11208 [2:00:03<05:29,  2.31s/it]\u001b[A\n"," 99% 11066/11208 [2:00:05<05:27,  2.31s/it]\u001b[A\n"," 99% 11067/11208 [2:00:07<05:25,  2.31s/it]\u001b[A\n"," 99% 11068/11208 [2:00:10<05:24,  2.31s/it]\u001b[A\n"," 99% 11069/11208 [2:00:12<05:20,  2.31s/it]\u001b[A\n"," 99% 11070/11208 [2:00:14<05:18,  2.31s/it]\u001b[A\n"," 99% 11071/11208 [2:00:17<05:17,  2.32s/it]\u001b[A\n"," 99% 11072/11208 [2:00:19<05:14,  2.31s/it]\u001b[A\n"," 99% 11073/11208 [2:00:21<05:12,  2.32s/it]\u001b[A\n"," 99% 11074/11208 [2:00:23<05:10,  2.32s/it]\u001b[A\n"," 99% 11075/11208 [2:00:26<05:06,  2.31s/it]\u001b[A\n"," 99% 11076/11208 [2:00:28<05:01,  2.29s/it]\u001b[A\n"," 99% 11077/11208 [2:00:30<05:00,  2.30s/it]\u001b[A\n"," 99% 11078/11208 [2:00:33<04:59,  2.31s/it]\u001b[A\n"," 99% 11079/11208 [2:00:35<04:58,  2.32s/it]\u001b[A\n"," 99% 11080/11208 [2:00:37<04:56,  2.32s/it]\u001b[A\n"," 99% 11081/11208 [2:00:40<04:55,  2.32s/it]\u001b[A\n"," 99% 11082/11208 [2:00:42<04:49,  2.30s/it]\u001b[A\n"," 99% 11083/11208 [2:00:44<04:49,  2.31s/it]\u001b[A\n"," 99% 11084/11208 [2:00:47<04:47,  2.32s/it]\u001b[A\n"," 99% 11085/11208 [2:00:49<04:45,  2.32s/it]\u001b[A\n"," 99% 11086/11208 [2:00:51<04:43,  2.33s/it]\u001b[A\n"," 99% 11087/11208 [2:00:54<04:42,  2.33s/it]\u001b[A\n"," 99% 11088/11208 [2:00:56<04:40,  2.33s/it]\u001b[A\n"," 99% 11089/11208 [2:00:58<04:37,  2.33s/it]\u001b[A\n"," 99% 11090/11208 [2:01:01<04:35,  2.33s/it]\u001b[A\n"," 99% 11091/11208 [2:01:03<04:32,  2.33s/it]\u001b[A\n"," 99% 11092/11208 [2:01:05<04:29,  2.32s/it]\u001b[A\n"," 99% 11093/11208 [2:01:08<04:27,  2.33s/it]\u001b[A\n"," 99% 11094/11208 [2:01:10<04:18,  2.26s/it]\u001b[A\n"," 99% 11095/11208 [2:01:12<04:17,  2.27s/it]\u001b[A\n"," 99% 11096/11208 [2:01:14<04:17,  2.30s/it]\u001b[A\n"," 99% 11097/11208 [2:01:17<04:15,  2.31s/it]\u001b[A\n"," 99% 11098/11208 [2:01:19<04:14,  2.32s/it]\u001b[A\n"," 99% 11099/11208 [2:01:21<04:12,  2.32s/it]\u001b[A\n"," 99% 11100/11208 [2:01:24<04:07,  2.29s/it]\u001b[A\n","\u001b[A{'loss': 1.6487, 'learning_rate': 4.817987152034262e-07, 'epoch': 2.97}\n","\n"," 99% 11100/11208 [2:01:24<04:07,  2.29s/it]\u001b[A\n"," 99% 11101/11208 [2:01:26<04:07,  2.31s/it]\u001b[A\n"," 99% 11102/11208 [2:01:28<04:05,  2.32s/it]\u001b[A\n"," 99% 11103/11208 [2:01:30<04:00,  2.29s/it]\u001b[A\n"," 99% 11104/11208 [2:01:33<03:59,  2.31s/it]\u001b[A\n"," 99% 11105/11208 [2:01:35<03:56,  2.29s/it]\u001b[A\n"," 99% 11106/11208 [2:01:37<03:55,  2.31s/it]\u001b[A\n"," 99% 11107/11208 [2:01:40<03:54,  2.32s/it]\u001b[A\n"," 99% 11108/11208 [2:01:42<03:47,  2.27s/it]\u001b[A\n"," 99% 11109/11208 [2:01:44<03:46,  2.29s/it]\u001b[A\n"," 99% 11110/11208 [2:01:47<03:45,  2.31s/it]\u001b[A\n"," 99% 11111/11208 [2:01:49<03:44,  2.32s/it]\u001b[A\n"," 99% 11112/11208 [2:01:51<03:42,  2.32s/it]\u001b[A\n"," 99% 11113/11208 [2:01:54<03:41,  2.33s/it]\u001b[A\n"," 99% 11114/11208 [2:01:56<03:39,  2.33s/it]\u001b[A\n"," 99% 11115/11208 [2:01:58<03:37,  2.33s/it]\u001b[A\n"," 99% 11116/11208 [2:02:01<03:34,  2.33s/it]\u001b[A\n"," 99% 11117/11208 [2:02:03<03:32,  2.33s/it]\u001b[A\n"," 99% 11118/11208 [2:02:05<03:30,  2.34s/it]\u001b[A\n"," 99% 11119/11208 [2:02:08<03:27,  2.34s/it]\u001b[A\n"," 99% 11120/11208 [2:02:10<03:25,  2.34s/it]\u001b[A\n"," 99% 11121/11208 [2:02:12<03:22,  2.33s/it]\u001b[A\n"," 99% 11122/11208 [2:02:15<03:19,  2.32s/it]\u001b[A\n"," 99% 11123/11208 [2:02:17<03:18,  2.33s/it]\u001b[A\n"," 99% 11124/11208 [2:02:19<03:16,  2.33s/it]\u001b[A\n"," 99% 11125/11208 [2:02:22<03:13,  2.33s/it]\u001b[A\n"," 99% 11126/11208 [2:02:24<03:11,  2.33s/it]\u001b[A\n"," 99% 11127/11208 [2:02:26<03:08,  2.32s/it]\u001b[A\n"," 99% 11128/11208 [2:02:29<03:06,  2.33s/it]\u001b[A\n"," 99% 11129/11208 [2:02:31<03:04,  2.33s/it]\u001b[A\n"," 99% 11130/11208 [2:02:33<03:02,  2.33s/it]\u001b[A\n"," 99% 11131/11208 [2:02:36<02:59,  2.34s/it]\u001b[A\n"," 99% 11132/11208 [2:02:38<02:57,  2.33s/it]\u001b[A\n"," 99% 11133/11208 [2:02:40<02:54,  2.33s/it]\u001b[A\n"," 99% 11134/11208 [2:02:42<02:50,  2.30s/it]\u001b[A\n"," 99% 11135/11208 [2:02:45<02:48,  2.31s/it]\u001b[A\n"," 99% 11136/11208 [2:02:47<02:47,  2.32s/it]\u001b[A\n"," 99% 11137/11208 [2:02:50<02:45,  2.33s/it]\u001b[A\n"," 99% 11138/11208 [2:02:52<02:43,  2.33s/it]\u001b[A\n"," 99% 11139/11208 [2:02:54<02:40,  2.33s/it]\u001b[A\n"," 99% 11140/11208 [2:02:57<02:38,  2.33s/it]\u001b[A\n"," 99% 11141/11208 [2:02:59<02:36,  2.34s/it]\u001b[A\n"," 99% 11142/11208 [2:03:01<02:32,  2.31s/it]\u001b[A\n"," 99% 11143/11208 [2:03:03<02:30,  2.31s/it]\u001b[A\n"," 99% 11144/11208 [2:03:06<02:28,  2.32s/it]\u001b[A\n"," 99% 11145/11208 [2:03:08<02:26,  2.33s/it]\u001b[A\n"," 99% 11146/11208 [2:03:10<02:24,  2.33s/it]\u001b[A\n"," 99% 11147/11208 [2:03:13<02:22,  2.33s/it]\u001b[A\n"," 99% 11148/11208 [2:03:15<02:20,  2.34s/it]\u001b[A\n"," 99% 11149/11208 [2:03:17<02:17,  2.34s/it]\u001b[A\n"," 99% 11150/11208 [2:03:20<02:15,  2.33s/it]\u001b[A\n"," 99% 11151/11208 [2:03:22<02:13,  2.33s/it]\u001b[A\n","100% 11152/11208 [2:03:24<02:10,  2.33s/it]\u001b[A\n","100% 11153/11208 [2:03:27<02:08,  2.34s/it]\u001b[A\n","100% 11154/11208 [2:03:29<02:05,  2.33s/it]\u001b[A\n","100% 11155/11208 [2:03:31<02:03,  2.33s/it]\u001b[A\n","100% 11156/11208 [2:03:34<02:01,  2.33s/it]\u001b[A\n","100% 11157/11208 [2:03:36<01:59,  2.33s/it]\u001b[A\n","100% 11158/11208 [2:03:38<01:56,  2.33s/it]\u001b[A\n","100% 11159/11208 [2:03:41<01:54,  2.33s/it]\u001b[A\n","100% 11160/11208 [2:03:43<01:52,  2.34s/it]\u001b[A\n","100% 11161/11208 [2:03:45<01:49,  2.34s/it]\u001b[A\n","100% 11162/11208 [2:03:48<01:47,  2.33s/it]\u001b[A\n","100% 11163/11208 [2:03:50<01:45,  2.33s/it]\u001b[A\n","100% 11164/11208 [2:03:52<01:42,  2.33s/it]\u001b[A\n","100% 11165/11208 [2:03:55<01:40,  2.33s/it]\u001b[A\n","100% 11166/11208 [2:03:57<01:36,  2.29s/it]\u001b[A\n","100% 11167/11208 [2:03:59<01:34,  2.31s/it]\u001b[A\n","100% 11168/11208 [2:04:02<01:32,  2.31s/it]\u001b[A\n","100% 11169/11208 [2:04:04<01:30,  2.32s/it]\u001b[A\n","100% 11170/11208 [2:04:06<01:28,  2.33s/it]\u001b[A\n","100% 11171/11208 [2:04:09<01:26,  2.33s/it]\u001b[A\n","100% 11172/11208 [2:04:11<01:23,  2.31s/it]\u001b[A\n","100% 11173/11208 [2:04:13<01:19,  2.26s/it]\u001b[A\n","100% 11174/11208 [2:04:15<01:17,  2.28s/it]\u001b[A\n","100% 11175/11208 [2:04:18<01:15,  2.30s/it]\u001b[A\n","100% 11176/11208 [2:04:20<01:13,  2.31s/it]\u001b[A\n","100% 11177/11208 [2:04:22<01:11,  2.32s/it]\u001b[A\n","100% 11178/11208 [2:04:25<01:09,  2.32s/it]\u001b[A\n","100% 11179/11208 [2:04:27<01:07,  2.33s/it]\u001b[A\n","100% 11180/11208 [2:04:29<01:05,  2.33s/it]\u001b[A\n","100% 11181/11208 [2:04:32<01:02,  2.33s/it]\u001b[A\n","100% 11182/11208 [2:04:34<00:59,  2.27s/it]\u001b[A\n","100% 11183/11208 [2:04:36<00:57,  2.29s/it]\u001b[A\n","100% 11184/11208 [2:04:39<00:55,  2.31s/it]\u001b[A\n","100% 11185/11208 [2:04:41<00:53,  2.32s/it]\u001b[A\n","100% 11186/11208 [2:04:43<00:51,  2.32s/it]\u001b[A\n","100% 11187/11208 [2:04:46<00:48,  2.33s/it]\u001b[A\n","100% 11188/11208 [2:04:48<00:46,  2.33s/it]\u001b[A\n","100% 11189/11208 [2:04:50<00:44,  2.33s/it]\u001b[A\n","100% 11190/11208 [2:04:53<00:42,  2.33s/it]\u001b[A\n","100% 11191/11208 [2:04:55<00:39,  2.34s/it]\u001b[A\n","100% 11192/11208 [2:04:57<00:37,  2.34s/it]\u001b[A\n","100% 11193/11208 [2:05:00<00:35,  2.34s/it]\u001b[A\n","100% 11194/11208 [2:05:02<00:32,  2.35s/it]\u001b[A\n","100% 11195/11208 [2:05:04<00:30,  2.31s/it]\u001b[A\n","100% 11196/11208 [2:05:07<00:27,  2.31s/it]\u001b[A\n","100% 11197/11208 [2:05:09<00:25,  2.32s/it]\u001b[A\n","100% 11198/11208 [2:05:11<00:23,  2.30s/it]\u001b[A\n","100% 11199/11208 [2:05:13<00:20,  2.31s/it]\u001b[A\n","100% 11200/11208 [2:05:16<00:18,  2.32s/it]\u001b[A\n","\u001b[A{'loss': 1.6791, 'learning_rate': 3.5688793718772306e-08, 'epoch': 3.0}\n","\n","100% 11200/11208 [2:05:16<00:18,  2.32s/it]\u001b[A\n","100% 11201/11208 [2:05:18<00:16,  2.31s/it]\u001b[A\n","100% 11202/11208 [2:05:20<00:13,  2.29s/it]\u001b[A\n","100% 11203/11208 [2:05:23<00:11,  2.30s/it]\u001b[A\n","100% 11204/11208 [2:05:25<00:09,  2.31s/it]\u001b[A\n","100% 11205/11208 [2:05:27<00:06,  2.32s/it]\u001b[A\n","100% 11206/11208 [2:05:30<00:04,  2.33s/it]\u001b[A\n","100% 11207/11208 [2:05:32<00:02,  2.33s/it]\u001b[A\n","100% 11208/11208 [2:05:34<00:00,  2.33s/it]\u001b[A[INFO|trainer.py:1892] 2022-12-14 22:42:57,872 >> \n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","\n","\u001b[A{'train_runtime': 7535.7815, 'train_samples_per_second': 11.9, 'train_steps_per_second': 1.487, 'train_loss': 0.4802762891972261, 'epoch': 3.0}\n","\n","100% 11208/11208 [2:05:34<00:00,  1.49it/s]\n","[INFO|trainer.py:2700] 2022-12-14 22:42:57,958 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3\n","[INFO|configuration_utils.py:447] 2022-12-14 22:42:57,967 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/config.json\n","[INFO|modeling_utils.py:1680] 2022-12-14 22:43:07,533 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-14 22:43:07,540 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-14 22:43:07,546 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/special_tokens_map.json\n","[INFO|tokenization_t5_fast.py:186] 2022-12-14 22:43:07,936 >> Copy vocab file to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/mT5-Base/epoch3/spiece.model\n","***** train metrics *****\n","  epoch                    =        3.0\n","  train_loss               =     0.4803\n","  train_runtime            = 2:05:35.78\n","  train_samples            =      29893\n","  train_samples_per_second =       11.9\n","  train_steps_per_second   =      1.487\n","INFO:__main__:*** Evaluate ***\n","[INFO|trainer.py:2955] 2022-12-14 22:43:08,062 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2957] 2022-12-14 22:43:08,062 >>   Num examples = 3737\n","[INFO|trainer.py:2960] 2022-12-14 22:43:08,062 >>   Batch size = 8\n","100% 468/468 [23:36<00:00,  3.03s/it]\n","***** eval metrics *****\n","  epoch                   =        3.0\n","  eval_gen_len            =    57.4094\n","  eval_loss               =     1.4744\n","  eval_rouge1             =    55.2368\n","  eval_rouge2             =    23.2094\n","  eval_rougeL             =    35.6606\n","  eval_rougeLsum          =    36.3718\n","  eval_runtime            = 0:23:39.45\n","  eval_samples            =       3737\n","  eval_samples_per_second =      2.633\n","  eval_steps_per_second   =       0.33\n","INFO:__main__:*** Predict ***\n","[INFO|trainer.py:2955] 2022-12-14 23:06:47,551 >> ***** Running Prediction *****\n","[INFO|trainer.py:2957] 2022-12-14 23:06:47,551 >>   Num examples = 3737\n","[INFO|trainer.py:2960] 2022-12-14 23:06:47,551 >>   Batch size = 8\n","100% 468/468 [23:13<00:00,  2.98s/it]\n","***** predict metrics *****\n","  predict_gen_len            =    57.2754\n","  predict_loss               =     1.5028\n","  predict_rouge1             =    54.8352\n","  predict_rouge2             =    22.7685\n","  predict_rougeL             =    35.3978\n","  predict_rougeLsum          =    36.1041\n","  predict_runtime            = 0:23:16.06\n","  predict_samples            =       3737\n","  predict_samples_per_second =      2.677\n","  predict_steps_per_second   =      0.335\n","[INFO|modelcard.py:449] 2022-12-14 23:30:03,815 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Summarization', 'type': 'summarization'}, 'metrics': [{'name': 'Rouge1', 'type': 'rouge', 'value': 55.2368}]}\n"]}]},{"cell_type":"markdown","metadata":{"id":"wjin48INF-fw"},"source":["# Train distilbart-xsum e3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KOuaLTj0JP8L","outputId":"5d6c4591-49c2-4255-830e-45cdf591e47f","executionInfo":{"status":"ok","timestamp":1670633425326,"user_tz":-420,"elapsed":34546,"user":{"displayName":"Duy Ngọc Cao","userId":"09742726668153199714"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=8,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=passive,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Distilbart-xsum_1/runs/Dec10_00-50-00_08e8c8787a0e,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Distilbart-xsum_1,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=4,\n","per_device_train_batch_size=4,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Distilbart-xsum_1,\n","save_on_each_node=False,\n","save_steps=1000,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","WARNING:datasets.builder:Using custom data configuration default-79d6ab05104a01e0\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n","INFO:datasets.builder:Overwrite dataset info from restored data version.\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","100% 3/3 [00:00<00:00, 484.52it/s]\n","[INFO|configuration_utils.py:654] 2022-12-10 00:50:01,307 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n","[INFO|configuration_utils.py:706] 2022-12-10 00:50:01,313 >> Model config BartConfig {\n","  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 3,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"eos_token_ids\": [\n","    2\n","  ],\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 62,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 11,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 6,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"save_step\": 58,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {},\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|configuration_utils.py:654] 2022-12-10 00:50:01,675 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n","[INFO|configuration_utils.py:706] 2022-12-10 00:50:01,675 >> Model config BartConfig {\n","  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 3,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"eos_token_ids\": [\n","    2\n","  ],\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 62,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 11,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 6,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"save_step\": 58,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {},\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 00:50:01,679 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/vocab.json\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 00:50:01,679 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/merges.txt\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 00:50:01,679 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 00:50:01,679 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 00:50:01,679 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1799] 2022-12-10 00:50:01,679 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/tokenizer_config.json\n","[INFO|configuration_utils.py:654] 2022-12-10 00:50:01,679 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n","[INFO|configuration_utils.py:706] 2022-12-10 00:50:01,680 >> Model config BartConfig {\n","  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 3,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"eos_token_ids\": [\n","    2\n","  ],\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 62,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 11,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 6,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"save_step\": 58,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {},\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|configuration_utils.py:654] 2022-12-10 00:50:01,797 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/config.json\n","[INFO|configuration_utils.py:706] 2022-12-10 00:50:01,799 >> Model config BartConfig {\n","  \"_name_or_path\": \"sshleifer/distilbart-xsum-12-3\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 3,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"eos_token_ids\": [\n","    2\n","  ],\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 62,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 11,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 6,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"save_step\": 58,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {},\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50264\n","}\n","\n","[INFO|modeling_utils.py:2238] 2022-12-10 00:50:01,929 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--sshleifer--distilbart-xsum-12-3/snapshots/1d2bfbc16dcdd28720f9f1d37be764e5cc5c78c8/pytorch_model.bin\n","[INFO|modeling_utils.py:2742] 2022-12-10 00:50:07,034 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:2750] 2022-12-10 00:50:07,034 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at sshleifer/distilbart-xsum-12-3.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-ee87464d52caa84e.arrow\n","Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-9c5f13fb77c8c805.arrow\n","Running tokenizer on validation dataset: 100% 4/4 [00:08<00:00,  2.00s/ba]\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-2b8cadbbb3976f3b.arrow\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1642] 2022-12-10 00:50:20,688 >> ***** Running training *****\n","[INFO|trainer.py:1643] 2022-12-10 00:50:20,688 >>   Num examples = 29893\n","[INFO|trainer.py:1644] 2022-12-10 00:50:20,688 >>   Num Epochs = 3\n","[INFO|trainer.py:1645] 2022-12-10 00:50:20,688 >>   Instantaneous batch size per device = 4\n","[INFO|trainer.py:1646] 2022-12-10 00:50:20,688 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n","[INFO|trainer.py:1647] 2022-12-10 00:50:20,688 >>   Gradient Accumulation steps = 8\n","[INFO|trainer.py:1648] 2022-12-10 00:50:20,689 >>   Total optimization steps = 2802\n","[INFO|trainer.py:1649] 2022-12-10 00:50:20,689 >>   Number of trainable parameters = 255121408\n","  0% 0/2802 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-12-10 00:50:20,737 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Traceback (most recent call last):\n","  File \"examples/pytorch/summarization/run_summarization.py\", line 736, in <module>\n","    main()\n","  File \"examples/pytorch/summarization/run_summarization.py\", line 655, in main\n","    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1535, in train\n","    return inner_training_loop(\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1783, in _inner_training_loop\n","    tr_loss_step = self.training_step(model, inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2531, in training_step\n","    loss = self.compute_loss(model, inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 2563, in compute_loss\n","    outputs = model(**inputs)\n","  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1190, in _call_impl\n","    return forward_call(*input, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/bart/modeling_bart.py\", line 1395, in forward\n","    lm_logits = lm_logits + self.final_logits_bias.to(lm_logits.device)\n","torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 14.76 GiB total capacity; 3.18 GiB already allocated; 18.75 MiB free; 3.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n","  0% 0/2802 [00:03<?, ?it/s]\n"]}],"source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"sshleifer/distilbart-xsum-12-3\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Distilbart-xsum_1\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size=4 \\\n","    --per_device_eval_batch_size=4 \\\n","    --gradient_accumulation_steps 8 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 1000 "]},{"cell_type":"markdown","metadata":{"id":"oxQASSDwGE3Y"},"source":["# Resume train distilbart-xsum when disconnected"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149266,"status":"ok","timestamp":1670512253223,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"},"user_tz":-420},"id":"XXluoSJPXUJ0","outputId":"7b1af16a-a170-4213-aa75-c5cfb8e0a57c"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=1,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=passive,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/runs/Dec08_15-08-30_49979243265e,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=500,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/,\n","overwrite_output_dir=False,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=8,\n","predict_with_generate=False,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","INFO:__main__:Checkpoint detected, resuming training at /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/checkpoint-4000. To avoid this behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\n","WARNING:datasets.builder:Using custom data configuration default-79d6ab05104a01e0\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n","INFO:datasets.builder:Overwrite dataset info from restored data version.\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","100% 3/3 [00:00<00:00, 565.57it/s]\n","[INFO|configuration_utils.py:652] 2022-12-08 15:08:33,170 >> loading configuration file /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/checkpoint-3000/config.json\n","[INFO|configuration_utils.py:706] 2022-12-08 15:08:33,174 >> Model config BartConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/\\u0110o\\u0302\\u0300 a\\u0301n DS310 - Xu\\u031b\\u0309 li\\u0301 ngo\\u0302n ngu\\u031b\\u0303 tu\\u031b\\u0323 nhie\\u0302n/Model/Pegasus-xsum/checkpoint-3000\",\n","  \"_num_labels\": 3,\n","  \"activation_dropout\": 0.0,\n","  \"activation_function\": \"gelu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": false,\n","  \"architectures\": [\n","    \"BartForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 3,\n","  \"decoder_start_token_id\": 2,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 12,\n","  \"eos_token_id\": 2,\n","  \"eos_token_ids\": [\n","    2\n","  ],\n","  \"extra_pos_embeddings\": 2,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_length\": 62,\n","  \"max_position_embeddings\": 1024,\n","  \"min_length\": 11,\n","  \"model_type\": \"bart\",\n","  \"no_repeat_ngram_size\": 3,\n","  \"normalize_before\": false,\n","  \"normalize_embedding\": true,\n","  \"num_beams\": 6,\n","  \"num_hidden_layers\": 12,\n","  \"output_past\": true,\n","  \"pad_token_id\": 1,\n","  \"prefix\": \" \",\n","  \"save_step\": 58,\n","  \"scale_embedding\": false,\n","  \"static_position_embeddings\": false,\n","  \"task_specific_params\": {},\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","[INFO|tokenization_utils_base.py:1797] 2022-12-08 15:08:33,185 >> loading file vocab.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-08 15:08:33,185 >> loading file merges.txt\n","[INFO|tokenization_utils_base.py:1797] 2022-12-08 15:08:33,185 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-08 15:08:33,185 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-08 15:08:33,185 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-08 15:08:33,185 >> loading file tokenizer_config.json\n","[INFO|modeling_utils.py:2225] 2022-12-08 15:08:33,270 >> loading weights file /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/checkpoint-3000/pytorch_model.bin\n","[INFO|modeling_utils.py:2732] 2022-12-08 15:08:39,015 >> All model checkpoint weights were used when initializing BartForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:2740] 2022-12-08 15:08:39,015 >> All the weights of BartForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/checkpoint-3000.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BartForConditionalGeneration for predictions without further training.\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-08a9fc8726272a63.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-00fdbc8f2e3030f4.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/csv/default-79d6ab05104a01e0/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-c74d016d64d855cd.arrow\n","[INFO|trainer.py:1965] 2022-12-08 15:08:43,341 >> Loading model from /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/checkpoint-4000.\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1643] 2022-12-08 15:08:52,860 >> ***** Running training *****\n","[INFO|trainer.py:1644] 2022-12-08 15:08:52,860 >>   Num examples = 29893\n","[INFO|trainer.py:1645] 2022-12-08 15:08:52,860 >>   Num Epochs = 3\n","[INFO|trainer.py:1646] 2022-12-08 15:08:52,861 >>   Instantaneous batch size per device = 8\n","[INFO|trainer.py:1647] 2022-12-08 15:08:52,861 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n","[INFO|trainer.py:1648] 2022-12-08 15:08:52,861 >>   Gradient Accumulation steps = 1\n","[INFO|trainer.py:1649] 2022-12-08 15:08:52,861 >>   Total optimization steps = 11211\n","[INFO|trainer.py:1650] 2022-12-08 15:08:52,862 >>   Number of trainable parameters = 255121408\n","[INFO|trainer.py:1672] 2022-12-08 15:08:52,864 >>   Continuing training from checkpoint, will skip to saved global_step\n","[INFO|trainer.py:1673] 2022-12-08 15:08:52,864 >>   Continuing training from epoch 1\n","[INFO|trainer.py:1674] 2022-12-08 15:08:52,864 >>   Continuing training from global step 4000\n","[INFO|trainer.py:1676] 2022-12-08 15:08:52,864 >>   Will skip the first 1 epochs then the first 263 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n","Skipping the first batches:   0% 0/263 [00:00<?, ?it/s]\n","  0% 0/11211 [00:00<?, ?it/s]\u001b[A[WARNING|logging.py:281] 2022-12-08 15:08:52,950 >> You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","Skipping the first batches: 100% 263/263 [00:00<00:00, 285.84it/s]\n","\n"," 36% 4001/11211 [00:03<00:05, 1331.76it/s]\u001b[A\n"," 36% 4023/11211 [00:20<00:05, 1331.76it/s]\u001b[A\n"," 36% 4024/11211 [00:20<00:49, 146.21it/s] \u001b[A\n"," 36% 4025/11211 [00:20<00:51, 138.62it/s]\u001b[A\n"," 36% 4049/11211 [00:40<00:51, 138.62it/s]\u001b[A\n"," 36% 4050/11211 [00:40<02:28, 48.13it/s] \u001b[A\n"," 36% 4051/11211 [00:41<02:34, 46.41it/s]\u001b[A\n"," 36% 4075/11211 [01:00<02:33, 46.41it/s]\u001b[A\n"," 36% 4076/11211 [01:00<05:39, 21.00it/s]\u001b[A\n"," 36% 4077/11211 [01:00<05:49, 20.40it/s]\u001b[A\n"," 37% 4096/11211 [01:15<10:03, 11.79it/s]\u001b[A\n"," 37% 4097/11211 [01:15<10:22, 11.43it/s]\u001b[A\n"," 37% 4109/11211 [01:25<15:15,  7.76it/s]\u001b[A\n"," 37% 4117/11211 [01:30<18:59,  6.22it/s]\u001b[A\n"," 37% 4123/11211 [01:35<22:54,  5.16it/s]\u001b[A\n"," 37% 4127/11211 [01:38<26:18,  4.49it/s]\u001b[A\n"," 37% 4130/11211 [01:40<29:30,  4.00it/s]\u001b[A\n"," 37% 4132/11211 [01:41<32:10,  3.67it/s]\u001b[A\n"," 37% 4134/11211 [01:43<35:39,  3.31it/s]\u001b[A\n"," 37% 4135/11211 [01:44<38:07,  3.09it/s]\u001b[A\n"," 37% 4136/11211 [01:45<41:15,  2.86it/s]\u001b[A\n"," 37% 4137/11211 [01:45<43:36,  2.70it/s]\u001b[A\n"," 37% 4138/11211 [01:46<48:24,  2.44it/s]\u001b[A\n"," 37% 4139/11211 [01:47<53:50,  2.19it/s]\u001b[A\n"," 37% 4140/11211 [01:48<59:42,  1.97it/s]\u001b[A\n"," 37% 4141/11211 [01:48<1:05:39,  1.79it/s]\u001b[A\n"," 37% 4142/11211 [01:49<1:09:19,  1.70it/s]\u001b[A\n"," 37% 4143/11211 [01:50<1:14:35,  1.58it/s]\u001b[A\n"," 37% 4144/11211 [01:51<1:17:01,  1.53it/s]\u001b[A\n"," 37% 4145/11211 [01:51<1:18:37,  1.50it/s]\u001b[A\n"," 37% 4146/11211 [01:52<1:20:58,  1.45it/s]\u001b[A\n"," 37% 4147/11211 [01:53<1:24:11,  1.40it/s]\u001b[A\n"," 37% 4148/11211 [01:53<1:23:33,  1.41it/s]\u001b[A\n"," 37% 4149/11211 [01:54<1:26:21,  1.36it/s]\u001b[A\n"," 37% 4150/11211 [01:55<1:28:23,  1.33it/s]\u001b[A\n"," 37% 4151/11211 [01:56<1:29:57,  1.31it/s]\u001b[A\n"," 37% 4152/11211 [01:57<1:31:01,  1.29it/s]\u001b[A\n"," 37% 4153/11211 [01:57<1:29:14,  1.32it/s]\u001b[A\n"," 37% 4154/11211 [01:58<1:30:22,  1.30it/s]\u001b[A\n"," 37% 4154/11211 [01:59<03:23, 34.73it/s]  \n","^C\n"]}],"source":["# Cách đơn giản để train tiếp là dẫn output_dir vào file chứa checkpoint (Vào drive đồ án nhóm để biết t sắp xếp thế nào trong đấy)\n","# Và bỏ câu lệnh overwrite_output_dir để train tiếp (code ở trên có câu lệnh này nhưng code này thì không có)\n","!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/checkpoint-3000\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Pegasus-xsum/\" \\\n","    --per_device_train_batch_size=8 \\\n","    --per_device_eval_batch_size=8 \\\n","    #--gradient_accumulation_steps 4 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 1000 "]},{"cell_type":"markdown","source":["# Train Pegasus-Large"],"metadata":{"id":"ApuJcZ95kFGT"}},{"cell_type":"code","source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"google/pegasus-large\" \\\n","    --do_train \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size 2 \\\n","    --per_device_eval_batch_size 8 \\\n","    --gradient_accumulation_steps 8 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 500 \\\n","    --lang \"vi_VN\" \\\n","    --max_source_length 700 \\\n","    --max_target_length 64"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wHj0vCEbkR_6","outputId":"cc0e2f57-831d-4136-96a5-c04c7deeedea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=True,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=8,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=passive,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/runs/Dec12_13-51-59_4f3ff1b8fe19,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=2,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","WARNING:datasets.builder:Using custom data configuration default-1d0dc81c08029efb\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n","INFO:datasets.builder:Overwrite dataset info from restored data version.\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","WARNING:datasets.builder:Found cached dataset csv (/root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317\n","\r  0% 0/3 [00:00<?, ?it/s]\r100% 3/3 [00:00<00:00, 625.74it/s]\n","[INFO|configuration_utils.py:654] 2022-12-12 13:51:59,792 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--pegasus-large/snapshots/51b039cd8c644561432f7bfbe75e65f720b38f66/config.json\n","[INFO|configuration_utils.py:706] 2022-12-12 13:51:59,793 >> Model config PegasusConfig {\n","  \"_name_or_path\": \"google/pegasus-large\",\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"relu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"PegasusForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 16,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 16,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 1,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 0.8,\n","  \"max_length\": 256,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"pegasus\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": false,\n","  \"num_beams\": 8,\n","  \"num_hidden_layers\": 16,\n","  \"pad_token_id\": 0,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": true,\n","  \"task_specific_params\": {\n","    \"summarization_aeslc\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_arxiv\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_big_patent\": {\n","      \"length_penalty\": 0.7,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_billsum\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_cnn_dailymail\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_gigaword\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 128\n","    },\n","    \"summarization_large\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_multi_news\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_newsroom\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_pubmed\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_reddit_tifu\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_wikihow\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 64,\n","      \"max_position_embeddings\": 512\n","    }\n","  },\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 96103\n","}\n","\n","[INFO|configuration_utils.py:654] 2022-12-12 13:51:59,815 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--pegasus-large/snapshots/51b039cd8c644561432f7bfbe75e65f720b38f66/config.json\n","[INFO|configuration_utils.py:706] 2022-12-12 13:51:59,816 >> Model config PegasusConfig {\n","  \"_name_or_path\": \"google/pegasus-large\",\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"relu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"PegasusForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 16,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 16,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 1,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 0.8,\n","  \"max_length\": 256,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"pegasus\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": false,\n","  \"num_beams\": 8,\n","  \"num_hidden_layers\": 16,\n","  \"pad_token_id\": 0,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": true,\n","  \"task_specific_params\": {\n","    \"summarization_aeslc\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_arxiv\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_big_patent\": {\n","      \"length_penalty\": 0.7,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_billsum\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_cnn_dailymail\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_gigaword\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 128\n","    },\n","    \"summarization_large\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_multi_news\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_newsroom\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_pubmed\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_reddit_tifu\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_wikihow\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 64,\n","      \"max_position_embeddings\": 512\n","    }\n","  },\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 96103\n","}\n","\n","[INFO|tokenization_utils_base.py:1799] 2022-12-12 13:51:59,818 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--pegasus-large/snapshots/51b039cd8c644561432f7bfbe75e65f720b38f66/spiece.model\n","[INFO|tokenization_utils_base.py:1799] 2022-12-12 13:51:59,818 >> loading file tokenizer.json from cache at None\n","[INFO|tokenization_utils_base.py:1799] 2022-12-12 13:51:59,818 >> loading file added_tokens.json from cache at None\n","[INFO|tokenization_utils_base.py:1799] 2022-12-12 13:51:59,818 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--pegasus-large/snapshots/51b039cd8c644561432f7bfbe75e65f720b38f66/special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1799] 2022-12-12 13:51:59,818 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--pegasus-large/snapshots/51b039cd8c644561432f7bfbe75e65f720b38f66/tokenizer_config.json\n","[INFO|configuration_utils.py:654] 2022-12-12 13:51:59,819 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--pegasus-large/snapshots/51b039cd8c644561432f7bfbe75e65f720b38f66/config.json\n","[INFO|configuration_utils.py:706] 2022-12-12 13:51:59,820 >> Model config PegasusConfig {\n","  \"_name_or_path\": \"google/pegasus-large\",\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"relu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"PegasusForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 16,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 16,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 1,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 0.8,\n","  \"max_length\": 256,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"pegasus\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": false,\n","  \"num_beams\": 8,\n","  \"num_hidden_layers\": 16,\n","  \"pad_token_id\": 0,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": true,\n","  \"task_specific_params\": {\n","    \"summarization_aeslc\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_arxiv\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_big_patent\": {\n","      \"length_penalty\": 0.7,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_billsum\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_cnn_dailymail\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_gigaword\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 128\n","    },\n","    \"summarization_large\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_multi_news\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_newsroom\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_pubmed\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_reddit_tifu\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_wikihow\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 64,\n","      \"max_position_embeddings\": 512\n","    }\n","  },\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 96103\n","}\n","\n","[INFO|configuration_utils.py:654] 2022-12-12 13:51:59,972 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--pegasus-large/snapshots/51b039cd8c644561432f7bfbe75e65f720b38f66/config.json\n","[INFO|configuration_utils.py:706] 2022-12-12 13:51:59,973 >> Model config PegasusConfig {\n","  \"_name_or_path\": \"google/pegasus-large\",\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"relu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"PegasusForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 16,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 16,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 1,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 0.8,\n","  \"max_length\": 256,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"pegasus\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": false,\n","  \"num_beams\": 8,\n","  \"num_hidden_layers\": 16,\n","  \"pad_token_id\": 0,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": true,\n","  \"task_specific_params\": {\n","    \"summarization_aeslc\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_arxiv\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_big_patent\": {\n","      \"length_penalty\": 0.7,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_billsum\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_cnn_dailymail\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_gigaword\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 128\n","    },\n","    \"summarization_large\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_multi_news\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_newsroom\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_pubmed\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_reddit_tifu\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_wikihow\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 64,\n","      \"max_position_embeddings\": 512\n","    }\n","  },\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 96103\n","}\n","\n","[INFO|modeling_utils.py:2238] 2022-12-12 13:52:00,399 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--pegasus-large/snapshots/51b039cd8c644561432f7bfbe75e65f720b38f66/pytorch_model.bin\n","[INFO|modeling_utils.py:2742] 2022-12-12 13:52:12,568 >> All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:2750] 2022-12-12 13:52:12,569 >> All the weights of PegasusForConditionalGeneration were initialized from the model checkpoint at google/pegasus-large.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PegasusForConditionalGeneration for predictions without further training.\n","Running tokenizer on train dataset:   0% 0/30 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-60b5cdc1fc333b8f.arrow\n","Running tokenizer on train dataset: 100% 30/30 [00:56<00:00,  1.88s/ba]\n","Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-a01853f98fcedf90.arrow\n","Running tokenizer on validation dataset: 100% 4/4 [00:06<00:00,  1.71s/ba]\n","Running tokenizer on prediction dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-d49cc5894a342642.arrow\n","Running tokenizer on prediction dataset: 100% 4/4 [00:06<00:00,  1.65s/ba]\n","/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","[INFO|trainer.py:1642] 2022-12-12 13:53:26,713 >> ***** Running training *****\n","[INFO|trainer.py:1643] 2022-12-12 13:53:26,713 >>   Num examples = 29893\n","[INFO|trainer.py:1644] 2022-12-12 13:53:26,713 >>   Num Epochs = 3\n","[INFO|trainer.py:1645] 2022-12-12 13:53:26,713 >>   Instantaneous batch size per device = 2\n","[INFO|trainer.py:1646] 2022-12-12 13:53:26,713 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n","[INFO|trainer.py:1647] 2022-12-12 13:53:26,713 >>   Gradient Accumulation steps = 8\n","[INFO|trainer.py:1648] 2022-12-12 13:53:26,713 >>   Total optimization steps = 5604\n","[INFO|trainer.py:1649] 2022-12-12 13:53:26,714 >>   Number of trainable parameters = 568699904\n","  0% 0/5604 [00:00<?, ?it/s][WARNING|logging.py:281] 2022-12-12 13:53:26,746 >> You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","{'loss': 2.0252, 'learning_rate': 4.9107780157030696e-05, 'epoch': 0.05}\n","{'loss': 1.8902, 'learning_rate': 4.821556031406139e-05, 'epoch': 0.11}\n","{'loss': 1.8293, 'learning_rate': 4.732334047109208e-05, 'epoch': 0.16}\n","{'loss': 1.8145, 'learning_rate': 4.643112062812277e-05, 'epoch': 0.21}\n","{'loss': 1.7923, 'learning_rate': 4.553890078515346e-05, 'epoch': 0.27}\n","  9% 500/5604 [57:23<9:46:52,  6.90s/it][INFO|trainer.py:2701] 2022-12-12 14:50:50,269 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-500\n","[INFO|configuration_utils.py:447] 2022-12-12 14:50:50,277 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-500/config.json\n","[INFO|modeling_utils.py:1671] 2022-12-12 14:50:59,597 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-12 14:50:59,613 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-12 14:50:59,624 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-500/special_tokens_map.json\n","{'loss': 1.7521, 'learning_rate': 4.4646680942184155e-05, 'epoch': 0.32}\n","{'loss': 1.722, 'learning_rate': 4.375446109921485e-05, 'epoch': 0.37}\n","{'loss': 1.736, 'learning_rate': 4.286224125624554e-05, 'epoch': 0.43}\n","{'loss': 1.7224, 'learning_rate': 4.1970021413276235e-05, 'epoch': 0.48}\n","{'loss': 1.7058, 'learning_rate': 4.107780157030692e-05, 'epoch': 0.54}\n"," 18% 1000/5604 [1:55:28<8:48:16,  6.88s/it][INFO|trainer.py:2701] 2022-12-12 15:48:55,720 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1000\n","[INFO|configuration_utils.py:447] 2022-12-12 15:48:55,730 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1000/config.json\n","[INFO|modeling_utils.py:1671] 2022-12-12 15:49:04,787 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1000/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-12 15:49:04,793 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1000/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-12 15:49:04,798 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1000/special_tokens_map.json\n","{'loss': 1.6932, 'learning_rate': 4.0185581727337615e-05, 'epoch': 0.59}\n","{'loss': 1.7079, 'learning_rate': 3.9293361884368315e-05, 'epoch': 0.64}\n","{'loss': 1.6756, 'learning_rate': 3.8401142041399e-05, 'epoch': 0.7}\n","{'loss': 1.6428, 'learning_rate': 3.7508922198429695e-05, 'epoch': 0.75}\n","{'loss': 1.6622, 'learning_rate': 3.661670235546039e-05, 'epoch': 0.8}\n"," 27% 1500/5604 [2:53:28<7:51:45,  6.90s/it][INFO|trainer.py:2701] 2022-12-12 16:46:55,446 >> Saving model checkpoint to /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1500\n","[INFO|configuration_utils.py:447] 2022-12-12 16:46:55,455 >> Configuration saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1500/config.json\n","[INFO|modeling_utils.py:1671] 2022-12-12 16:47:04,508 >> Model weights saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1500/pytorch_model.bin\n","[INFO|tokenization_utils_base.py:2157] 2022-12-12 16:47:04,515 >> tokenizer config file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1500/tokenizer_config.json\n","[INFO|tokenization_utils_base.py:2164] 2022-12-12 16:47:04,520 >> Special tokens file saved in /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/checkpoint-1500/special_tokens_map.json\n","{'loss': 1.6509, 'learning_rate': 3.572448251249108e-05, 'epoch': 0.86}\n","{'loss': 1.6359, 'learning_rate': 3.483226266952177e-05, 'epoch': 0.91}\n","{'loss': 1.6482, 'learning_rate': 3.394004282655247e-05, 'epoch': 0.96}\n"," 32% 1807/5604 [3:29:17<7:16:32,  6.90s/it]"]}]},{"cell_type":"markdown","source":["# Eval Pegasus-Large"],"metadata":{"id":"WMXFsTtJUCcm"}},{"cell_type":"code","source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large\" \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size 2 \\\n","    --per_device_eval_batch_size 8 \\\n","    --gradient_accumulation_steps 8 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 500 \\\n","    --lang \"vi_VN\" \\\n","    --max_source_length 700 \\\n","    --max_target_length 64"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TdKxC8DJUFZl","outputId":"e0e7b5a6-5feb-4068-90c3-bdbcda7aebac","executionInfo":{"status":"ok","timestamp":1670958097806,"user_tz":-420,"elapsed":5811206,"user":{"displayName":"Duy Ngọc ADN","userId":"08142011782414942112"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=8,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=passive,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/runs/Dec13_17-24-52_98fb6ff529ff,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=8,\n","per_device_train_batch_size=2,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","WARNING:datasets.builder:Using custom data configuration default-1d0dc81c08029efb\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n","INFO:datasets.builder:Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n","Downloading data files: 100% 3/3 [00:00<00:00, 1740.38it/s]\n","INFO:datasets.download.download_manager:Downloading took 0.0 min\n","INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n","INFO:datasets.utils.py_utils:Spawning 3 processes for 3 objects in slices of [1, 1, 1]\n","Extracting data files #0:   0% 0/1 [00:00<?, ?obj/s]\n","\n","Extracting data files #2:   0% 0/1 [00:00<?, ?obj/s]\u001b[A\u001b[A\n","Extracting data files #2: 100% 1/1 [00:00<00:00, 44.79obj/s]\n","Extracting data files #1: 100% 1/1 [00:00<00:00, 43.71obj/s]\n","Extracting data files #0: 100% 1/1 [00:00<00:00, 24.92obj/s]\n","INFO:datasets.utils.py_utils:Finished 3 processes\n","INFO:datasets.utils.py_utils:Unpacked 3 objects\n","INFO:datasets.utils.info_utils:Unable to verify checksums.\n","INFO:datasets.builder:Generating train split\n","INFO:datasets.builder:Generating validation split\n","INFO:datasets.builder:Generating test split\n","INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n","Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n","100% 3/3 [00:00<00:00, 458.48it/s]\n","[INFO|configuration_utils.py:652] 2022-12-13 17:25:07,310 >> loading configuration file /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/config.json\n","[INFO|configuration_utils.py:706] 2022-12-13 17:25:07,312 >> Model config PegasusConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/\\u0110o\\u0302\\u0300 a\\u0301n DS310 - Xu\\u031b\\u0309 li\\u0301 ngo\\u0302n ngu\\u031b\\u0303 tu\\u031b\\u0323 nhie\\u0302n/Model/pegasus-large\",\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"relu\",\n","  \"add_bias_logits\": false,\n","  \"add_final_layer_norm\": true,\n","  \"architectures\": [\n","    \"PegasusForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classif_dropout\": 0.0,\n","  \"classifier_dropout\": 0.0,\n","  \"d_model\": 1024,\n","  \"decoder_attention_heads\": 16,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_layers\": 16,\n","  \"decoder_start_token_id\": 0,\n","  \"dropout\": 0.1,\n","  \"encoder_attention_heads\": 16,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_layers\": 16,\n","  \"eos_token_id\": 1,\n","  \"extra_pos_embeddings\": 1,\n","  \"force_bos_token_to_be_generated\": false,\n","  \"forced_eos_token_id\": 1,\n","  \"gradient_checkpointing\": false,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 0.8,\n","  \"max_length\": 256,\n","  \"max_position_embeddings\": 1024,\n","  \"model_type\": \"pegasus\",\n","  \"normalize_before\": true,\n","  \"normalize_embedding\": false,\n","  \"num_beams\": 8,\n","  \"num_hidden_layers\": 16,\n","  \"pad_token_id\": 0,\n","  \"scale_embedding\": true,\n","  \"static_position_embeddings\": true,\n","  \"task_specific_params\": {\n","    \"summarization_aeslc\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_arxiv\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_big_patent\": {\n","      \"length_penalty\": 0.7,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_billsum\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_cnn_dailymail\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_gigaword\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 32,\n","      \"max_position_embeddings\": 128\n","    },\n","    \"summarization_large\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_multi_news\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_newsroom\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_pubmed\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 1024\n","    },\n","    \"summarization_reddit_tifu\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 128,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_wikihow\": {\n","      \"length_penalty\": 0.6,\n","      \"max_length\": 256,\n","      \"max_position_embeddings\": 512\n","    },\n","    \"summarization_xsum\": {\n","      \"length_penalty\": 0.8,\n","      \"max_length\": 64,\n","      \"max_position_embeddings\": 512\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 96103\n","}\n","\n","[INFO|tokenization_utils_base.py:1797] 2022-12-13 17:25:07,896 >> loading file spiece.model\n","[INFO|tokenization_utils_base.py:1797] 2022-12-13 17:25:07,896 >> loading file tokenizer.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-13 17:25:07,896 >> loading file added_tokens.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-13 17:25:07,896 >> loading file special_tokens_map.json\n","[INFO|tokenization_utils_base.py:1797] 2022-12-13 17:25:07,896 >> loading file tokenizer_config.json\n","[INFO|modeling_utils.py:2248] 2022-12-13 17:25:09,632 >> loading weights file /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large/pytorch_model.bin\n","[INFO|modeling_utils.py:2806] 2022-12-13 17:25:43,676 >> All model checkpoint weights were used when initializing PegasusForConditionalGeneration.\n","\n","[INFO|modeling_utils.py:2814] 2022-12-13 17:25:43,677 >> All the weights of PegasusForConditionalGeneration were initialized from the model checkpoint at /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/pegasus-large.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use PegasusForConditionalGeneration for predictions without further training.\n","Running tokenizer on validation dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-91448645d528e321.arrow\n","Running tokenizer on validation dataset: 100% 4/4 [00:06<00:00,  1.66s/ba]\n","Running tokenizer on prediction dataset:   0% 0/4 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-6d0f3bffc4a9b5e8.arrow\n","Running tokenizer on prediction dataset: 100% 4/4 [00:06<00:00,  1.64s/ba]\n","Downloading builder script: 100% 6.27k/6.27k [00:00<00:00, 5.29MB/s]\n","INFO:__main__:*** Evaluate ***\n","[INFO|trainer.py:2955] 2022-12-13 17:26:10,198 >> ***** Running Evaluation *****\n","[INFO|trainer.py:2957] 2022-12-13 17:26:10,198 >>   Num examples = 3737\n","[INFO|trainer.py:2960] 2022-12-13 17:26:10,198 >>   Batch size = 8\n","[WARNING|logging.py:281] 2022-12-13 17:26:10,215 >> You're using a PegasusTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","100% 468/468 [47:35<00:00,  6.10s/it]\n","***** eval metrics *****\n","  eval_gen_len            =    62.7225\n","  eval_loss               =     1.4469\n","  eval_rouge1             =    26.1054\n","  eval_rouge2             =    11.2514\n","  eval_rougeL             =    20.5374\n","  eval_rougeLsum          =    20.6808\n","  eval_runtime            = 0:47:48.71\n","  eval_samples            =       3737\n","  eval_samples_per_second =      1.303\n","  eval_steps_per_second   =      0.163\n","INFO:__main__:*** Predict ***\n","[INFO|trainer.py:2955] 2022-12-13 18:13:59,497 >> ***** Running Prediction *****\n","[INFO|trainer.py:2957] 2022-12-13 18:13:59,497 >>   Num examples = 3737\n","[INFO|trainer.py:2960] 2022-12-13 18:13:59,497 >>   Batch size = 8\n","100% 468/468 [47:28<00:00,  6.09s/it]\n","***** predict metrics *****\n","  predict_gen_len            =     62.711\n","  predict_loss               =     1.4736\n","  predict_rouge1             =    25.6606\n","  predict_rouge2             =    10.9794\n","  predict_rougeL             =    20.1206\n","  predict_rougeLsum          =    20.2448\n","  predict_runtime            = 0:47:34.85\n","  predict_samples            =       3737\n","  predict_samples_per_second =      1.309\n","  predict_steps_per_second   =      0.164\n","[INFO|modelcard.py:449] 2022-12-13 19:01:34,451 >> Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Summarization', 'type': 'summarization'}}\n"]}]},{"cell_type":"markdown","source":["# Eval Prophetnet"],"metadata":{"id":"pgfx1sAqj7p-"}},{"cell_type":"code","source":["!python examples/pytorch/summarization/run_summarization.py \\\n","    --model_name_or_path \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large/checkpoint-11210\" \\\n","    --do_eval \\\n","    --do_predict \\\n","    --train_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/train.csv\" \\\n","    --validation_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/val.csv\" \\\n","    --test_file \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Data/Data4T5/test.csv\" \\\n","    --source_prefix \"summarize: \" \\\n","    --output_dir \"/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large\" \\\n","    --overwrite_output_dir \\\n","    --per_device_train_batch_size=16 \\\n","    --per_device_eval_batch_size=16 \\\n","    --gradient_accumulation_steps 16 \\\n","    --predict_with_generate \\\n","    --logging_steps 100 \\\n","    --num_train_epochs 3 \\\n","    --save_steps 500 \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1i7wO-dbkB4c","executionInfo":{"status":"ok","timestamp":1671003443103,"user_tz":-420,"elapsed":21314,"user":{"displayName":"Duy Ngọc Cao","userId":"06440551053090445393"}},"outputId":"75650885-f599-4c12-d5cf-adcafbc336e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n","INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n","_n_gpu=1,\n","adafactor=False,\n","adam_beta1=0.9,\n","adam_beta2=0.999,\n","adam_epsilon=1e-08,\n","auto_find_batch_size=False,\n","bf16=False,\n","bf16_full_eval=False,\n","data_seed=None,\n","dataloader_drop_last=False,\n","dataloader_num_workers=0,\n","dataloader_pin_memory=True,\n","ddp_bucket_cap_mb=None,\n","ddp_find_unused_parameters=None,\n","ddp_timeout=1800,\n","debug=[],\n","deepspeed=None,\n","disable_tqdm=False,\n","do_eval=True,\n","do_predict=True,\n","do_train=False,\n","eval_accumulation_steps=None,\n","eval_delay=0,\n","eval_steps=None,\n","evaluation_strategy=no,\n","fp16=False,\n","fp16_backend=auto,\n","fp16_full_eval=False,\n","fp16_opt_level=O1,\n","fsdp=[],\n","fsdp_min_num_params=0,\n","fsdp_transformer_layer_cls_to_wrap=None,\n","full_determinism=False,\n","generation_max_length=None,\n","generation_num_beams=None,\n","gradient_accumulation_steps=16,\n","gradient_checkpointing=False,\n","greater_is_better=None,\n","group_by_length=False,\n","half_precision_backend=auto,\n","hub_model_id=None,\n","hub_private_repo=False,\n","hub_strategy=every_save,\n","hub_token=<HUB_TOKEN>,\n","ignore_data_skip=False,\n","include_inputs_for_metrics=False,\n","jit_mode_eval=False,\n","label_names=None,\n","label_smoothing_factor=0.0,\n","learning_rate=5e-05,\n","length_column_name=length,\n","load_best_model_at_end=False,\n","local_rank=-1,\n","log_level=passive,\n","log_level_replica=passive,\n","log_on_each_node=True,\n","logging_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large/runs/Dec14_07-37-13_3f6a4e87b151,\n","logging_first_step=False,\n","logging_nan_inf_filter=True,\n","logging_steps=100,\n","logging_strategy=steps,\n","lr_scheduler_type=linear,\n","max_grad_norm=1.0,\n","max_steps=-1,\n","metric_for_best_model=None,\n","mp_parameters=,\n","no_cuda=False,\n","num_train_epochs=3.0,\n","optim=adamw_hf,\n","optim_args=None,\n","output_dir=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large,\n","overwrite_output_dir=True,\n","past_index=-1,\n","per_device_eval_batch_size=16,\n","per_device_train_batch_size=16,\n","predict_with_generate=True,\n","prediction_loss_only=False,\n","push_to_hub=False,\n","push_to_hub_model_id=None,\n","push_to_hub_organization=None,\n","push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n","ray_scope=last,\n","remove_unused_columns=True,\n","report_to=['tensorboard'],\n","resume_from_checkpoint=None,\n","run_name=/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large,\n","save_on_each_node=False,\n","save_steps=500,\n","save_strategy=steps,\n","save_total_limit=None,\n","seed=42,\n","sharded_ddp=[],\n","skip_memory_metrics=True,\n","sortish_sampler=False,\n","tf32=None,\n","torch_compile=False,\n","torch_compile_backend=None,\n","torch_compile_mode=None,\n","torchdynamo=None,\n","tpu_metrics_debug=False,\n","tpu_num_cores=None,\n","use_ipex=False,\n","use_legacy_prediction_loop=False,\n","use_mps_device=False,\n","warmup_ratio=0.0,\n","warmup_steps=0,\n","weight_decay=0.0,\n","xpu_backend=None,\n",")\n","WARNING:datasets.builder:Using custom data configuration default-1d0dc81c08029efb\n","INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/csv\n","INFO:datasets.builder:Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n","Downloading data files: 100% 3/3 [00:00<00:00, 2178.10it/s]\n","INFO:datasets.download.download_manager:Downloading took 0.0 min\n","INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n","INFO:datasets.utils.py_utils:Spawning 3 processes for 3 objects in slices of [1, 1, 1]\n","\n","Extracting data files #0:   0% 0/1 [00:00<?, ?obj/s]\n","\n","Extracting data files #1: 100% 1/1 [00:00<00:00, 45.64obj/s]\n","Extracting data files #2: 100% 1/1 [00:00<00:00, 43.62obj/s]\n","Extracting data files #0: 100% 1/1 [00:00<00:00, 27.69obj/s]\n","INFO:datasets.utils.py_utils:Finished 3 processes\n","INFO:datasets.utils.py_utils:Unpacked 3 objects\n","INFO:datasets.utils.info_utils:Unable to verify checksums.\n","INFO:datasets.builder:Generating train split\n","INFO:datasets.builder:Generating validation split\n","INFO:datasets.builder:Generating test split\n","INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n","Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-1d0dc81c08029efb/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n","100% 3/3 [00:00<00:00, 546.39it/s]\n","[INFO|configuration_utils.py:652] 2022-12-14 07:37:21,699 >> loading configuration file /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large/checkpoint-11210/config.json\n","[INFO|configuration_utils.py:706] 2022-12-14 07:37:21,701 >> Model config ProphetNetConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/\\u0110o\\u0302\\u0300 a\\u0301n DS310 - Xu\\u031b\\u0309 li\\u0301 ngo\\u0302n ngu\\u031b\\u0303 tu\\u031b\\u0323 nhie\\u0302n/Model/Prophetnet-Large/checkpoint-11210\",\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"gelu\",\n","  \"add_cross_attention\": true,\n","  \"architectures\": [\n","    \"ProphetNetForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 102,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_max_position_embeddings\": 514,\n","  \"decoder_start_token_id\": 102,\n","  \"disable_ngram_loss\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_max_position_embeddings\": 513,\n","  \"eos_token_id\": 102,\n","  \"eps\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"prophetnet\",\n","  \"ngram\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"num_buckets\": 32,\n","  \"num_decoder_attention_heads\": 16,\n","  \"num_decoder_layers\": 12,\n","  \"num_encoder_attention_heads\": 16,\n","  \"num_encoder_layers\": 12,\n","  \"output_past\": false,\n","  \"pad_token_id\": 0,\n","  \"prefix\": \" \",\n","  \"relative_max_distance\": 128,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","[INFO|tokenization_auto.py:451] 2022-12-14 07:37:21,702 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n","[INFO|configuration_utils.py:652] 2022-12-14 07:37:21,704 >> loading configuration file /content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large/checkpoint-11210/config.json\n","[INFO|configuration_utils.py:706] 2022-12-14 07:37:21,705 >> Model config ProphetNetConfig {\n","  \"_name_or_path\": \"/content/drive/MyDrive/\\u0110o\\u0302\\u0300 a\\u0301n DS310 - Xu\\u031b\\u0309 li\\u0301 ngo\\u0302n ngu\\u031b\\u0303 tu\\u031b\\u0323 nhie\\u0302n/Model/Prophetnet-Large/checkpoint-11210\",\n","  \"activation_dropout\": 0.1,\n","  \"activation_function\": \"gelu\",\n","  \"add_cross_attention\": true,\n","  \"architectures\": [\n","    \"ProphetNetForConditionalGeneration\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"bos_token_id\": 102,\n","  \"decoder_ffn_dim\": 4096,\n","  \"decoder_layerdrop\": 0.0,\n","  \"decoder_max_position_embeddings\": 514,\n","  \"decoder_start_token_id\": 102,\n","  \"disable_ngram_loss\": false,\n","  \"dropout\": 0.1,\n","  \"early_stopping\": true,\n","  \"encoder_ffn_dim\": 4096,\n","  \"encoder_layerdrop\": 0.0,\n","  \"encoder_max_position_embeddings\": 513,\n","  \"eos_token_id\": 102,\n","  \"eps\": 0.0,\n","  \"hidden_size\": 1024,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"init_std\": 0.02,\n","  \"is_encoder_decoder\": true,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"length_penalty\": 2.0,\n","  \"max_length\": 142,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"prophetnet\",\n","  \"ngram\": 2,\n","  \"no_repeat_ngram_size\": 3,\n","  \"num_beams\": 4,\n","  \"num_buckets\": 32,\n","  \"num_decoder_attention_heads\": 16,\n","  \"num_decoder_layers\": 12,\n","  \"num_encoder_attention_heads\": 16,\n","  \"num_encoder_layers\": 12,\n","  \"output_past\": false,\n","  \"pad_token_id\": 0,\n","  \"prefix\": \" \",\n","  \"relative_max_distance\": 128,\n","  \"task_specific_params\": {\n","    \"summarization\": {\n","      \"early_stopping\": true,\n","      \"length_penalty\": 2.0,\n","      \"no_repeat_ngram_size\": 3,\n","      \"num_beams\": 4\n","    }\n","  },\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.26.0.dev0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 30522\n","}\n","\n","Traceback (most recent call last):\n","  File \"examples/pytorch/summarization/run_summarization.py\", line 736, in <module>\n","    main()\n","  File \"examples/pytorch/summarization/run_summarization.py\", line 409, in main\n","    tokenizer = AutoTokenizer.from_pretrained(\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/models/auto/tokenization_auto.py\", line 664, in from_pretrained\n","    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n","  File \"/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\", line 1785, in from_pretrained\n","    raise EnvironmentError(\n","OSError: Can't load tokenizer for '/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large/checkpoint-11210'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '/content/drive/MyDrive/Đồ án DS310 - Xử lí ngôn ngữ tự nhiên/Model/Prophetnet-Large/checkpoint-11210' is the correct path to a directory containing all relevant files for a ProphetNetTokenizer tokenizer.\n"]}]},{"cell_type":"markdown","source":["# Result from Model"],"metadata":{"id":"Mg4Tv7A4CHS2"}},{"cell_type":"code","source":[],"metadata":{"id":"jugpwKcsHT7X"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":["C7P5PuioHByl","V04vs23tRZk3","BnDsRb5xhJnX","mQxAMeSBpUPA","wjin48INF-fw","oxQASSDwGE3Y","x0ni2aa_EMXp","mXSCo7DUFSfB"]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}